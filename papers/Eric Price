Determining the implied volatility in the Dupire equation for vanilla
  European call options

  The Black-Scholes model gives vanilla Europen call option prices as a
function of the volatility. We prove Lipschitz stability in the inverse problem
of determining the implied volatility, which is a function of the underlying
asset, from a collection of quoted option prices with different strikes.


Stable reconstruction of the volatility in a regime-switching local
  volatility model

  Prices of European call options in a regime-switching local volatility model
can be computed by solving a parabolic system which generalises the classical
Black and Scholes equation, giving these prices as functionals of the local
volatilities. We prove Lipschitz stability for the inverse problem of
determining the local volatilities from quoted call option prices for a range
of strikes, if the calls are indexed by the different states of the continuous
Markov chain which governs the regime switches.


Lead, Follow, or Go Your Own Way: Empirical Evidence Against
  Leader-Follower Behavior in Electronic Markets

  Low search costs in Internet markets can be used by consumers to find low
prices, but can also be used by retailers to monitor competitors' prices. This
price monitoring can lead to price matching, resulting in dampened price
competition and higher prices in some cases. This paper analyzes price data for
316 bestselling, computer, and random book titles gathered from 32 retailers
between August 1999 and January 2000. In contrast to previous studies we find
no evidence of leader-follow behavior for the vast majority of retailers we
study. Further, the few cases of leader-follow behavior we observe seem to be
associated with managerial convenience as opposed to anti-competitive behavior.
We offer a methodology that can be used by future academic researchers or
government regulators to check for anti-competitive price matching behavior in
future time periods or in additional product categories.


Self-Confirming Price Prediction Strategies for Simultaneous One-Shot
  Auctions

  Bidding in simultaneous auctions is challenging because an agent's value for
a good in one auction may depend on the uncertain outcome of other auctions:
the so-called exposure problem. Given the gap in understanding of general
simultaneous auction games, previous works have tackled this problem with
heuristic strategies that employ probabilistic price predictions. We define a
concept of self-confirming prices, and show that within an independent private
value model, Bayes-Nash equilibrium can be fully characterized as a profile of
optimal price prediction strategies with self-confirming predictions. We
exhibit practical procedures to compute approximately optimal bids given a
probabilistic price prediction, and near self-confirming price predictions
given a price-prediction strategy. An extensive empirical game-theoretic
analysis demonstrates that self-confirming price prediction strategies are
effective in simultaneous auction games with both complementary and
substitutable preference structures.


Price and Quantity Trajectories: Second-order Dynamics

  In two previous papers the author developed a second-order price adjustment
(t\^atonnement) process. This paper extends the approach to include both
quantity and price adjustments. We demonstrate three results: a analogue to
physical energy, called "activity" arises naturally in the model, and is not
conserved in general; price and quantity trajectories must either end at a
local minimum of a scalar potential or circulate endlessly; and disturbances
into a subspace of substitutable commodities decay over time. From this we
argue, although we do not prove, that the model features global stability,
combined with local instability, a characteristic of many real markets.
Following these observations and a brief survey of empirical results for
price-setting and consumption behavior in markets for "real" goods (as opposed
to financial markets), we conjecture that Stigler and Becker's well-known
theory of consumer preference opens the possibility of substantial degeneracy
in commodity space, and therefore that price and quantity trajectories could
lie on a relatively low-dimensional subspace within the full commodity space.


Price Jump Prediction in Limit Order Book

  A limit order book provides information on available limit order prices and
their volumes. Based on these quantities, we give an empirical result on the
relationship between the bid-ask liquidity balance and trade sign and we show
that liquidity balance on best bid/best ask is quite informative for predicting
the future market order's direction. Moreover, we define price jump as a sell
(buy) market order arrival which is executed at a price which is smaller
(larger) than the best bid (best ask) price at the moment just after the
precedent market order arrival. Features are then extracted related to limit
order volumes, limit order price gaps, market order information and limit order
event information. Logistic regression is applied to predict the price jump
from the limit order book's feature. LASSO logistic regression is introduced to
help us make variable selection from which we are capable to highlight the
importance of different features in predicting the future price jump. In order
to get rid of the intraday data seasonality, the analysis is based on two
separated datasets: morning dataset and afternoon dataset. Based on an analysis
on forty largest French stocks of CAC40, we find that trade sign and market
order size as well as the liquidity on the best bid (best ask) are consistently
informative for predicting the incoming price jump.


Second-order Price Dynamics: Approach to Equilibrium with Perpetual
  Arbitrage

  The notion that economies should normally be in equilibrium is by now
well-established; equally well-established is that economies are almost never
precisely in equilibrium. Using a very general formulation, we show that under
dynamics that are second-order in time a price system can remain away from
equilibrium with permanent and repeating opportunities for arbitrage, even when
a damping term drives the system towards equilibrium. We also argue that
second-order dynamic equations emerge naturally when there are heterogeneous
economic actors, some behaving as active and knowledgeable arbitrageurs, and
others using heuristics. The essential mechanism is that active arbitrageurs
are able to repeatedly benefit from the suboptimal heuristics that govern most
economic behavior.


Optimal Lower Bound for Itemset Frequency Indicator Sketches

  Given a database, a common problem is to find the pairs or $k$-tuples of
items that frequently co-occur. One specific problem is to create a small space
"sketch" of the data that records which $k$-tuples appear in more than an
$\epsilon$ fraction of rows of the database.
  We improve the lower bound of Liberty, Mitzenmacher, and Thaler [LMT14],
showing that $\Omega(\frac{1}{\epsilon}d \log (\epsilon d))$ bits are necessary
even in the case of $k=2$. This matches the sampling upper bound for all
$\epsilon \geq 1/d^{.99}$, and (in the case of $k=2$) another trivial upper
bound for $\epsilon = 1/d$.


VCG Payments for Portfolio Allocations in Online Advertising

  Some online advertising offers pay only when an ad elicits a response.
Randomness and uncertainty about response rates make showing those ads a risky
investment for online publishers. Like financial investors, publishers can use
portfolio allocation over multiple advertising offers to pursue revenue while
controlling risk. Allocations over multiple offers do not have a distinct
winner and runner-up, so the usual second-price mechanism does not apply. This
paper develops a pricing mechanism for portfolio allocations. The mechanism is
efficient, truthful, and rewards offers that reduce risk.


A Merton-Like Approach to Pricing Debt based on a non-Gaussian Asset
  Model

  This paper is a contribution to the Proceedings of the Workshop
  Complexity, Metastability and Nonextensivity held in Erice 20-26 July 2004,
to be published by World Scientific. We propose a generalization to Merton's
model for evaluating credit spreads. In his original work, a company's assets
were assumed to follow a log-normal process. We introduce fat tails and skew
into this model, along the same lines as in the option pricing model of Borland
and Bouchaud (2004, Quantitative Finance 4) and illustrate the effects of each
component. Preliminary empirical results indicate that this model fits well to
empirically observed credit spreads with a parameterization that also matched
observed stock return distributions and option prices.


A quantitative model of trading and price formation in financial markets

  We use standard physics techniques to model trading and price formation in a
market under the assumption that order arrival and cancellations are Poisson
random processes. This model makes testable predictions for the most basic
properties of a market, such as the diffusion rate of prices, which is the
standard measure of financial risk, and the spread and price impact functions,
which are the main determinants of transaction cost. Guided by dimensional
analysis, simulation, and mean field theory, we find scaling relations in terms
of order flow rates. We show that even under completely random order flow the
need to store supply and demand to facilitate trading induces anomalous
diffusion and temporal structure in prices.


Bayesian Budget Feasibility with Posted Pricing

  We consider the problem of budget feasible mechanism design proposed by
Singer (2010), but in a Bayesian setting. A principal has a public value for
hiring a subset of the agents and a budget, while the agents have private costs
for being hired. We consider both additive and submodular value functions of
the principal. We show that there are simple, practical, ex post budget
balanced posted pricing mechanisms that approximate the value obtained by the
Bayesian optimal mechanism that is budget balanced only in expectation. A main
motivating application for this work is the crowdsourcing large projects, e.g.,
on Mechanical Turk, where workers are drawn from a large population and posted
pricing is standard. Our analysis methods relate to contention resolution
schemes in submodular optimization of Vondrak et al. (2011) and the correlation
gap analysis of Yan (2011).


Quantifying Volatility Reduction in German Day-ahead Spot Market in the
  Period 2006 through 2016

  In Europe, Germany is taking the lead in the switch from the conventional to
renewable energy. This poses new challenges as wind and solar energy are
fundamentally intermittent, weather-dependent and less predictable. It is
therefore of considerable interest to investigate the evolution of price
volatility in this post-transition era. There are a number of reasons, however,
that makes the practical studies difficult. For instance, EPEX prices can be
zero or negative. Consequently, the standard approach in financial time series
analysis to switch to logarithmic measures is inapplicable. Furthermore, in
contrast to the stock market prices which are only available for trading days,
EPEX prices cover the whole year, including weekends and holidays. Accordingly,
there is a lot of underlying variability in the data which has nothing to do
with volatility, but simply reflects diurnal activity patterns. An important
distinction of the present work is the application of matrix decomposition
techniques, namely the singular value decomposition (SVD), for defining an
alternative notion of volatility. This approach is systematically more robust
toward outliers and also the diurnal patterns. Our observations show that the
day-ahead market is becoming less volatile in recent years.


Cumulative Prospect Theory Based Dynamic Pricing for Shared Mobility on
  Demand Services

  Cumulative Prospect Theory (CPT) is a modeling tool widely used in behavioral
economics and cognitive psychology that captures subjective decision making of
individuals under risk or uncertainty. In this paper, we propose a dynamic
pricing strategy for Shared Mobility on Demand Services (SMoDS) using a
behavioral model based on CPT. This dynamic pricing strategy together with
dynamic routing via an optimization algorithm that we have developed earlier,
provide a complete solution customized for SMoDS of multi-passenger transport.
The basic principles of CPT and the derivation of the passenger behavioral
model in the SMoDS context are described in detail. The implications of CPT on
dynamic pricing of the SMoDS are delineated using computational experiments
involving passenger preferences. These implications include interpretation of
the classic fourfold pattern of risk attitudes, strong risk aversion over mixed
prospects, and behavioral preferences of self reference. Overall, it is argued
that the use of the CPT framework corresponds to a crucial building block in
designing socio-technical systems by allowing quantification of subjective
decision making that is perceived to be otherwise qualitative.


Smart expansion and fast calibration for jump diffusion

  Using Malliavin calculus techniques, we derive an analytical formula for the
price of European options, for any model including local volatility and Poisson
jump process. We show that the accuracy of the formula depends on the
smoothness of the payoff function. Our approach relies on an asymptotic
expansion related to small diffusion and small jump frequency/size. Our formula
has excellent accuracy (the error on implied Black-Scholes volatilities for
call option is smaller than 2 bp for various strikes and maturities).
Additionally, model calibration becomes very rapid.


Non-Markovian Beables vs. Massive Parallelism

  A simple dynamical model over a discrete classical state space is presented.
In a certain limit, it reduces to one in a class of models subsuming Bell's
field-theoretic version of Bohmian mechanics. But it exhibits the massive
parallelism native to quantum mechanics only as an emergent phenomenon, in
contrast with Bell's and other hidden variable theories. While still non-local
in its dynamics, the model thus restores our ability to regard a system as a
combination of separate, localized parts, at the price of admitting
non-Markovian dynamics.


Ensemble Validation: Selectivity has a Price, but Variety is Free

  Suppose some classifiers are selected from a set of hypothesis classifiers to
form an equally-weighted ensemble that selects a member classifier at random
for each input example. Then the ensemble has an error bound consisting of the
average error bound for the member classifiers, a term for selectivity that
varies from zero (if all hypothesis classifiers are selected) to a standard
uniform error bound (if only a single classifier is selected), and small
constants. There is no penalty for using a richer hypothesis set if the same
fraction of the hypothesis classifiers are selected for the ensemble.


Radiative falloff of a scalar field in a weakly curved spacetime without
  symmetries

  We consider a massless scalar field propagating in a weakly curved spacetime
whose metric is a solution to the linearized Einstein field equations. The
spacetime is assumed to be stationary and asymptotically flat, but no other
symmetries are imposed -- the spacetime can rotate and deviate strongly from
spherical symmetry. We prove that the late-time behavior of the scalar field is
identical to what it would be in a spherically-symmetric spacetime: it decays
in time according to an inverse power-law, with a power determined by the
angular profile of the initial wave packet (Price falloff theorem). The field's
late-time dynamics is insensitive to the nonspherical aspects of the metric,
and it is governed entirely by the spacetime's total gravitational mass; other
multipole moments, and in particular the spacetime's total angular momentum, do
not enter in the description of the field's late-time behavior. This extended
formulation of Price's falloff theorem appears to be at odds with previous
studies of radiative decay in the spacetime of a Kerr black hole. We show,
however, that the contradiction is only apparent, and that it is largely an
artifact of the Boyer-Lindquist coordinates adopted in these studies.


Efficient Sketches for the Set Query Problem

  We develop an algorithm for estimating the values of a vector x in R^n over a
support S of size k from a randomized sparse binary linear sketch Ax of size
O(k). Given Ax and S, we can recover x' with ||x' - x_S||_2 <= eps ||x -
x_S||_2 with probability at least 1 - k^{-\Omega(1)}. The recovery takes O(k)
time.
  While interesting in its own right, this primitive also has a number of
applications. For example, we can:
  1. Improve the linear k-sparse recovery of heavy hitters in Zipfian
distributions with O(k log n) space from a (1+eps) approximation to a (1 +
o(1)) approximation, giving the first such approximation in O(k log n) space
when k <= O(n^{1-eps}).
  2. Recover block-sparse vectors with O(k) space and a (1+eps) approximation.
Previous algorithms required either omega(k) space or omega(1) approximation.


K-Median Clustering, Model-Based Compressive Sensing, and Sparse
  Recovery for Earth Mover Distance

  We initiate the study of sparse recovery problems under the Earth-Mover
Distance (EMD). Specifically, we design a distribution over m x n matrices A
such that for any x, given Ax, we can recover a k-sparse approximation to x
under the EMD distance. One construction yields m = O(k log(n/k)) and a 1 +
epsilon approximation factor, which matches the best achievable bound for other
error measures, such as the L_1 norm. Our algorithms are obtained by exploiting
novel connections to other problems and areas, such as streaming algorithms for
k-median clustering and model-based compressive sensing. We also provide novel
algorithms and results for the latter problems.


Nearly Optimal Sparse Fourier Transform

  We consider the problem of computing the k-sparse approximation to the
discrete Fourier transform of an n-dimensional signal. We show:
  * An O(k log n)-time randomized algorithm for the case where the input signal
has at most k non-zero Fourier coefficients, and
  * An O(k log n log(n/k))-time randomized algorithm for general input signals.
  Both algorithms achieve o(n log n) time, and thus improve over the Fast
Fourier Transform, for any k = o(n). They are the first known algorithms that
satisfy this property. Also, if one assumes that the Fast Fourier Transform is
optimal, the algorithm for the exactly k-sparse case is optimal for any k =
n^{\Omega(1)}.
  We complement our algorithmic results by showing that any algorithm for
computing the sparse Fourier transform of a general signal must use at least
\Omega(k log(n/k)/ log log n) signal samples, even if it is allowed to perform
adaptive sampling.


Sample-Optimal Average-Case Sparse Fourier Transform in Two Dimensions

  We present the first sample-optimal sublinear time algorithms for the sparse
Discrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our
algorithms are analyzed for /average case/ signals. For signals whose spectrum
is exactly sparse, our algorithms use O(k) samples and run in O(k log k) time,
where k is the expected sparsity of the signal. For signals whose spectrum is
approximately sparse, our algorithm uses O(k log n) samples and runs in O(k
log^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of
samples used by our algorithms matches the known lower bounds for the
respective signal models.
  By a known reduction, our algorithms give similar results for the
one-dimensional sparse Discrete Fourier Transform when n is a power of a small
composite number (e.g., n = 6^t).


Lower Bounds for Sparse Recovery

  We consider the following k-sparse recovery problem: design an m x n matrix
A, such that for any signal x, given Ax we can efficiently recover x'
satisfying
  ||x-x'||_1 <= C min_{k-sparse} x"} ||x-x"||_1.
  It is known that there exist matrices A with this property that have only O(k
log (n/k)) rows.
  In this paper we show that this bound is tight. Our bound holds even for the
more general /randomized/ version of the problem, where A is a random variable
and the recovery algorithm is required to work for any fixed x with constant
probability (over A).


Binary Embedding: Fundamental Limits and Fast Algorithm

  Binary embedding is a nonlinear dimension reduction methodology where high
dimensional data are embedded into the Hamming cube while preserving the
structure of the original space. Specifically, for an arbitrary $N$ distinct
points in $\mathbb{S}^{p-1}$, our goal is to encode each point using
$m$-dimensional binary strings such that we can reconstruct their geodesic
distance up to $\delta$ uniform distortion. Existing binary embedding
algorithms either lack theoretical guarantees or suffer from running time
$O\big(mp\big)$. We make three contributions: (1) we establish a lower bound
that shows any binary embedding oblivious to the set of points requires $m =
\Omega(\frac{1}{\delta^2}\log{N})$ bits and a similar lower bound for
non-oblivious embeddings into Hamming distance; (2) [DELETED, see comment]; (3)
we also provide an analytic result about embedding a general set of points $K
\subseteq \mathbb{S}^{p-1}$ with even infinite size. Our theoretical findings
are supported through experiments on both synthetic and real data sets.


Trend without hiccups: a Kalman filter approach

  Have you ever felt miserable because of a sudden whipsaw in the price that
triggered an unfortunate trade? In an attempt to remove this noise, technical
analysts have used various types of moving averages (simple, exponential,
adaptive one or using Nyquist criterion). These tools may have performed
decently but we show in this paper that this can be improved dramatically
thanks to the optimal filtering theory of Kalman filters (KF). We explain the
basic concepts of KF and its optimum criterion. We provide a pseudo code for
this new technical indicator that demystifies its complexity. We show that this
new smoothing device can be used to better forecast price moves as lag is
reduced. We provide 4 Kalman filter models and their performance on the SP500
mini-future contract. Results are quite illustrative of the efficiency of KF
models with better net performance achieved by the KF model combining smoothing
and extremum position.


Smooth solutions to portfolio liquidation problems under price-sensitive
  market impact

  We consider the stochastic control problem of a financial trader that needs
to unwind a large asset portfolio within a short period of time. The trader can
simultaneously submit active orders to a primary market and passive orders to a
dark pool. Our framework is flexible enough to allow for price-dependent impact
functions describing the trading costs in the primary market and
price-dependent adverse selection costs associated with dark pool trading. We
prove that the value function can be characterized in terms of the unique
smooth solution to a PDE with singular terminal value, establish its explicit
asymptotic behavior at the terminal time, and give the optimal trading strategy
in feedback form.


Computing large market equilibria using abstractions

  Computing market equilibria is an important practical problem for market
design (e.g. fair division, item allocation). However, computing equilibria
requires large amounts of information (e.g. all valuations for all buyers for
all items) and compute power. We consider ameliorating these issues by applying
a method used for solving complex games: constructing a coarsened abstraction
of a given market, solving for the equilibrium in the abstraction, and lifting
the prices and allocations back to the original market. We show how to bound
important quantities such as regret, envy, Nash social welfare, Pareto
optimality, and maximin share when the abstracted prices and allocations are
used in place of the real equilibrium. We then study two abstraction methods of
interest for practitioners: 1) filling in unknown valuations using techniques
from matrix completion, 2) reducing the problem size by aggregating groups of
buyers/items into smaller numbers of representative buyers/items and solving
for equilibrium in this coarsened market. We find that in real data
allocations/prices that are relatively close to equilibria can be computed from
even very coarse abstractions.


fVSS: A New Secure and Cost-Efficient Scheme for Cloud Data Warehouses

  Cloud business intelligence is an increasingly popular choice to deliver
decision support capabilities via elastic, pay-per-use resources. However, data
security issues are one of the top concerns when dealing with sensitive data.
In this pa-per, we propose a novel approach for securing cloud data warehouses
by flexible verifiable secret sharing, fVSS. Secret sharing encrypts and
distributes data over several cloud ser-vice providers, thus enforcing data
privacy and availability. fVSS addresses four shortcomings in existing secret
sharing-based approaches. First, it allows refreshing the data ware-house when
some service providers fail. Second, it allows on-line analysis processing.
Third, it enforces data integrity with the help of both inner and outer
signatures. Fourth, it helps users control the cost of cloud warehousing by
balanc-ing the load among service providers with respect to their pricing
policies. To illustrate fVSS' efficiency, we thoroughly compare it with
existing secret sharing-based approaches with respect to security features,
querying power and data storage and computing costs.


Statistical theory of the continuous double auction

  Most modern financial markets use a continuous double auction mechanism to
store and match orders and facilitate trading. In this paper we develop a
microscopic dynamical statistical model for the continuous double auction under
the assumption of IID random order flow, and analyze it using simulation,
dimensional analysis, and theoretical tools based on mean field approximations.
The model makes testable predictions for basic properties of markets, such as
price volatility, the depth of stored supply and demand vs. price, the bid-ask
spread, the price impact function, and the time and probability of filling
orders. These predictions are based on properties of order flow and the limit
order book, such as share volume of market and limit orders, cancellations,
typical order size, and tick size. Because these quantities can all be measured
directly there are no free parameters. We show that the order size, which can
be cast as a nondimensional granularity parameter, is in most cases a more
significant determinant of market behavior than tick size. We also provide an
explanation for the observed highly concave nature of the price impact
function. On a broader level, this work suggests how stochastic models based on
zero-intelligence agents may be useful to probe the structure of market
institutions. Like the model of perfect rationality, a stochastic-zero
intelligence model can be used to make strong predictions based on a compact
set of assumptions, even if these assumptions are not fully believable.


Symmetry restoration by pricing in a duopoly of perishable goods

  Competition is a main tenet of economics, and the reason is that a perfectly
competitive equilibrium is Pareto-efficient in the absence of externalities and
public goods. Whether a product is selected in a market crucially relates to
its competitiveness, but the selection in turn affects the landscape of
competition. Such a feedback mechanism has been illustrated in a duopoly model
by Lambert et al., in which a buyer's satisfaction is updated depending on the
{\em freshness} of a purchased product. The probability for buyer $n$ to select
seller $i$ is assumed to be $p_{n,i} \propto e^{ S_{n,i}/T}$, where $S_{n,i}$
is the buyer's satisfaction and $T$ is an effective temperature to introduce
stochasticity. If $T$ decreases below a critical point $T_c$, the system
undergoes a transition from a symmetric phase to an asymmetric one, in which
only one of the two sellers is selected. In this work, we extend the model by
incorporating a simple price system. By considering a greed factor $g$ to
control how the satisfaction depends on the price, we argue the existence of an
oscillatory phase in addition to the symmetric and asymmetric ones in the
$(T,g)$ plane, and estimate the phase boundaries through mean-field
approximations. The analytic results show that the market preserves the
inherent symmetry between the sellers for lower $T$ in the presence of the
price system, which is confirmed by our numerical simulations.


Pacing Equilibrium in First-Price Auction Markets

  In ad auctions--the prevalent monetization mechanism of Internet
companies--advertisers compete for online impressions in a sequential auction
market. Since advertisers are typically budget-constrained, a common tool
employed to improve their ROI is that of pacing, i.e., uniform scaling of their
bids to preserve their budget for a longer duration. If the advertisers are
excessively paced, they end up not spending their budget, while if they are not
sufficiently paced, they use up their budget too soon. Therefore, it is
important that they are paced at just the right amount, a solution concept that
we call a pacing equilibrium. In this paper, we study pacing equilibria in the
context of first-price auctions, which are popular in the theory of ad
mechanisms. We show existence, uniqueness, and efficient computability of
first-price pacing equilibria (FPPE), while also establishing several other
salient features of this solution concept. In the process, we uncover a sharp
contrast between these solutions and second price pacing equilibria (SPPE), the
latter being known to produce non-unique, fragile solutions that are also
computationally hard to obtain. Simulations show that FPPE have better revenue
properties than SPPE, that bidders have lower ex-post regret, and that
incentives to misreport budgets for thick markets are smaller.


Matters of Gravity, the newsletter of the APS Topical Group on
  Gravitation

  Research Briefs:
  Cosmic microwave background anisotropy experiments, by Sean Carroll
  LISA Project Update by Bill Folkner
  An update on the r-mode instability, by Nils Andersson
  Laboratory experiments: news from MG9, by Riley Newman
  Progress toward Commissioning the LIGO detectors, by Stan Whitcomb
  160 Hours of Data Taken on TAMA300, by Seiji Kawamura
  Conference reports:
  Kipfest, by Richard Price
  Third Capra meeting, by Eric Poisson
  GR at the XIIIth Congress on Mathematical Physics, by Abhay Ashtekar
  3rd International LISA Symposium, by Curt Cutler


Parallel Computing for QCD on a Pentium Cluster

  Motivated by the computational demands of our research and budgetary
constraints which are common to many research institutions, we built a ``poor
man's supercomputer'', a cluster of PC nodes which together can perform
parallel calculations at a fraction of the price of a commercial supercomputer.
We describe the construction, cost, and performance of our cluster.


Light-Cone Gauge for 1+1 Strings

  Explicit construction of the light-cone gauge quantum theory of bosonic
strings in 1+1 spacetime dimensions reveals unexpected structures. One is the
existence of a gauge choice that gives a free action at the price of
propagating ghosts and a nontrivial BRST charge. Fixing this gauge leaves a
U(1) Kac-Moody algebra of residual symmetry, generated by a conformal tensor of
rank two and a conformal scalar. Another is that the BRST charge made from
these currents is nilpotent when the action includes a linear dilaton
background, independent of the particular value of the dilaton gradient.
Spacetime Lorentz invariance in this theory is still elusive, however, because
of the linear dilaton background and the nature of the gauge symmetries.


A probabilistic max-plus numerical method for solving stochastic control
  problems

  We consider fully nonlinear Hamilton-Jacobi-Bellman equations associated to
diffusion control problems involving a finite set-valued (or switching) control
and possibly a continuum-valued control. We construct a lower complexity
probabilistic numerical algorithm by combining the idempotent expansion
properties obtained by McEneaney, Kaise and Han (2011) for solving such
problems with a numerical probabilistic method such as the one proposed by
Fahim, Touzi and Warin (2011) for solving some fully nonlinear parabolic
partial differential equations. Numerical tests on a small example of pricing
and hedging an option are presented.


Sequential Bayesian Learning for Merton's Jump Model with Stochastic
  Volatility

  Jump stochastic volatility models are central to financial econometrics for
volatility forecasting, portfolio risk management, and derivatives pricing.
Markov Chain Monte Carlo (MCMC) algorithms are computationally unfeasible for
the sequential learning of volatility state variables and parameters, whereby
the investor must update all posterior and predictive densities as new
information arrives. We develop a particle filtering and learning algorithm to
sample posterior distribution in Merton's jump stochastic volatility. This
allows to filter spot volatilities and jump times, together with sequentially
updating (learning) of jump and volatility parameters. We illustrate our
methodology on Google's stock return. We conclude with directions for future
research.


Robust polynomial regression up to the information theoretic limit

  We consider the problem of robust polynomial regression, where one receives
samples $(x_i, y_i)$ that are usually within $\sigma$ of a polynomial $y =
p(x)$, but have a $\rho$ chance of being arbitrary adversarial outliers.
Previously, it was known how to efficiently estimate $p$ only when $\rho <
\frac{1}{\log d}$. We give an algorithm that works for the entire feasible
range of $\rho < 1/2$, while simultaneously improving other parameters of the
problem. We complement our algorithm, which gives a factor 2 approximation,
with impossibility results that show, for example, that a $1.09$ approximation
is impossible even with infinitely many samples.


Stochastic Multi-armed Bandits in Constant Space

  We consider the stochastic bandit problem in the sublinear space setting,
where one cannot record the win-loss record for all $K$ arms. We give an
algorithm using $O(1)$ words of space with regret \[
  \sum_{i=1}^{K}\frac{1}{\Delta_i}\log \frac{\Delta_i}{\Delta}\log T \] where
$\Delta_i$ is the gap between the best arm and arm $i$ and $\Delta$ is the gap
between the best and the second-best arms. If the rewards are bounded away from
$0$ and $1$, this is within an $O(\log 1/\Delta)$ factor of the optimum regret
possible without space constraints.


The Supersingularity of Hurwitz Curves

  We study when Hurwitz curves are supersingular. Specifically, we show that
the curve $H_{n,\ell}: X^nY^\ell + Y^nZ^\ell + Z^nX^\ell = 0$, with $n$ and
$\ell$ relatively prime, is supersingular over the finite field
$\mathbb{F}_{p}$ if and only if there exists an integer $i$ such that $p^i
\equiv -1 \bmod (n^2 - n\ell + \ell^2)$. If this holds, we prove that it is
also true that the curve is maximal over $\mathbb{F}_{p^{2i}}$. Further, we
provide a complete table of supersingular Hurwitz curves of genus less than 5
for characteristic less than 37.


Lower Bounds for Adaptive Sparse Recovery

  We give lower bounds for the problem of stable sparse recovery from
/adaptive/ linear measurements. In this problem, one would like to estimate a
vector $x \in \R^n$ from $m$ linear measurements $A_1x,..., A_mx$. One may
choose each vector $A_i$ based on $A_1x,..., A_{i-1}x$, and must output $x*$
satisfying |x* - x|_p \leq (1 + \epsilon) \min_{k\text{-sparse} x'} |x - x'|_p
with probability at least $1-\delta>2/3$, for some $p \in \{1,2\}$. For $p=2$,
it was recently shown that this is possible with $m = O(\frac{1}{\epsilon}k
\log \log (n/k))$, while nonadaptively it requires $\Theta(\frac{1}{\epsilon}k
\log (n/k))$. It is also known that even adaptively, it takes $m =
\Omega(k/\epsilon)$ for $p = 2$. For $p = 1$, there is a non-adaptive upper
bound of $\tilde{O}(\frac{1}{\sqrt{\epsilon}} k\log n)$. We show:
  * For $p=2$, $m = \Omega(\log \log n)$. This is tight for $k = O(1)$ and
constant $\epsilon$, and shows that the $\log \log n$ dependence is correct.
  * If the measurement vectors are chosen in $R$ "rounds", then $m = \Omega(R
\log^{1/R} n)$. For constant $\epsilon$, this matches the previously known
upper bound up to an O(1) factor in $R$.
  * For $p=1$, $m = \Omega(k/(\sqrt{\epsilon} \cdot \log k/\epsilon))$. This
shows that adaptivity cannot improve more than logarithmic factors, providing
the analog of the $m = \Omega(k/\epsilon)$ bound for $p = 2$.


On the Power of Adaptivity in Sparse Recovery

  The goal of (stable) sparse recovery is to recover a $k$-sparse approximation
$x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is
to recover $x*$ such that ||x-x*||_p <= C min_{k-sparse x'} ||x-x'||_q for some
constant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or
$p=q=2$, this task can be accomplished using $m=O(k \log (n/k))$ non-adaptive
measurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].
  In this paper we show that if one is allowed to perform measurements that are
adaptive, then the number of measurements can be considerably reduced.
Specifically, for $C=1+eps$ and $p=q=2$ we show - A scheme with $m=O((1/eps)k
log log (n eps/k))$ measurements that uses $O(log* k \log \log (n eps/k))$
rounds. This is a significant improvement over the best possible non-adaptive
bound. - A scheme with $m=O((1/eps) k log (k/eps) + k \log (n/k))$ measurements
that uses /two/ rounds. This improves over the best possible non-adaptive
bound. To the best of our knowledge, these are the first results of this type.
As an independent application, we show how to solve the problem of finding a
duplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using
$O(log n)$ bits of space and $O(log log n)$ passes, improving over the best
possible space complexity achievable using a single pass.


(1+eps)-approximate Sparse Recovery

  The problem central to sparse recovery and compressive sensing is that of
stable sparse recovery: we want a distribution of matrices A in R^{m\times n}
such that, for any x \in R^n and with probability at least 2/3 over A, there is
an algorithm to recover x* from Ax with
  ||x* - x||_p <= C min_{k-sparse x'} ||x - x'||_p for some constant C > 1 and
norm p. The measurement complexity of this problem is well understood for
constant C > 1. However, in a variety of applications it is important to obtain
C = 1 + eps for a small eps > 0, and this complexity is not well understood. We
resolve the dependence on eps in the number of measurements required of a
k-sparse recovery algorithm, up to polylogarithmic factors for the central
cases of p = 1 and p = 2. Namely, we give new algorithms and lower bounds that
show the number of measurements required is (1/eps^{p/2})k polylog(n). For p =
2, our bound of (1/eps) k log(n/k) is tight up to constant factors. We also
give matching bounds when the output is required to be k-sparse, in which case
we achieve (1/eps^p) k polylog(n). This shows the distinction between the
complexity of sparse and non-sparse outputs is fundamental.


Tight bounds for learning a mixture of two gaussians

  We consider the problem of identifying the parameters of an unknown mixture
of two arbitrary $d$-dimensional gaussians from a sequence of independent
random samples. Our main results are upper and lower bounds giving a
computationally efficient moment-based estimator with an optimal convergence
rate, thus resolving a problem introduced by Pearson (1894). Denoting by
$\sigma^2$ the variance of the unknown mixture, we prove that
$\Theta(\sigma^{12})$ samples are necessary and sufficient to estimate each
parameter up to constant additive error when $d=1.$ Our upper bound extends to
arbitrary dimension $d>1$ up to a (provably necessary) logarithmic loss in $d$
using a novel---yet simple---dimensionality reduction technique. We further
identify several interesting special cases where the sample complexity is
notably smaller than our optimal worst-case bound. For instance, if the means
of the two components are separated by $\Omega(\sigma)$ the sample complexity
reduces to $O(\sigma^2)$ and this is again optimal.
  Our results also apply to learning each component of the mixture up to small
error in total variation distance, where our algorithm gives strong
improvements in sample complexity over previous work. We also extend our lower
bound to mixtures of $k$ Gaussians, showing that $\Omega(\sigma^{6k-2})$
samples are necessary to estimate each parameter up to constant additive error.


Modelling Information Incorporation in Markets, with Application to
  Detecting and Explaining Events

  We develop a model of how information flows into a market, and derive
algorithms for automatically detecting and explaining relevant events. We
analyze data from twenty-two "political stock markets" (i.e., betting markets
on political outcomes) on the Iowa Electronic Market (IEM). We prove that,
under certain efficiency assumptions, prices in such betting markets will on
average approach the correct outcomes over time, and show that IEM data
conforms closely to the theory. We present a simple model of a betting market
where information is revealed over time, and show a qualitative correspondence
between the model and real market data. We also present an algorithm for
automatically detecting significant events and generating semantic explanations
of their origin. The algorithm operates by discovering significant changes in
vocabulary on online news sources (using expected entropy loss) that align with
major price spikes in related betting markets.


Adversarial Examples from Cryptographic Pseudo-Random Generators

  In our recent work (Bubeck, Price, Razenshteyn, arXiv:1805.10204) we argued
that adversarial examples in machine learning might be due to an inherent
computational hardness of the problem. More precisely, we constructed a binary
classification task for which (i) a robust classifier exists; yet no
non-trivial accuracy can be obtained with an efficient algorithm in (ii) the
statistical query model. In the present paper we significantly strengthen both
(i) and (ii): we now construct a task which admits (i') a maximally robust
classifier (that is it can tolerate perturbations of size comparable to the
size of the examples themselves); and moreover we prove computational hardness
of learning this task under (ii') a standard cryptographic assumption.


Curved Space or Curved Vacuum?

  While the simple picture of a spatially flat, matter plus cosmological
constant universe fits current observation of the accelerated expansion, strong
consideration has also been given to models with dynamical vacuum energy. We
examine the tradeoff of ``curving'' the vacuum but retaining spatial flatness,
vs. curving space but retaining the cosmological constant. These different
breakdowns in the simple picture could readily be distinguished by combined
high accuracy supernovae and cosmic microwave background distance measurements.
If we allow the uneasy situation of both breakdowns, the curvature can still be
measured to 1%, but at the price of degrading estimation of the equation of
state time variation by 60% or more, unless additional information (such as
weak lensing data or a tight matter density prior) is included.


Gravitational perturbations of the Schwarzschild spacetime: A practical
  covariant and gauge-invariant formalism

  We present a formalism to study the metric perturbations of the Schwarzschild
spacetime. The formalism is gauge invariant, and it is also covariant under
two-dimensional coordinate transformations that leave the angular coordinates
unchanged. The formalism is applied to the typical problem of calculating the
gravitational waves produced by material sources moving in the Schwarzschild
spacetime. We examine the radiation escaping to future null infinity as well as
the radiation crossing the event horizon. The waveforms, the energy radiated,
and the angular-momentum radiated can all be expressed in terms of two
gauge-invariant scalar functions that satisfy one-dimensional wave equations.
The first is the Zerilli-Moncrief function, which satisfies the Zerilli
equation, and which represents the even-parity sector of the perturbation. The
second is the Cunningham-Price-Moncrief function, which satisfies the
Regge-Wheeler equation, and which represents the odd-parity sector of the
perturbation. The covariant forms of these wave equations are presented here,
complete with covariant source terms that are derived from the stress-energy
tensor of the matter responsible for the perturbation. Our presentation of the
formalism is concluded with a separate examination of the monopole and dipole
components of the metric perturbation.


Integral Field Spectrographs: a user's view

  We easily tend to think of Integral-Field Spectrographs (IFS) along two
opposing trends: as either the beautiful combination between photometry and
spectroscopy, or as our worst nightmare including the dark side of both worlds.
I favour a view where each IFS is considered individually, as one instrument
with specific performances which can be used optimally for a certain range of
scientific programs. It is indeed true that data-wise, IFS do sometime merge
the characteristics of classic (e.g., long-slit) spectrographs with annoying
issues associated with Imagers. This is in fact the price to pay to access a
drastically different perspective of our favourite targets. The challenge is
then to provide the necessary tools to properly handle the corresponding data.
However, this should certainly not be thought as something specific to IFS:
such a challenge should be accepted for any instrument, and most importantly
solved prior to its delivery at the telescope.


Second-Order, Dissipative TÃ¢tonnement: Economic Interpretation and
  2-Point Limit Cycles

  This paper proposes an alternative to the classical price-adjustment
mechanism (called "t\^{a}tonnement" after Walras) that is second-order in time.
The proposed mechanism, an analogue to the damped harmonic oscillator, provides
a dynamic equilibration process that depends only on local information. We show
how such a process can result from simple behavioural rules. The discrete-time
form of the model can result in two-step limit cycles, but as the distance
covered by the cycle depends on the size of the damping, the proposed mechanism
can lead to both highly unstable and relatively stable behaviour, as observed
in real economies.


Post-periapsis pancakes: sustenance for self-gravity in tidal disruption
  events

  A tidal disruption event, which occurs when a star is destroyed by the
gravitational field of a supermassive black hole, produces a stream of debris,
the evolution of which ultimately determines the observational properties of
the event. Here we show that a post-periapsis caustic -- a location where the
locus of gas parcels comprising the stream would collapse into a
two-dimensional surface if they evolved solely in the gravitational field of
the hole -- occurs when the pericenter distance of the star is on the order of
the tidal radius of the hole. It is demonstrated that this "pancake" induces
significant density perturbations in the debris stream, and, for stiffer
equations of state (adiabatic index $\gamma \gtrsim 5/3$), these fluctuations
are sufficient to gravitationally destabilize the stream, resulting in its
fragmentation into bound clumps. The results of our findings are discussed in
the context of the observational properties of tidal disruption events.


