Zero Shot Recognition with Unreliable Attributes

  In principle, zero-shot learning makes it possible to train a recognitionmodel simply by specifying the category's attributes. For example, withclassifiers for generic attributes like \emph{striped} and \emph{four-legged},one can construct a classifier for the zebra category by enumerating whichproperties it possesses---even without providing zebra training images. Inpractice, however, the standard zero-shot paradigm suffers because attributepredictions in novel images are hard to get right. We propose a novel randomforest approach to train zero-shot models that explicitly accounts for theunreliability of attribute predictions. By leveraging statistics about eachattribute's error tendencies, our method obtains more robust discriminativemodels for the unseen classes. We further devise extensions to handle thefew-shot scenario and unreliable attribute descriptions. On three datasets, wedemonstrate the benefit for visual category learning with zero or few trainingexamples, a critical domain for rare categories or categories defined on thefly.

Large-Margin Determinantal Point Processes

  Determinantal point processes (DPPs) offer a powerful approach to modelingdiversity in many applications where the goal is to select a diverse subset. Westudy the problem of learning the parameters (the kernel matrix) of a DPP fromlabeled training data. We make two contributions. First, we show how toreparameterize a DPP's kernel matrix with multiple kernel functions, thusenhancing modeling flexibility. Second, we propose a novel parameter estimationtechnique based on the principle of large margin separation. In contrast to thestate-of-the-art method of maximum likelihood estimation, our large-margin lossfunction explicitly models errors in selecting the target subsets, and it canbe customized to trade off different types of errors (precision vs. recall).Extensive empirical studies validate our contributions, including applicationson challenging document and video summarization, where flexibility in modelingthe kernel matrix and balancing different errors is indispensable.

Learning image representations tied to ego-motion

  Understanding how images of objects and scenes behave in response to specificego-motions is a crucial aspect of proper visual development, yet existingvisual learning methods are conspicuously disconnected from the physical sourceof their images. We propose to exploit proprioceptive motor signals to provideunsupervised regularization in convolutional neural networks to learn visualrepresentations from egocentric video. Specifically, we enforce that ourlearned features exhibit equivariance i.e. they respond predictably totransformations associated with distinct ego-motions. With three datasets, weshow that our unsupervised feature learning approach significantly outperformsprevious approaches on visual recognition and next-best-view prediction tasks.In the most challenging test, we show that features learned from video capturedon an autonomous driving platform improve large-scale scene recognition instatic images from a disjoint domain.

Discovering Attribute Shades of Meaning with the Crowd

  To learn semantic attributes, existing methods typically train onediscriminative model for each word in a vocabulary of nameable properties.However, this "one model per word" assumption is problematic: while a wordmight have a precise linguistic definition, it need not have a precise visualdefinition. We propose to discover shades of attribute meaning. Given anattribute name, we use crowdsourced image labels to discover the latent factorsunderlying how different annotators perceive the named concept. We show thatstructure in those latent factors helps reveal shades, that is, interpretationsfor the attribute shared by some group of annotators. Using these shades, wetrain classifiers to capture the primary (often subtle) variants of theattribute. The resulting models are both semantic and visually precise. Bycatering to users' interpretations, they improve attribute prediction accuracyon novel images. Shades also enable more successful attribute-based imagesearch, by providing robust personalized models for retrieving multi-attributequery results. They are widely applicable to tasks that involve describingvisual content, such as zero-shot category learning and organization of photocollections.

Slow and steady feature analysis: higher order temporal coherence in  video

  How can unlabeled video augment visual learning? Existing methods perform"slow" feature analysis, encouraging the representations of temporally closeframes to exhibit only small differences. While this standard approach capturesthe fact that high-level visual signals change slowly over time, it fails tocapture *how* the visual content changes. We propose to generalize slow featureanalysis to "steady" feature analysis. The key idea is to impose a prior thathigher order derivatives in the learned feature space must be small. To thisend, we train a convolutional neural network with a regularizer on tuples ofsequential frames from unlabeled video. It encourages feature changes over timeto be smooth, i.e., similar to the most recent changes. Using five diversedatasets, including unlabeled YouTube and KITTI videos, we demonstrate ourmethod's impact on object, scene, and action recognition tasks. We further showthat our features learned from unlabeled video can even surpass a standardheavily supervised pretraining approach.

Summary Transfer: Exemplar-based Subset Selection for Video  Summarization

  Video summarization has unprecedented importance to help us digest, browse,and search today's ever-growing video collections. We propose a novel subsetselection technique that leverages supervision in the form of human-createdsummaries to perform automatic keyframe-based video summarization. The mainidea is to nonparametrically transfer summary structures from annotated videosto unseen test videos. We show how to extend our method to exploit semanticside information about the video's category/genre to guide the transfer processby those training videos semantically consistent with the test input. We alsoshow how to generalize our method to subshot-based summarization, which notonly reduces computational costs but also provides more flexible ways ofdefining visual similarity across subshots spanning several frames. We conductextensive evaluation on several benchmarks and demonstrate promising results,outperforming existing methods in several settings.

Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video

  Understanding the camera wearer's activity is central to egocentric vision,yet one key facet of that activity is inherently invisible to the camera--thewearer's body pose. Prior work focuses on estimating the pose of hands and armswhen they come into view, but this 1) gives an incomplete view of the full bodyposture, and 2) prevents any pose estimate at all in many frames, since thehands are only visible in a fraction of daily life activities. We propose toinfer the "invisible pose" of a person behind the egocentric camera. Given asingle video, our efficient learning-based approach returns the full body 3Djoint positions for each frame. Our method exploits cues from the dynamicmotion signatures of the surrounding scene--which changes predictably as afunction of body pose--as well as static scene structures that reveal theviewpoint (e.g., sitting vs. standing). We further introduce a novel energyminimization scheme to infer the pose sequence. It uses soft predictions of theposes per time instant together with a non-parametric model of human posedynamics over longer windows. Our method outperforms an array of possiblealternatives, including deep learning approaches for direct pose regressionfrom images.

Leaving Some Stones Unturned: Dynamic Feature Prioritization for  Activity Detection in Streaming Video

  Current approaches for activity recognition often ignore constraints oncomputational resources: 1) they rely on extensive feature computation toobtain rich descriptors on all frames, and 2) they assume batch-mode access tothe entire test video at once. We propose a new active approach to activityrecognition that prioritizes "what to compute when" in order to make timelypredictions. The main idea is to learn a policy that dynamically schedules thesequence of features to compute on selected frames of a given test video. Incontrast to traditional static feature selection, our approach continuallyre-prioritizes computation based on the accumulated history of observations andaccounts for the transience of those observations in ongoing video. We developvariants to handle both the batch and streaming settings. On two challengingdatasets, our method provides significantly better accuracy than alternativetechniques for a wide range of computational budgets.

Detecting Engagement in Egocentric Video

  In a wearable camera video, we see what the camera wearer sees. While thismakes it easy to know roughly what he chose to look at, it does not immediatelyreveal when he was engaged with the environment. Specifically, at what momentsdid his focus linger, as he paused to gather more information about somethinghe saw? Knowing this answer would benefit various applications in videosummarization and augmented reality, yet prior work focuses solely on the"what" question (estimating saliency, gaze) without considering the "when"(engagement). We propose a learning-based approach that uses long-termegomotion cues to detect engagement, specifically in browsing scenarios whereone frequently takes in new visual information (e.g., shopping, touring). Weintroduce a large, richly annotated dataset for ego-engagement that is thefirst of its kind. Our approach outperforms a wide array of existing methods.We show engagement can be detected well independent of both scene appearanceand the camera wearer's identity.

Video Summarization with Long Short-term Memory

  We propose a novel supervised learning technique for summarizing videos byautomatically selecting keyframes or key subshots. Casting the problem as astructured prediction problem on sequential data, our main idea is to use LongShort-Term Memory (LSTM), a special type of recurrent neural networks to modelthe variable-range dependencies entailed in the task of video summarization.Our learning models attain the state-of-the-art results on two benchmark videodatasets. Detailed analysis justifies the design of the models. In particular,we show that it is crucial to take into consideration the sequential structuresin videos and model them. Besides advances in modeling techniques, we introducetechniques to address the need of a large number of annotated data for trainingcomplex learning models. There, our main idea is to exploit the existence ofauxiliary annotated video datasets, albeit heterogeneous in visual styles andcontents. Specifically, we show domain adaptation techniques can improvesummarization by reducing the discrepancies in statistical properties acrossthose datasets.

Efficient Activity Detection in Untrimmed Video with Max-Subgraph Search

  We propose an efficient approach for activity detection in video that unifiesactivity categorization with space-time localization. The main idea is to poseactivity detection as a maximum-weight connected subgraph problem. Offline, welearn a binary classifier for an activity category using positive videoexemplars that are "trimmed" in time to the activity of interest. Then, given anovel \emph{untrimmed} video sequence, we decompose it into a 3D array ofspace-time nodes, which are weighted based on the extent to which theircomponent features support the learned activity model. To perform detection, wethen directly localize instances of the activity by solving for themaximum-weight connected subgraph in the test video's space-time graph. We showthat this detection strategy permits an efficient branch-and-cut solution forthe best-scoring---and possibly non-cubically shaped---portion of the video fora given activity classifier. The upshot is a fast method that can search abroader space of space-time region candidates than was previously practical,which we find often leads to more accurate detection. We demonstrate theproposed algorithm on four datasets, and we show its speed and accuracyadvantages over multiple existing search strategies.

Visual Question: Predicting If a Crowd Will Agree on the Answer

  Visual question answering (VQA) systems are emerging from a desire to empowerusers to ask any natural language question about visual content and receive avalid answer in response. However, close examination of the VQA problem revealsan unavoidable, entangled problem that multiple humans may or may not alwaysagree on a single answer to a visual question. We train a model toautomatically predict from a visual question whether a crowd would agree on asingle answer. We then propose how to exploit this system in a novelapplication to efficiently allocate human effort to collect answers to visualquestions. Specifically, we propose a crowdsourcing system that automaticallysolicits fewer human responses when answer agreement is expected and more humanresponses when answer disagreement is expected. Our system improves uponexisting crowdsourcing systems, typically eliminating at least 20% of humaneffort with no loss to the information collected from the crowd.

Crowdsourcing in Computer Vision

  Computer vision systems require large amounts of manually annotated data toproperly learn challenging visual concepts. Crowdsourcing platforms offer aninexpensive method to capture human knowledge and understanding, for a vastnumber of visual perception tasks. In this survey, we describe the types ofannotations computer vision researchers have collected using crowdsourcing, andhow they have ensured that this data is of high quality while annotation effortis minimized. We begin by discussing data collection on both classic (e.g.,object recognition) and recent (e.g., visual story-telling) vision tasks. Wethen summarize key design decisions for creating effective data collectioninterfaces and workflows, and present strategies for intelligently selectingthe most important data instances to annotate. Finally, we conclude with somethoughts on the future of crowdsourcing in computer vision.

Object-Centric Representation Learning from Unlabeled Videos

  Supervised (pre-)training currently yields state-of-the-art performance forrepresentation learning for visual recognition, yet it comes at the cost of (1)intensive manual annotations and (2) an inherent restriction in the scope ofdata relevant for learning. In this work, we explore unsupervised featurelearning from unlabeled video. We introduce a novel object-centric approach totemporal coherence that encourages similar representations to be learned forobject-like regions segmented from nearby frames. Our framework relies on aSiamese-triplet network to train a deep convolutional neural network (CNN)representation. Compared to existing temporal coherence methods, our idea hasthe advantage of lightweight preprocessing of the unlabeled video (no trackingrequired) while still being able to extract object-level regions from which tolearn invariances. Furthermore, as we show in results on several standarddatasets, our method typically achieves substantial accuracy gains overcompeting unsupervised methods for image classification and retrieval tasks.

On-Demand Learning for Deep Image Restoration

  While machine learning approaches to image restoration offer great promise,current methods risk training models fixated on performing well only for imagecorruption of a particular level of difficulty---such as a certain level ofnoise or blur. First, we examine the weakness of conventional "fixated" modelsand demonstrate that training general models to handle arbitrary levels ofcorruption is indeed non-trivial. Then, we propose an on-demand learningalgorithm for training image restoration models with deep convolutional neuralnetworks. The main idea is to exploit a feedback mechanism to self-generatetraining instances where they are needed most, thereby learning models that cangeneralize across difficulty levels. On four restoration tasks---imageinpainting, pixel interpolation, image deblurring, and image denoising---andthree diverse datasets, our approach consistently outperforms both the statusquo training procedure and curriculum learning alternatives.

Pano2Vid: Automatic Cinematography for Watching 360$^{\circ}$ Videos

  We introduce the novel task of Pano2Vid $-$ automatic cinematography inpanoramic 360$^{\circ}$ videos. Given a 360$^{\circ}$ video, the goal is todirect an imaginary camera to virtually capture natural-looking normalfield-of-view (NFOV) video. By selecting "where to look" within the panorama ateach time step, Pano2Vid aims to free both the videographer and the end viewerfrom the task of determining what to watch. Towards this goal, we first compilea dataset of 360$^{\circ}$ videos downloaded from the web, together withhuman-edited NFOV camera trajectories to facilitate evaluation. Next, wepropose AutoCam, a data-driven approach to solve the Pano2Vid task. AutoCamleverages NFOV web video to discriminatively identify space-time "glimpses" ofinterest at each time instant, and then uses dynamic programming to selectoptimal human-like camera trajectories. Through experimental evaluation onmultiple newly defined Pano2Vid performance measures against several baselines,we show that our method successfully produces informative videos that couldconceivably have been captured by human videographers.

Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic  Images

  Distinguishing subtle differences in attributes is valuable, yet learning tomake visual comparisons remains non-trivial. Not only is the number of possiblecomparisons quadratic in the number of training images, but also access toimages adequately spanning the space of fine-grained visual differences islimited. We propose to overcome the sparsity of supervision problem viasynthetically generated images. Building on a state-of-the-art image generationengine, we sample pairs of training images exhibiting slight modifications ofindividual attributes. Augmenting real training image pairs with theseexamples, we then train attribute ranking models to predict the relativestrength of an attribute in novel pairs of real images. Our results on datasetsof faces and fashion images show the great promise of bootstrapping imperfectimage generators to counteract sample sparsity for learning to rank.

Pixel Objectness

  We propose an end-to-end learning framework for generating foreground objectsegmentations. Given a single novel image, our approach produces pixel-levelmasks for all "object-like" regions---even for object categories never seenduring training. We formulate the task as a structured prediction problem ofassigning foreground/background labels to all pixels, implemented using a deepfully convolutional network. Key to our idea is training with a mix ofimage-level object category examples together with relatively few images withboundary-level annotations. Our method substantially improves thestate-of-the-art on foreground segmentation for ImageNet and MIT ObjectDiscovery datasets. Furthermore, on over 1 million images, we show that itgeneralizes well to segment object categories unseen in the foreground mapsused for training. Finally, we demonstrate how our approach benefits imageretrieval and image retargeting, both of which flourish when given ourhigh-quality foreground maps.

FusionSeg: Learning to combine motion and appearance for fully automatic  segmention of generic objects in videos

  We propose an end-to-end learning framework for segmenting generic objects invideos. Our method learns to combine appearance and motion information toproduce pixel level segmentation masks for all prominent objects in videos. Weformulate this task as a structured prediction problem and design a two-streamfully convolutional neural network which fuses together motion and appearancein a unified framework. Since large-scale video datasets with pixel levelsegmentations are problematic, we show how to bootstrap weakly annotated videostogether with existing image recognition datasets for training. Throughexperiments on three challenging video segmentation benchmarks, our methodsubstantially improves the state-of-the-art for segmenting generic (unseen)objects. Code and pre-trained models are available on the project website.

Making 360$^{\circ}$ Video Watchable in 2D: Learning Videography for  Click Free Viewing

  360$^{\circ}$ video requires human viewers to actively control "where" tolook while watching the video. Although it provides a more immersive experienceof the visual content, it also introduces additional burden for viewers;awkward interfaces to navigate the video lead to suboptimal viewingexperiences. Virtual cinematography is an appealing direction to remedy theseproblems, but conventional methods are limited to virtual environments or relyon hand-crafted heuristics. We propose a new algorithm for virtualcinematography that automatically controls a virtual camera within a360$^{\circ}$ video. Compared to the state of the art, our algorithm allowsmore general camera control, avoids redundant outputs, and extracts its outputvideos substantially more efficiently. Experimental results on over 7 hours ofreal "in the wild" video show that our generalized camera control is crucialfor viewing 360$^{\circ}$ video, while the proposed efficient algorithm isessential for making the generalized control computationally tractable.

Fashion Forward: Forecasting Visual Style in Fashion

  What is the future of fashion? Tackling this question from a data-drivenvision perspective, we propose to forecast visual style trends before theyoccur. We introduce the first approach to predict the future popularity ofstyles discovered from fashion images in an unsupervised manner. Using thesestyles as a basis, we train a forecasting model to represent their trends overtime. The resulting model can hypothesize new mixtures of styles that willbecome popular in the future, discover style dynamics (trendy vs. classic), andname the key visual attributes that will dominate tomorrow's fashion. Wedemonstrate our idea applied to three datasets encapsulating 80,000 fashionproducts sold across six years on Amazon. Results indicate that fashionforecasting benefits greatly from visual analysis, much more than textual ormeta-data cues surrounding products.

Learning the Latent "Look": Unsupervised Discovery of a Style-Coherent  Embedding from Fashion Images

  What defines a visual style? Fashion styles emerge organically from howpeople assemble outfits of clothing, making them difficult to pin down with acomputational model. Low-level visual similarity can be too specific to detectstylistically similar images, while manually crafted style categories can betoo abstract to capture subtle style differences. We propose an unsupervisedapproach to learn a style-coherent representation. Our method leveragesprobabilistic polylingual topic models based on visual attributes to discover aset of latent style factors. Given a collection of unlabeled fashion images,our approach mines for the latent styles, then summarizes outfits by how theymix those styles. Our approach can organize galleries of outfits by stylewithout requiring any style labels. Experiments on over 100K images demonstrateits promise for retrieving, mixing, and summarizing fashion images by theirstyle.

ShapeCodes: Self-Supervised Feature Learning by Lifting Views to  Viewgrids

  We introduce an unsupervised feature learning approach that embeds 3D shapeinformation into a single-view image representation. The main idea is aself-supervised training objective that, given only a single 2D image, requiresall unseen views of the object to be predictable from learned features. Weimplement this idea as an encoder-decoder convolutional neural network. Thenetwork maps an input image of an unknown category and unknown viewpoint to alatent space, from which a deconvolutional decoder can best "lift" the image toits complete viewgrid showing the object from all viewing angles. Ourclass-agnostic training procedure encourages the representation to capturefundamental shape primitives and semantic regularities in a data-drivenmanner---without manual semantic labels. Our results on two widely-used shapedatasets show 1) our approach successfully learns to perform "mental rotation"even for objects unseen during training, and 2) the learned latent space is apowerful representation for object recognition, outperforming several existingunsupervised feature learning methods.

Learning to Look Around: Intelligently Exploring Unseen Environments for  Unknown Tasks

  It is common to implicitly assume access to intelligently captured inputs(e.g., photos from a human photographer), yet autonomously capturing goodobservations is itself a major challenge. We address the problem of learning tolook around: if a visual agent has the ability to voluntarily acquire new viewsto observe its environment, how can it learn efficient exploratory behaviors toacquire informative observations? We propose a reinforcement learning solution,where the agent is rewarded for actions that reduce its uncertainty about theunobserved portions of its environment. Based on this principle, we develop arecurrent neural network-based approach to perform active completion ofpanoramic natural scenes and 3D object shapes. Crucially, the learned policiesare not tied to any recognition task nor to the particular semantic contentseen during training. As a result, 1) the learned "look around" behavior isrelevant even for new tasks in unseen environments, and 2) training dataacquisition involves no manual labeling. Through tests in diverse settings, wedemonstrate that our approach learns useful generic policies that transfer tonew unseen tasks and environments. Completion episodes are shown athttps://goo.gl/BgWX3W.

Creating Capsule Wardrobes from Fashion Images

  We propose to automatically create capsule wardrobes. Given an inventory ofcandidate garments and accessories, the algorithm must assemble a minimal setof items that provides maximal mix-and-match outfits. We pose the task as asubset selection problem. To permit efficient subset selection over the spaceof all outfit combinations, we develop submodular objective functions capturingthe key ingredients of visual compatibility, versatility, and user-specificpreference. Since adding garments to a capsule only expands its possibleoutfits, we devise an iterative approach to allow near-optimal submodularfunction maximization. Finally, we present an unsupervised approach to learnvisual compatibility from "in the wild" full body outfit photos; thecompatibility metric translates well to cleaner catalog photos and improvesover existing methods. Our results on thousands of pieces from popular fashionwebsites show that automatic capsule creation has potential to mimic skilledfashionistas in assembling flexible wardrobes, while being significantly morescalable.

Learning Compressible 360Â° Video Isomers

  Standard video encoders developed for conventional narrow field-of-view videoare widely applied to 360{\deg} video as well, with reasonable results.However, while this approach commits arbitrarily to a projection of thespherical frames, we observe that some orientations of a 360{\deg} video, onceprojected, are more compressible than others. We introduce an approach topredict the sphere rotation that will yield the maximal compression rate. Givenvideo clips in their original encoding, a convolutional neural network learnsthe association between a clip's visual content and its compressibility atdifferent rotations of a cubemap projection. Given a novel video, ourlearning-based approach efficiently infers the most compressible direction inone shot, without repeated rendering and compression of the source video. Wevalidate our idea on thousands of video clips and multiple popular videocodecs. The results show that this untapped dimension of 360{\deg} compressionhas substantial potential--"good" rotations are typically 8-10% morecompressible than bad ones, and our learning approach can predict them reliably82% of the time.

VizWiz Grand Challenge: Answering Visual Questions from Blind People

  The study of algorithms to automatically answer visual questions currently ismotivated by visual question answering (VQA) datasets constructed in artificialVQA settings. We propose VizWiz, the first goal-oriented VQA dataset arisingfrom a natural VQA setting. VizWiz consists of over 31,000 visual questionsoriginating from blind people who each took a picture using a mobile phone andrecorded a spoken question about it, together with 10 crowdsourced answers pervisual question. VizWiz differs from the many existing VQA datasets because (1)images are captured by blind photographers and so are often poor quality, (2)questions are spoken and so are more conversational, and (3) often visualquestions cannot be answered. Evaluation of modern algorithms for answeringvisual questions and deciding if a visual question is answerable reveals thatVizWiz is a challenging dataset. We introduce this dataset to encourage alarger community to develop more generalized algorithms that can assist blindpeople.

Attributes as Operators: Factorizing Unseen Attribute-Object  Compositions

  We present a new approach to modeling visual attributes. Prior work castsattributes in a similar role as objects, learning a latent representation whereproperties (e.g., sliced) are recognized by classifiers much in the way objects(e.g., apple) are. However, this common approach fails to separate theattributes observed during training from the objects with which they arecomposed, making it ineffectual when encountering new attribute-objectcompositions. Instead, we propose to model attributes as operators. Ourapproach learns a semantic embedding that explicitly factors out attributesfrom their accompanying objects, and also benefits from novel regularizersexpressing attribute operators' effects (e.g., blunt should undo the effects ofsharp). Not only does our approach align conceptually with the linguistic roleof attributes as modifiers, but it also generalizes to recognize unseencompositions of objects and attributes. We validate our approach on twochallenging datasets and demonstrate significant improvements over thestate-of-the-art. In addition, we show that not only can our model recognizeunseen compositions robustly in an open-world setting, it can also generalizeto compositions where objects themselves were unseen during training.

Compare and Contrast: Learning Prominent Visual Differences

  Relative attribute models can compare images in terms of all detectedproperties or attributes, exhaustively predicting which image is fancier, morenatural, and so on without any regard to ordering. However, when humans compareimages, certain differences will naturally stick out and come to mind first.These most noticeable differences, or prominent differences, are likely to bedescribed first. In addition, many differences, although present, may not bementioned at all. In this work, we introduce and model prominent differences, arich new functionality for comparing images. We collect instance-levelannotations of most noticeable differences, and build a model trained onrelative attribute features that predicts prominent differences for unseenpairs. We test our model on the challenging UT-Zap50K shoes and LFW10 facesdatasets, and outperform an array of baseline methods. We then demonstrate howour prominence model improves two vision tasks, image search and descriptiongeneration, enabling more natural communication between people and visionsystems.

Snap Angle Prediction for 360$^{\circ}$ Panoramas

  360$^{\circ}$ panoramas are a rich medium, yet notoriously difficult tovisualize in the 2D image plane. We explore how intelligent rotations of aspherical image may enable content-aware projection with fewer perceptibledistortions. Whereas existing approaches assume the viewpoint is fixed,intuitively some viewing angles within the sphere preserve high-level objectsbetter than others. To discover the relationship between these optimal snapangles and the spherical panorama's content, we develop a reinforcementlearning approach for the cubemap projection model. Implemented as a deeprecurrent neural network, our method selects a sequence of rotation actions andreceives reward for avoiding cube boundaries that overlap with importantforeground objects. We show our approach creates more visually pleasingpanoramas while using 5x less computation than the baseline.

Learning to Separate Object Sounds by Watching Unlabeled Video

  Perceiving a scene most fully requires all the senses. Yet modeling howobjects look and sound is challenging: most natural scenes and events containmultiple objects, and the audio track mixes all the sound sources together. Wepropose to learn audio-visual object models from unlabeled video, then exploitthe visual context to perform audio source separation in novel videos. Ourapproach relies on a deep multi-instance multi-label learning framework todisentangle the audio frequency bases that map to individual visual objects,even without observing/hearing those objects in isolation. We show how therecovered disentangled bases can be used to guide audio source separation toobtain better-separated, object-level sounds. Our work is the first to learnaudio source separation from large-scale "in the wild" videos containingmultiple audio sources per video. We obtain state-of-the-art results onvisually-aided audio source separation and audio denoising. Our video results:http://vision.cs.utexas.edu/projects/separating_object_sounds/

Sidekick Policy Learning for Active Visual Exploration

  We consider an active visual exploration scenario, where an agent mustintelligently select its camera motions to efficiently reconstruct the fullenvironment from only a limited set of narrow field-of-view glimpses. While theagent has full observability of the environment during training, it has onlypartial observability once deployed, being constrained by what portions it hasseen and what camera motions are permissible. We introduce sidekick policylearning to capitalize on this imbalance of observability. The main idea is apreparatory learning phase that attempts simplified versions of the eventualexploration task, then guides the agent via reward shaping or initial policysupervision. To support interpretation of the resulting policies, we alsodevelop a novel policy visualization technique. Results on active visualexploration tasks with 360 scenes and 3D objects show that sidekicksconsistently improve performance and convergence rates over existing methods.Code, data and demos are available.

SpotTune: Transfer Learning through Adaptive Fine-tuning

  Transfer learning, which allows a source task to affect the inductive bias ofthe target task, is widely used in computer vision. The typical way ofconducting transfer learning with deep neural networks is to fine-tune a modelpre-trained on the source task using data from the target task. In this paper,we propose an adaptive fine-tuning approach, called SpotTune, which finds theoptimal fine-tuning strategy per instance for the target data. In SpotTune,given an image from the target task, a policy network is used to make routingdecisions on whether to pass the image through the fine-tuned layers or thepre-trained layers. We conduct extensive experiments to demonstrate theeffectiveness of the proposed approach. Our method outperforms the traditionalfine-tuning approach on 12 out of 14 standard datasets.We also compare SpotTunewith other state-of-the-art fine-tuning strategies, showing superiorperformance. On the Visual Decathlon datasets, our method achieves the highestscore across the board without bells and whistles.

2.5D Visual Sound

  Binaural audio provides a listener with 3D sound sensation, allowing a richperceptual experience of the scene. However, binaural recordings are scarcelyavailable and require nontrivial expertise and equipment to obtain. We proposeto convert common monaural audio into binaural audio by leveraging video. Thekey idea is that visual frames reveal significant spatial cues that, whileexplicitly lacking in the accompanying single-channel audio, are stronglylinked to it. Our multi-modal approach recovers this link from unlabeled video.We devise a deep convolutional neural network that learns to decode themonaural (single-channel) soundtrack into its binaural counterpart by injectingvisual information about object and scene configurations. We call the resultingoutput 2.5D visual sound---the visual stream helps "lift" the flat singlechannel audio into spatialized sound. In addition to sound generation, we showthe self-supervised representation learned by our network benefits audio-visualsource separation. Our video results:http://vision.cs.utexas.edu/projects/2.5D_visual_sound/

Grounded Human-Object Interaction Hotspots from Video

  Learning how to interact with objects is an important step towards embodiedvisual intelligence, but existing techniques suffer from heavy supervision orsensing requirements. We propose an approach to learn human-object interaction"hotspots" directly from video. Rather than treat affordances as a manuallysupervised semantic segmentation task, our approach learns about interactionsby watching videos of real human behavior and anticipating afforded actions.Given a novel image or video, our model infers a spatial hotspot map indicatinghow an object would be manipulated in a potential interaction-- even if theobject is currently at rest. Through results with both first and third personvideo, we show the value of grounding affordances in real human-objectinteractions. Not only are our weakly supervised hotspots competitive withstrongly supervised affordance methods, but they can also anticipate objectinteraction for novel object categories.

Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion

  Estimating the relative rigid pose between two RGB-D scans of the sameunderlying environment is a fundamental problem in computer vision, robotics,and computer graphics. Most existing approaches allow only limited maximumrelative pose changes since they require considerable overlap between the inputscans. We introduce a novel deep neural network that extends the scope toextreme relative poses, with little or even no overlap between the input scans.The key idea is to infer more complete scene information about the underlyingenvironment and match on the completed scans. In particular, instead of onlyperforming scene completion from each individual scan, our approach alternatesbetween relative pose estimation and scene completion. This allows us toperform scene completion by utilizing information from both input scans at lateiterations, resulting in better results for both scene completion and relativepose estimation. Experimental results on benchmark datasets show that ourapproach leads to considerable improvements over state-of-the-art approachesfor relative pose estimation. In particular, our approach provides encouragingrelative pose estimates even between non-overlapping scans.

Thinking Outside the Pool: Active Training Image Creation for Relative  Attributes

  Current wisdom suggests more labeled image data is always better, andobtaining labels is the bottleneck. Yet curating a pool of sufficiently diverseand informative images is itself a challenge. In particular, training imagecuration is problematic for fine-grained attributes, where the subtle visualdifferences of interest may be rare within traditional image sources. Wepropose an active image generation approach to address this issue. The mainidea is to jointly learn the attribute ranking task while also learning togenerate novel realistic image samples that will benefit that task. Weintroduce an end-to-end framework that dynamically "imagines" image pairs thatwould confuse the current model, presents them to human annotators forlabeling, then improves the predictive model with the new examples. Withresults on two datasets, we show that by thinking outside the pool of realimages, our approach gains generalization accuracy for challenging fine-grainedattribute comparisons.

Less is More: Learning Highlight Detection from Video Duration

  Highlight detection has the potential to significantly ease video browsing,but existing methods often suffer from expensive supervision requirements,where human viewers must manually identify highlights in training videos. Wepropose a scalable unsupervised solution that exploits video duration as animplicit supervision signal. Our key insight is that video segments fromshorter user-generated videos are more likely to be highlights than those fromlonger videos, since users tend to be more selective about the content whencapturing shorter videos. Leveraging this insight, we introduce a novel rankingframework that prefers segments from shorter videos, while properly accountingfor the inherent noise in the (unlabeled) training data. We use it to train ahighlight detector with 10M hashtagged Instagram videos. In experiments on twochallenging public video highlight detection benchmarks, our methodsubstantially improves the state-of-the-art for unsupervised highlightdetection.

Next-Active-Object prediction from Egocentric Videos

  Although First Person Vision systems can sense the environment from theuser's perspective, they are generally unable to predict his intentions andgoals. Since human activities can be decomposed in terms of atomic actions andinteractions with objects, intelligent wearable systems would benefit from theability to anticipate user-object interactions. Even if this task is nottrivial, the First Person Vision paradigm can provide important cues to addressthis challenge. We propose to exploit the dynamics of the scene to recognizenext-active-objects before an object interaction begins. We train a classifierto discriminate trajectories leading to an object activation from all othersand forecast next-active-objects by analyzing fixed-length trajectory segmentswithin a temporal sliding window. The proposed method compares favorably withrespect to several baselines on the Activity of Daily Living (ADL) egocentricdataset comprising 10 hours of videos acquired by 20 subjects while performingunconstrained interactions with several objects.

Predicting Important Objects for Egocentric Video Summarization

  We present a video summarization approach for egocentric or "wearable" cameradata. Given hours of video, the proposed method produces a compact storyboardsummary of the camera wearer's day. In contrast to traditional keyframeselection techniques, the resulting summary focuses on the most importantobjects and people with which the camera wearer interacts. To accomplish this,we develop region cues indicative of high-level saliency in egocentricvideo---such as the nearness to hands, gaze, and frequency of occurrence---andlearn a regressor to predict the relative importance of any new region based onthese cues. Using these predictions and a simple form of temporal eventdetection, our method selects frames for the storyboard that reflect the keyobject-driven happenings. We adjust the compactness of the final summary giveneither an importance selection criterion or a length budget; for the latter, wedesign an efficient dynamic programming solution that accounts for importance,visual uniqueness, and temporal displacement. Critically, the approach isneither camera-wearer-specific nor object-specific; that means the learnedimportance metric need not be trained for a given user or context, and it canpredict the importance of objects and people that have never been seenpreviously. Our results on two egocentric video datasets show the method'spromise relative to existing techniques for saliency and summarization.

Video Analysis for Body-worn Cameras in Law Enforcement

  The social conventions and expectations around the appropriate use of imagingand video has been transformed by the availability of video cameras in ourpockets. The impact on law enforcement can easily be seen by watching thenightly news; more and more arrests, interventions, or even routine stops arebeing caught on cell phones or surveillance video, with both positive andnegative consequences. This proliferation of the use of video has led lawenforcement to look at the potential benefits of incorporating video capturesystematically in their day to day operations. At the same time, recognition ofthe inevitability of widespread use of video for police operations has caused arush to deploy all types of cameras, including body worn cameras. However, thevast majority of police agencies have limited experience in utilizing video toits full advantage, and thus do not have the capability to fully realize thevalue of expanding their video capabilities. In this white paper, we highlightsome of the technology needs and challenges of body-worn cameras, and we relatethese needs to the relevant state of the art in computer vision and multimediaresearch. We conclude with a set of recommendations.

Detangling People: Individuating Multiple Close People and Their Body  Parts via Region Assembly

  Today's person detection methods work best when people are in common uprightposes and appear reasonably well spaced out in the image. However, in many realimages, that's not what people do. People often appear quite close to eachother, e.g., with limbs linked or heads touching, and their poses are often notpedestrian-like. We propose an approach to detangle people in multi-personimages. We formulate the task as a region assembly problem. Starting from alarge set of overlapping regions from body part semantic segmentation andgeneric object proposals, our optimization approach reassembles those piecestogether into multiple person instances. It enforces that the composed bodypart regions of each person instance obey constraints on relative sizes, mutualspatial relationships, foreground coverage, and exclusive label assignmentswhen overlapping. Since optimal region assembly is a challenging combinatorialproblem, we present a Lagrangian relaxation method to accelerate the lowerbound estimation, thereby enabling a fast branch and bound solution for theglobal optimum. As output, our method produces a pixel-level map indicatingboth 1) the body part labels (arm, leg, torso, and head), and 2) which partsbelong to which individual person. Our results on three challenging datasetsshow our method is robust to clutter, occlusion, and complex poses. Itoutperforms a variety of competing methods, including existing detector CRFmethods and region CNN approaches. In addition, we demonstrate its impact on aproxemics recognition task, which demands a precise representation of "whosebody part is where" in crowded images.

Subjects and Their Objects: Localizing Interactees for a Person-Centric  View of Importance

  Understanding images with people often entails understanding their\emph{interactions} with other objects or people. As such, given a novel image,a vision system ought to infer which other objects/people play an importantrole in a given person's activity. However, existing methods are limited tolearning action-specific interactions (e.g., how the pose of a tennis playerrelates to the position of his racquet when serving the ball) for improvedrecognition, making them unequipped to reason about novel interactions withactions or objects unobserved in the training data.  We propose to predict the "interactee" in novel images---that is, to localizethe \emph{object} of a person's action. Given an arbitrary image with adetected person, the goal is to produce a saliency map indicating the mostlikely positions and scales where that person's interactee would be found. Tothat end, we explore ways to learn the generic, action-independent connectionsbetween (a) representations of a person's pose, gaze, and scene cues and (b)the interactee object's position and scale. We provide results on a newlycollected UT Interactee dataset spanning more than 10,000 images from SUN,PASCAL, and COCO. We show that the proposed interaction-informed saliencymetric has practical utility for four tasks: contextual object detection, imageretargeting, predicting object importance, and data-driven natural languagescene description. All four scenarios reveal the value in linking the subjectto its object in order to understand the story of an image.

Look-ahead before you leap: end-to-end active recognition by forecasting  the effect of motion

  Visual recognition systems mounted on autonomous moving agents face thechallenge of unconstrained data, but simultaneously have the opportunity toimprove their performance by moving to acquire new views of test data. In thiswork, we first show how a recurrent neural network-based system may be trainedto perform end-to-end learning of motion policies suited for this "activerecognition" setting. Further, we hypothesize that active vision requires anagent to have the capacity to reason about the effects of its motions on itsview of the world. To verify this hypothesis, we attempt to induce thiscapacity in our active recognition pipeline, by simultaneously learning toforecast the effects of the agent's motions on its internal representation ofthe environment conditional on all past views. Results across two challengingdatasets confirm both that our end-to-end system successfully learns meaningfulpolicies for active category recognition, and that "learning to look ahead"further boosts recognition performance.

Click Carving: Segmenting Objects in Video with Point Clicks

  We present a novel form of interactive video object segmentation where a fewclicks by the user helps the system produce a full spatio-temporal segmentationof the object of interest. Whereas conventional interactive pipelines take theuser's initialization as a starting point, we show the value in the systemtaking the lead even in initialization. In particular, for a given video frame,the system precomputes a ranked list of thousands of possible segmentationhypotheses (also referred to as object region proposals) using image and motioncues. Then, the user looks at the top ranked proposals, and clicks on theobject boundary to carve away erroneous ones. This process iterates (typically2-3 times), and each time the system revises the top ranked proposal set, untilthe user is satisfied with a resulting segmentation mask. Finally, the mask ispropagated across the video to produce a spatio-temporal object tube. On threechallenging datasets, we provide extensive comparisons with both existing workand simpler alternative methods. In all, the proposed Click Carving approachstrikes an excellent balance of accuracy and human effort. It outperforms allsimilarly fast methods, and is competitive or better than those requiring 2 to12 times the effort.

Predicting Foreground Object Ambiguity and Efficiently Crowdsourcing the  Segmentation(s)

  We propose the ambiguity problem for the foreground object segmentation taskand motivate the importance of estimating and accounting for this ambiguitywhen designing vision systems. Specifically, we distinguish between imageswhich lead multiple annotators to segment different foreground objects(ambiguous) versus minor inter-annotator differences of the same object. Takingimages from eight widely used datasets, we crowdsource labeling the images as"ambiguous" or "not ambiguous" to segment in order to construct a new datasetwe call STATIC. Using STATIC, we develop a system that automatically predictswhich images are ambiguous. Experiments demonstrate the advantage of ourprediction system over existing saliency-based methods on images from visionbenchmarks and images taken by blind people who are trying to recognize objectsin their environment. Finally, we introduce a crowdsourcing system to achievecost savings for collecting the diversity of all valid "ground truth"foreground object segmentations by collecting extra segmentations only whenambiguity is expected. Experiments show our system eliminates up to 47% ofhuman effort compared to existing crowdsourcing methods with no loss incapturing the diversity of ground truths.

Learning Spherical Convolution for Fast Features from 360Â° Imagery

  While 360{\deg} cameras offer tremendous new possibilities in vision,graphics, and augmented reality, the spherical images they produce make corefeature extraction non-trivial. Convolutional neural networks (CNNs) trained onimages from perspective cameras yield "flat" filters, yet 360{\deg} imagescannot be projected to a single plane without significant distortion. A naivesolution that repeatedly projects the viewing sphere to all tangent planes isaccurate, but much too computationally intensive for real problems. We proposeto learn a spherical convolutional network that translates a planar CNN toprocess 360{\deg} imagery directly in its equirectangular projection. Ourapproach learns to reproduce the flat filter outputs on 360{\deg} data,sensitive to the varying distortion effects across the viewing sphere. The keybenefits are 1) efficient feature extraction for 360{\deg} images and video,and 2) the ability to leverage powerful pre-trained networks researchers havecarefully honed (together with massive labeled image training sets) forperspective images. We validate our approach compared to several alternativemethods in terms of both raw CNN output accuracy as well as applying astate-of-the-art "flat" object detector to 360{\deg} data. Our method yieldsthe most accurate results while saving orders of magnitude in computationversus the existing exact reprojection solution.

BlockDrop: Dynamic Inference Paths in Residual Networks

  Very deep convolutional neural networks offer excellent recognition results,yet their computational expense limits their impact for many real-worldapplications. We introduce BlockDrop, an approach that learns to dynamicallychoose which layers of a deep network to execute during inference so as to bestreduce total computation without degrading prediction accuracy. Exploiting therobustness of Residual Networks (ResNets) to layer dropping, our frameworkselects on-the-fly which residual blocks to evaluate for a given novel image.In particular, given a pretrained ResNet, we train a policy network in anassociative reinforcement learning setting for the dual reward of utilizing aminimal number of blocks while preserving recognition accuracy. We conductextensive experiments on CIFAR and ImageNet. The results provide strongquantitative and qualitative evidence that these learned policies not onlyaccelerate inference but also encode meaningful visual information. Built upona ResNet-101 model, our method achieves a speedup of 20\% on average, going ashigh as 36\% for some images, while maintaining the same 76.4\% top-1 accuracyon ImageNet.

Im2Flow: Motion Hallucination from Static Images for Action Recognition

  Existing methods to recognize actions in static images take the images attheir face value, learning the appearances---objects, scenes, and bodyposes---that distinguish each action class. However, such models are deprivedof the rich dynamic structure and motions that also define human activity. Wepropose an approach that hallucinates the unobserved future motion implied by asingle snapshot to help static-image action recognition. The key idea is tolearn a prior over short-term dynamics from thousands of unlabeled videos,infer the anticipated optical flow on novel static images, and then traindiscriminative models that exploit both streams of information. Our maincontributions are twofold. First, we devise an encoder-decoder convolutionalneural network and a novel optical flow encoding that can translate a staticimage into an accurate flow map. Second, we show the power of hallucinated flowfor recognition, successfully transferring the learned motion into a standardtwo-stream network for activity recognition. On seven datasets, we demonstratethe power of the approach. It not only achieves state-of-the-art accuracy fordense optical flow prediction, but also consistently enhances recognition ofactions and dynamic scenes.

Pixel Objectness: Learning to Segment Generic Objects Automatically in  Images and Videos

  We propose an end-to-end learning framework for segmenting generic objects inboth images and videos. Given a novel image or video, our approach produces apixel-level mask for all "object-like" regions---even for object categoriesnever seen during training. We formulate the task as a structured predictionproblem of assigning an object/background label to each pixel, implementedusing a deep fully convolutional network. When applied to a video, our modelfurther incorporates a motion stream, and the network learns to combine bothappearance and motion and attempts to extract all prominent objects whetherthey are moving or not. Beyond the core model, a second contribution of ourapproach is how it leverages varying strengths of training annotations.Pixel-level annotations are quite difficult to obtain, yet crucial for traininga deep network approach for segmentation. Thus we propose ways to exploitweakly labeled data for learning dense foreground segmentation. For images, weshow the value in mixing object category examples with image-level labelstogether with relatively few images with boundary-level annotations. For video,we show how to bootstrap weakly annotated videos together with the networktrained for image segmentation. Through experiments on multiple challengingimage and video segmentation benchmarks, our method offers consistently strongresults and improves the state-of-the-art for fully automatic segmentation ofgeneric (unseen) objects. In addition, we demonstrate how our approach benefitsimage retrieval and image retargeting, both of which flourish when given ourhigh-quality foreground maps. Code, models, and videos areat:http://vision.cs.utexas.edu/projects/pixelobjectness/

