Agent Development Toolkits

  Development of agents as well as their wide usage requires good underlyinginfrastructure. Literature indicates scarcity of agent development tools ininitial years of research which limited the exploitation of this beneficialtechnology. However, today a wide variety of tools are available, fordeveloping robust infrastructure. This technical note provides a deep overviewof such tools and contrasts features provided by them.

Detecting Weak but Hierarchically-Structured Patterns in Networks

  The ability to detect weak distributed activation patterns in networks iscritical to several applications, such as identifying the onset of anomalousactivity or incipient congestion in the Internet, or faint traces of abiochemical spread by a sensor network. This is a challenging problem sinceweak distributed patterns can be invisible in per node statistics as well as aglobal network-wide aggregate. Most prior work considers situations in whichthe activation/non-activation of each node is statistically independent, butthis is unrealistic in many problems. In this paper, we consider structuredpatterns arising from statistical dependencies in the activation process. Ourcontributions are three-fold. First, we propose a sparsifying transform thatsuccinctly represents structured activation patterns that conform to ahierarchical dependency graph. Second, we establish that the proposed transformfacilitates detection of very weak activation patterns that cannot be detectedwith existing methods. Third, we show that the structure of the hierarchicaldependency graph governing the activation process, and hence the networktransform, can be learnt from very few (logarithmic in network size)independent snapshots of network activity.

Optimal rates for first-order stochastic convex optimization under  Tsybakov noise condition

  We focus on the problem of minimizing a convex function $f$ over a convex set$S$ given $T$ queries to a stochastic first order oracle. We argue that thecomplexity of convex minimization is only determined by the rate of growth ofthe function around its minimizer $x^*_{f,S}$, as quantified by a Tsybakov-likenoise condition. Specifically, we prove that if $f$ grows at least as fast as$\|x-x^*_{f,S}\|^\kappa$ around its minimum, for some $\kappa > 1$, then theoptimal rate of learning $f(x^*_{f,S})$ is$\Theta(T^{-\frac{\kappa}{2\kappa-2}})$. The classic rate $\Theta(1/\sqrt T)$for convex functions and $\Theta(1/T)$ for strongly convex functions arespecial cases of our result for $\kappa \rightarrow \infty$ and $\kappa=2$, andeven faster rates are attained for $\kappa <2$. We also derive tight bounds forthe complexity of learning $x_{f,S}^*$, where the optimal rate is$\Theta(T^{-\frac{1}{2\kappa-2}})$. Interestingly, these precise rates forconvex optimization also characterize the complexity of active learning and ourresults further strengthen the connections between the two fields, both ofwhich rely on feedback-driven queries.

On the Bootstrap for Persistence Diagrams and Landscapes

  Persistent homology probes topological properties from point clouds andfunctions. By looking at multiple scales simultaneously, one can record thebirths and deaths of topological features as the scale varies. In this paper weuse a statistical technique, the empirical bootstrap, to separate topologicalsignal from topological noise. In particular, we derive confidence sets forpersistence diagrams and confidence bands for persistence landscapes.

Subspace Learning from Extremely Compressed Measurements

  We consider learning the principal subspace of a large set of vectors from anextremely small number of compressive measurements of each vector. Ourtheoretical results show that even a constant number of measurements per columnsuffices to approximate the principal subspace to arbitrary precision, providedthat the number of vectors is large. This result is achieved by a simplealgorithm that computes the eigenvectors of an estimate of the covariancematrix. The main insight is to exploit an averaging effect that arises fromapplying a different random projection to each vector. We provide a number ofsimulations confirming our theoretical results.

Feature Selection For High-Dimensional Clustering

  We present a nonparametric method for selecting informative features inhigh-dimensional clustering problems. We start with a screening step that usesa test for multimodality. Then we apply kernel density estimation and modeclustering to the selected features. The output of the method consists of alist of relevant features, and cluster assignments. We provide explicit boundson the error rate of the resulting clustering. In addition, we provide thefirst error bounds on mode based clustering.

Risk Bounds For Mode Clustering

  Density mode clustering is a nonparametric clustering method. The clustersare the basins of attraction of the modes of a density estimator. We study therisk of mode-based clustering. We show that the clustering risk over thecluster cores --- the regions where the density is high --- is very small evenin high dimensions. And under a low noise condition, the overall cluster riskis small even beyond the cores, in high dimensions.

Signal Representations on Graphs: Tools and Applications

  We present a framework for representing and modeling data on graphs. Based onthis framework, we study three typical classes of graph signals: smooth graphsignals, piecewise-constant graph signals, and piecewise-smooth graph signals.For each class, we provide an explicit definition of the graph signals andconstruct a corresponding graph dictionary with desirable properties. We thenstudy how such graph dictionary works in two standard tasks: approximation andsampling followed with recovery, both from theoretical as well as algorithmicperspectives. Finally, for each class, we present a case study of a real-worldproblem by using the proposed methodology.

Stochastic Zeroth-order Optimization in High Dimensions

  We consider the problem of optimizing a high-dimensional convex functionusing stochastic zeroth-order queries. Under sparsity assumptions on thegradients or function values, we present two algorithms: a successivecomponent/feature selection algorithm and a noisy mirror descent algorithmusing Lasso gradient estimates, and show that both algorithms have convergencerates that de- pend only logarithmically on the ambient dimension of theproblem. Empirical results confirm our theoretical findings and show that thealgorithms we design outperform classical zeroth-order optimization methods inthe high-dimensional setting.

Adaptive Hausdorff estimation of density level sets

  Consider the problem of estimating the $\gamma$-level set$G^*_{\gamma}=\{x:f(x)\geq\gamma\}$ of an unknown $d$-dimensional densityfunction $f$ based on $n$ independent observations $X_1,...,X_n$ from thedensity. This problem has been addressed under global error criteria related tothe symmetric set difference. However, in certain applications a spatiallyuniform mode of convergence is desirable to ensure that the estimated set isclose to the target set everywhere. The Hausdorff error criterion provides thisdegree of uniformity and, hence, is more appropriate in such situations. It isknown that the minimax optimal rate of error convergence for the Hausdorffmetric is $(n/\log n)^{-1/(d+2\alpha)}$ for level sets with boundaries thathave a Lipschitz functional form, where the parameter $\alpha$ characterizesthe regularity of the density around the level of interest. However, theestimators proposed in previous work are nonadaptive to the density regularityand require knowledge of the parameter $\alpha$. Furthermore, previouslydeveloped estimators achieve the minimax optimal rate for rather restrictedclasses of sets (e.g., the boundary fragment and star-shaped sets) thateffectively reduce the set estimation problem to a function estimation problem.This characterization precludes level sets with multiple connected components,which are fundamental to many applications. This paper presents a fullydata-driven procedure that is adaptive to unknown regularity conditions andachieves near minimax optimal Hausdorff error control for a class of densitylevel sets with very general shapes and multiple connected components.

Stability of Density-Based Clustering

  High density clusters can be characterized by the connected components of alevel set $L(\lambda) = \{x:\ p(x)>\lambda\}$ of the underlying probabilitydensity function $p$ generating the data, at some appropriate level$\lambda\geq 0$. The complete hierarchical clustering can be characterized by acluster tree ${\cal T}= \bigcup_{\lambda} L(\lambda)$. In this paper, we studythe behavior of a density level set estimate $\widehat L(\lambda)$ and clustertree estimate $\widehat{\cal{T}}$ based on a kernel density estimator withkernel bandwidth $h$. We define two notions of instability to measure thevariability of $\widehat L(\lambda)$ and $\widehat{\cal{T}}$ as a function of$h$, and investigate the theoretical properties of these instability measures.

Active Clustering: Robust and Efficient Hierarchical Clustering using  Adaptively Selected Similarities

  Hierarchical clustering based on pairwise similarities is a common tool usedin a broad range of scientific applications. However, in many problems it maybe expensive to obtain or compute similarities between the items to beclustered. This paper investigates the hierarchical clustering of N items basedon a small subset of pairwise similarities, significantly less than thecomplete set of N(N-1)/2 similarities. First, we show that if the intraclustersimilarities exceed intercluster similarities, then it is possible to correctlydetermine the hierarchical clustering from as few as 3N log N similarities. Wedemonstrate this order of magnitude savings in the number of pairwisesimilarities necessitates sequentially selecting which similarities to obtainin an adaptive fashion, rather than picking them at random. We then propose anactive clustering method that is robust to a limited fraction of anomaloussimilarities, and show how even in the presence of these noisy similarityvalues we can resolve the hierarchical clustering using only O(N log^2 N)pairwise similarities.

Adaptive Semisupervised Inference

  Semisupervised methods inevitably invoke some assumption that links themarginal distribution of the features to the regression function of the label.Most commonly, the cluster or manifold assumptions are used which imply thatthe regression function is smooth over high-density clusters or manifoldssupporting the data. A generalization of these assumptions is that theregression function is smooth with respect to some density sensitive distance.This motivates the use of a density based metric for semisupervised learning.We analyze this setting and make the following contributions - (a) we propose asemi-supervised learner that uses a density-sensitive kernel and show that itprovides better performance than any supervised learner if the density supportset has a small condition number and (b) we show that it is possible to adaptto the degree of semi-supervisedness using data-dependent choice of a parameterthat controls sensitivity of the distance metric to the density. This ensuresthat the semisupervised learner never performs worse than a supervised learnereven if the assumptions fail to hold.

Minimax Rates for Homology Inference

  Often, high dimensional data lie close to a low-dimensional submanifold andit is of interest to understand the geometry of these submanifolds. Thehomology groups of a manifold are important topological invariants that providean algebraic summary of the manifold. These groups contain rich topologicalinformation, for instance, about the connected components, holes, tunnels andsometimes the dimension of the manifold. In this paper, we consider thestatistical problem of estimating the homology of a manifold from noisy samplesunder several different noise models. We derive upper and lower bounds on theminimax risk for this problem. Our upper bounds are based on estimators whichare constructed from a union of balls of appropriate radius around carefullyselected points. In each case we establish complementary lower bounds using LeCam's lemma.

Density-sensitive semisupervised inference

  Semisupervised methods are techniques for using labeled data$(X_1,Y_1),\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\ldots,X_N$to make predictions. These methods invoke some assumptions that link themarginal distribution $P_X$ of X to the regression function f(x). For example,it is common to assume that f is very smooth over high density regions of$P_X$. Many of the methods are ad-hoc and have been shown to work in specificexamples but are lacking a theoretical foundation. We provide a minimaxframework for analyzing semisupervised methods. In particular, we study methodsbased on metrics that are sensitive to the distribution $P_X$. Our modelincludes a parameter $\alpha$ that controls the strength of the semisupervisedassumption. We then use the data to adapt to $\alpha$.

Changepoint Detection over Graphs with the Spectral Scan Statistic

  We consider the change-point detection problem of deciding, based on noisymeasurements, whether an unknown signal over a given graph is constant or isinstead piecewise constant over two connected induced subgraphs of relativelylow cut size. We analyze the corresponding generalized likelihood ratio (GLR)statistics and relate it to the problem of finding a sparsest cut in a graph.We develop a tractable relaxation of the GLR statistic based on thecombinatorial Laplacian of the graph, which we call the spectral scanstatistic, and analyze its properties. We show how its performance as a testingprocedure depends directly on the spectrum of the graph, and use this result toexplicitly derive its asymptotic properties on few significant graphtopologies. Finally, we demonstrate both theoretically and by simulations thatthe spectral scan statistic can outperform naive testing procedures based onedge thresholding and $\chi^2$ testing.

Detecting Activations over Graphs using Spanning Tree Wavelet Bases

  We consider the detection of activations over graphs under Gaussian noise,where signals are piece-wise constant over the graph. Despite the wideapplicability of such a detection algorithm, there has been little success inthe development of computationally feasible methods with proveable theoreticalguarantees for general graph topologies. We cast this as a hypothesis testingproblem, and first provide a universal necessary condition for asymptoticdistinguishability of the null and alternative hypotheses. We then introducethe spanning tree wavelet basis over graphs, a localized basis that reflectsthe topology of the graph, and prove that for any spanning tree, this approachcan distinguish null from alternative in a low signal-to-noise regime. Lastly,we improve on this result and show that using the uniform spanning tree in thebasis construction yields a randomized test with stronger theoreticalguarantees that in many cases matches our necessary conditions. Specifically,we obtain near-optimal performance in edge transitive graphs, $k$-nearestneighbor graphs, and $\epsilon$-graphs.

Efficient Active Algorithms for Hierarchical Clustering

  Advances in sensing technologies and the growth of the internet have resultedin an explosion in the size of modern datasets, while storage and processingpower continue to lag behind. This motivates the need for algorithms that areefficient, both in terms of the number of measurements needed and running time.To combat the challenges associated with large datasets, we propose a generalframework for active hierarchical clustering that repeatedly runs anoff-the-shelf clustering algorithm on small subsets of the data and comes withguarantees on performance, measurement complexity and runtime complexity. Weinstantiate this framework with a simple spectral clustering algorithm andprovide concrete results on its performance, showing that, under someassumptions, this algorithm recovers all clusters of size ?(log n) using O(nlog^2 n) similarities and runs in O(n log^3 n) time for a dataset of n objects.Through extensive experimentation we also demonstrate that this framework ispractically alluring.

Recovering Block-structured Activations Using Compressive Measurements

  We consider the problems of detection and localization of a contiguous blockof weak activation in a large matrix, from a small number of noisy, possiblyadaptive, compressive (linear) measurements. This is closely related to theproblem of compressed sensing, where the task is to estimate a sparse vectorusing a small number of linear measurements. Contrary to results in compressedsensing, where it has been shown that neither adaptivity nor contiguousstructure help much, we show that for reliable localization the magnitude ofthe weakest signals is strongly influenced by both structure and the ability tochoose measurements adaptively while for detection neither adaptivity norstructure reduce the requirement on the magnitude of the signal. Wecharacterize the precise tradeoffs between the various problem parameters, thesignal strength and the number of measurements required to reliably detect andlocalize the block of activation. The sufficient conditions are complementedwith information theoretic lower bounds.

Distribution-Free Distribution Regression

  `Distribution regression' refers to the situation where a response Y dependson a covariate P where P is a probability distribution. The model is Y=f(P) +mu where f is an unknown regression function and mu is a random error.Typically, we do not observe P directly, but rather, we observe a sample fromP. In this paper we develop theory and methods for distribution-free versionsof distribution regression. This means that we do not make distributionalassumptions about the error term mu and covariate P. We prove that when theeffective dimension is small enough (as measured by the doubling dimension),then the excess prediction risk converges to zero with a polynomial rate.

Confidence sets for persistence diagrams

  Persistent homology is a method for probing topological properties of pointclouds and functions. The method involves tracking the birth and death oftopological features (2000) as one varies a tuning parameter. Features withshort lifetimes are informally considered to be "topological noise," and thosewith a long lifetime are considered to be "topological signal." In this paper,we bring some statistical ideas to persistent homology. In particular, wederive confidence sets that allow us to separate topological signal fromtopological noise.

Low-Rank Matrix and Tensor Completion via Adaptive Sampling

  We study low rank matrix and tensor completion and propose novel algorithmsthat employ adaptive sampling schemes to obtain strong performance guarantees.Our algorithms exploit adaptivity to identify entries that are highlyinformative for learning the column space of the matrix (tensor) andconsequently, our results hold even when the row space is highly coherent, incontrast with previous analyses. In the absence of noise, we show that one canexactly recover a $n \times n$ matrix of rank $r$ from merely $\Omega(nr^{3/2}\log(r))$ matrix entries. We also show that one can recover an order $T$tensor using $\Omega(n r^{T-1/2}T^2 \log(r))$ entries. For noisy recovery, ouralgorithm consistently estimates a low rank matrix corrupted with noise using$\Omega(n r^{3/2} \textrm{polylog}(n))$ entries. We complement our study withsimulations that verify our theory and demonstrate the scalability of ouralgorithms.

Recovering Graph-Structured Activations using Adaptive Compressive  Measurements

  We study the localization of a cluster of activated vertices in a graph, fromadaptively designed compressive measurements. We propose a hierarchicalpartitioning of the graph that groups the activated vertices into fewpartitions, so that a top-down sensing procedure can identify these partitions,and hence the activations, using few measurements. By exploiting the clusterstructure, we are able to provide localization guarantees at weaker signal tonoise ratios than in the unstructured setting. We complement this performanceguarantee with an information theoretic lower bound, providing a necessarysignal-to-noise ratio for any algorithm to successfully localize the cluster.We verify our analysis with some simulations, demonstrating the practicality ofour algorithm.

Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean  Separation

  While several papers have investigated computationally and statisticallyefficient methods for learning Gaussian mixtures, precise minimax bounds fortheir statistical performance as well as fundamental limits in high-dimensionalsettings are not well-understood. In this paper, we provide precise informationtheoretic bounds on the clustering accuracy and sample complexity of learning amixture of two isotropic Gaussians in high dimensions under small meanseparation. If there is a sparse subset of relevant dimensions that determinethe mean separation, then the sample complexity only depends on the number ofrelevant dimensions and mean separation, and can be achieved by a simplecomputationally efficient procedure. Our results provide the first step of atheoretical basis for recent methods that combine feature selection andclustering.

Cluster Trees on Manifolds

  In this paper we investigate the problem of estimating the cluster tree for adensity $f$ supported on or near a smooth $d$-dimensional manifold $M$isometrically embedded in $\mathbb{R}^D$. We analyze a modified version of a$k$-nearest neighbor based algorithm recently proposed by Chaudhuri andDasgupta. The main results of this paper show that under mild assumptions on$f$ and $M$, we obtain rates of convergence that depend on $d$ only but not onthe ambient dimension $D$. We also show that similar (albeit non-algorithmic)results can be obtained for kernel density estimators. We sketch a constructionof a sample complexity lower bound instance for a natural class of manifoldoblivious clustering algorithms. We further briefly consider the known manifoldcase and show that in this case a spatially adaptive algorithm achieves betterrates.

Tight Lower Bounds for Homology Inference

  The homology groups of a manifold are important topological invariants thatprovide an algebraic summary of the manifold. These groups contain richtopological information, for instance, about the connected components, holes,tunnels and sometimes the dimension of the manifold. In earlier work, we haveconsidered the statistical problem of estimating the homology of a manifoldfrom noiseless samples and from noisy samples under several different noisemodels. We derived upper and lower bounds on the minimax risk for this problem.In this note we revisit the noiseless case. In previous work we used Le Cam'slemma to establish a lower bound that differed from the upper bound of Niyogi,Smale and Weinberger by a polynomial factor in the condition number.  In this note we use a different construction based on the direct analysis ofthe likelihood ratio test to show that the upper bound of Niyogi, Smale andWeinberger is in fact tight, thus establishing rate optimal asymptotic minimaxbounds for the problem. The techniques we use here extend in a straightforwardway to the noisy settings considered in our earlier work.

FuSSO: Functional Shrinkage and Selection Operator

  We present the FuSSO, a functional analogue to the LASSO, that efficientlyfinds a sparse set of functional input covariates to regress a real-valuedresponse against. The FuSSO does so in a semi-parametric fashion, making noparametric assumptions about the nature of input functional covariates andassuming a linear form to the mapping of functional covariates to the response.We provide a statistical backing for use of the FuSSO via proof of asymptoticsparsistency under various conditions. Furthermore, we observe good results onboth synthetic and real-world data.

Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan  Statistic

  The detection of anomalous activity in graphs is a statistical problem thatarises in many applications, such as network surveillance, disease outbreakdetection, and activity monitoring in social networks. Beyond its wideapplicability, graph structured anomaly detection serves as a case study in thedifficulty of balancing computational complexity with statistical power. Inthis work, we develop from first principles the generalized likelihood ratiotest for determining if there is a well connected region of activation over thevertices in the graph in Gaussian noise. Because this test is computationallyinfeasible, we provide a relaxation, called the Lovasz extended scan statistic(LESS) that uses submodularity to approximate the intractable generalizedlikelihood ratio. We demonstrate a connection between LESS and maximuma-posteriori inference in Markov random fields, which provides us with apoly-time algorithm for LESS. Using electrical network theory, we are able tocontrol type 1 error for LESS and prove conditions under which LESS is riskconsistent. Finally, we consider specific graph models, the torus, k-nearestneighbor graphs, and epsilon-random graphs. We show that on these graphs ourresults provide near-optimal performance by matching our results to known lowerbounds.

On the Decreasing Power of Kernel and Distance based Nonparametric  Hypothesis Tests in High Dimensions

  This paper is about two related decision theoretic problems, nonparametrictwo-sample testing and independence testing. There is a belief that tworecently proposed solutions, based on kernels and distances between pairs ofpoints, behave well in high-dimensional settings. We identify different sourcesof misconception that give rise to the above belief. Specifically, wedifferentiate the hardness of estimation of test statistics from the hardnessof testing whether these statistics are zero or not, and explicitly discuss anotion of "fair" alternative hypotheses for these problems as dimensionincreases. We then demonstrate that the power of these tests actually dropspolynomially with increasing dimension against fair alternatives. We end withsome theoretical insights and shed light on the \textit{median heuristic} forkernel bandwidth selection. Our work advances the current understanding of thepower of modern nonparametric hypothesis tests in high dimensions.

Efficient Sparse Clustering of High-Dimensional Non-spherical Gaussian  Mixtures

  We consider the problem of clustering data points in high dimensions, i.e.when the number of data points may be much smaller than the number ofdimensions. Specifically, we consider a Gaussian mixture model (GMM) withnon-spherical Gaussian components, where the clusters are distinguished by onlya few relevant dimensions. The method we propose is a combination of a recentapproach for learning parameters of a Gaussian mixture model and sparse lineardiscriminant analysis (LDA). In addition to cluster assignments, the methodreturns an estimate of the set of features relevant for clustering. Our resultsindicate that the sample complexity of clustering depends on the sparsity ofthe relevant feature set, while only scaling logarithmically with the ambientdimension. Additionally, we require much milder assumptions than existing workon clustering in high dimensions. In particular, we do not require sphericalclusters nor necessitate mean separation along relevant dimensions.

Graph Connectivity in Noisy Sparse Subspace Clustering

  Subspace clustering is the problem of clustering data points into a union oflow-dimensional linear/affine subspaces. It is the mathematical abstraction ofmany important problems in computer vision, image processing and machinelearning. A line of recent work (4, 19, 24, 20) provided strong theoreticalguarantee for sparse subspace clustering (4), the state-of-the-art algorithmfor subspace clustering, on both noiseless and noisy data sets. It was shownthat under mild conditions, with high probability no two points from differentsubspaces are clustered together. Such guarantee, however, is not sufficientfor the clustering to be correct, due to the notorious "graph connectivityproblem" (15). In this paper, we investigate the graph connectivity problem fornoisy sparse subspace clustering and show that a simple post-processingprocedure is capable of delivering consistent clustering under certain "generalposition" or "restricted eigenvalue" assumptions. We also show that ourcondition is almost tight with adversarial noise perturbation by constructing acounter-example. These results provide the first exact clustering guarantee ofnoisy SSC for subspaces of dimension greater then 3.

Signal Recovery on Graphs: Random versus Experimentally Designed  Sampling

  We study signal recovery on graphs based on two sampling strategies: randomsampling and experimentally designed sampling. We propose a new class of smoothgraph signals, called approximately bandlimited, which generalizes thebandlimited class and is similar to the globally smooth class. We then proposetwo recovery strategies based on random sampling and experimentally designedsampling. The proposed recovery strategy based on experimentally designedsampling is similar to the leverage scores used in the matrix approximation. Weshow that while both strategies are unbiased estimators for the low-frequencycomponents, the convergence rate of experimentally designed sampling is muchfaster than that of random sampling when a graph is irregular. We validate theproposed recovery strategies on three specific graphs: a ring graph, anErd\H{o}s-R\'enyi graph, and a star graph. The simulation results support thetheoretical analysis.

Extreme Compressive Sampling for Covariance Estimation

  This paper studies the problem of estimating the covariance of a collectionof vectors using only highly compressed measurements of each vector. Anestimator based on back-projections of these compressive samples is proposedand analyzed. A distribution-free analysis shows that by observing just asingle linear measurement of each vector, one can consistently estimate thecovariance matrix, in both infinity and spectral norm, and this same analysisleads to precise rates of convergence in both norms. Via information-theoretictechniques, lower bounds showing that this estimator is minimax-optimal forboth infinity and spectral norm estimation problems are established. Theseresults are also specialized to give matching upper and lower bounds forestimating the population covariance of a collection of Gaussian vectors, againin the compressive measurement model. The analysis conducted in this papershows that the effective sample complexity for this problem is scaled by afactor of $m^2/d^2$ where $m$ is the compression dimension and $d$ is theambient dimension. Applications to subspace learning (Principal ComponentsAnalysis) and learning over distributed sensor networks are also discussed.

A statistical perspective of sampling scores for linear regression

  In this paper, we consider a statistical problem of learning a linear modelfrom noisy samples. Existing work has focused on approximating the leastsquares solution by using leverage-based scores as an importance samplingdistribution. However, no finite sample statistical guarantees and nocomputationally efficient optimal sampling strategies have been proposed. Toevaluate the statistical properties of different sampling strategies, wepropose a simple yet effective estimator, which is easy for theoreticalanalysis and is useful in multitask linear regression. We derive the exact meansquare error of the proposed estimator for any given sampling scores. Based onminimizing the mean square error, we propose the optimal sampling scores forboth estimator and predictor, and show that they are influenced by thenoise-to-signal ratio. Numerical simulations match the theoretical analysiswell.

On Computationally Tractable Selection of Experiments in  Measurement-Constrained Regression Models

  We derive computationally tractable methods to select a small subset ofexperiment settings from a large pool of given design points. The primary focusis on linear regression models, while the technique extends to generalizedlinear models and Delta's method (estimating functions of linear regressionmodels) as well. The algorithms are based on a continuous relaxation of anotherwise intractable combinatorial optimization problem, with sampling orgreedy procedures as post-processing steps. Formal approximation guarantees areestablished for both algorithms, and numerical results on both synthetic andreal-world data confirm the effectiveness of the proposed methods.

Minimax Lower Bounds for Linear Independence Testing

  Linear independence testing is a fundamental information-theoretic andstatistical problem that can be posed as follows: given $n$ points$\{(X_i,Y_i)\}^n_{i=1}$ from a $p+q$ dimensional multivariate distributionwhere $X_i \in \mathbb{R}^p$ and $Y_i \in\mathbb{R}^q$, determine whether $a^TX$ and $b^T Y$ are uncorrelated for every $a \in \mathbb{R}^p, b\in\mathbb{R}^q$ or not. We give minimax lower bound for this problem (when $p+q,n\to \infty$, $(p+q)/n \leq \kappa < \infty$, without sparsity assumptions). Insummary, our results imply that $n$ must be at least as large as $\sqrt{pq}/\|\Sigma_{XY}\|_F^2$ for any procedure (test) to have non-trivial power,where $\Sigma_{XY}$ is the cross-covariance matrix of $X,Y$. We also providesome evidence that the lower bound is tight, by connections to two-sampletesting and regression in specific settings.

Data Poisoning Attacks on Factorization-Based Collaborative Filtering

  Recommendation and collaborative filtering systems are important in moderninformation and e-commerce applications. As these systems are becomingincreasingly popular in the industry, their outputs could affect businessdecision making, introducing incentives for an adversarial party to compromisethe availability or integrity of such systems. We introduce a data poisoningattack on collaborative filtering systems. We demonstrate how a powerfulattacker with full knowledge of the learner can generate malicious data so asto maximize his/her malicious objectives, while at the same time mimickingnormal user behavior to avoid being detected. While the complete knowledgeassumption seems extreme, it enables a robust assessment of the vulnerabilityof collaborative filtering schemes to highly motivated attacks. We presentefficient solutions for two popular factorization-based collaborative filteringalgorithms: the \emph{alternative minimization} formulation and the\emph{nuclear norm minimization} method. Finally, we test the effectiveness ofour proposed algorithms on real-world data and discuss potential defensivestrategies.

Hypothesis Transfer Learning via Transformation Functions

  We consider the Hypothesis Transfer Learning (HTL) problem where oneincorporates a hypothesis trained on the source domain into the learningprocedure of the target domain. Existing theoretical analysis either onlystudies specific algorithms or only presents upper bounds on the generalizationerror but not on the excess risk. In this paper, we propose a unifiedalgorithm-dependent framework for HTL through a novel notion of transformationfunction, which characterizes the relation between the source and the targetdomains. We conduct a general risk analysis of this framework and inparticular, we show for the first time, if two domains are related, HTL enjoysfaster convergence rates of excess risks for Kernel Smoothing and Kernel RidgeRegression than those of the classical non-transfer learning settings.Experiments on real world data demonstrate the effectiveness of our framework.

Computationally Efficient Robust Estimation of Sparse Functionals

  Many conventional statistical procedures are extremely sensitive to seeminglyminor deviations from modeling assumptions. This problem is exacerbated inmodern high-dimensional settings, where the problem dimension can grow with andpossibly exceed the sample size. We consider the problem of robust estimationof sparse functionals, and provide a computationally and statisticallyefficient algorithm in the high-dimensional setting. Our theory identifies aunified set of deterministic conditions under which our algorithm guaranteesaccurate recovery. By further establishing that these deterministic conditionshold with high-probability for a wide range of statistical models, our theoryapplies to many problems of considerable interest including sparse mean andcovariance estimation; sparse linear regression; and sparse generalized linearmodels.

Noise-Tolerant Interactive Learning from Pairwise Comparisons

  We study the problem of interactively learning a binary classifier usingnoisy labeling and pairwise comparison oracles, where the comparison oracleanswers which one in the given two instances is more likely to be positive.Learning from such oracles has multiple applications where obtaining directlabels is harder but pairwise comparisons are easier, and the algorithm canleverage both types of oracles. In this paper, we attempt to characterize howthe access to an easier comparison oracle helps in improving the label andtotal query complexity. We show that the comparison oracle reduces the learningproblem to that of learning a threshold function. We then present an algorithmthat interactively queries the label and comparison oracles and we characterizeits query complexity under Tsybakov and adversarial noise conditions for thecomparison and labeling oracles. Our lower bounds show that our label and totalquery complexity is almost optimal.

Gradient Descent Can Take Exponential Time to Escape Saddle Points

  Although gradient descent (GD) almost always escapes saddle pointsasymptotically [Lee et al., 2016], this paper shows that even with fairlynatural random initialization schemes and non-pathological functions, GD can besignificantly slowed down by saddle points, taking exponential time to escape.On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin etal., 2017] is not slowed down by saddle points - it can find an approximatelocal minimizer in polynomial time. This result implies that GD is inherentlyslower than perturbed GD, and justifies the importance of adding perturbationsfor efficient non-convex optimization. While our focus is theoretical, we alsopresent experiments that illustrate our theoretical findings.

Near-Optimal Discrete Optimization for Experimental Design: A Regret  Minimization Approach

  The experimental design problem concerns the selection of k points from apotentially large design pool of p-dimensional vectors, so as to maximize thestatistical efficiency regressed on the selected k design points. Statisticalefficiency is measured by optimality criteria, including A(verage),D(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for theT-optimality, exact optimization is NP-hard.  We propose a polynomial-time regret minimization framework to achieve a$(1+\varepsilon)$ approximation with only $O(p/\varepsilon^2)$ design points,for all the optimality criteria above.  In contrast, to the best of our knowledge, before our work, nopolynomial-time algorithm achieves $(1+\varepsilon)$ approximations forD/E/G-optimality, and the best poly-time algorithm achieving$(1+\varepsilon)$-approximation for A/V-optimality requires $k =\Omega(p^2/\varepsilon)$ design points.

Towards Understanding the Generalization Bias of Two Layer Convolutional  Linear Classifiers with Gradient Descent

  A major challenge in understanding the generalization of deep learning is toexplain why (stochastic) gradient descent can exploit the network architectureto find solutions that have good generalization performance when using highcapacity models. We find simple but realistic examples showing that thisphenomenon exists even when learning linear classifiers --- between two linearnetworks with the same capacity, the one with a convolutional layer cangeneralize better than the other when the data distribution has some underlyingspatial structure. We argue that this difference results from a combination ofthe convolution architecture, data distribution and gradient descent, all ofwhich are necessary to be included in a meaningful analysis. We provide ageneral analysis of the generalization performance as a function of datadistribution and convolutional filter size, given gradient descent as theoptimization algorithm, then interpret the results using concrete examples.Experimental results show that our analysis is able to explain what happens inour introduced examples.

Multiresolution Representations for Piecewise-Smooth Signals on Graphs

  What is a mathematically rigorous way to describe the taxi-pickupdistribution in Manhattan, or the profile information in online socialnetworks? A deep understanding of representing those data not only providesinsights to the data properties, but also benefits to many subsequentprocessing procedures, such as denoising, sampling, recovery and localization.In this paper, we model those complex and irregular data as piecewise-smoothgraph signals and propose a graph dictionary to effectively represent thosegraph signals. We first propose the graph multiresolution analysis, whichprovides a principle to design good representations. We then propose acoarse-to-fine approach, which iteratively partitions a graph into twosubgraphs until we reach individual nodes. This approach efficiently implementsthe graph multiresolution analysis and the induced graph dictionary promotessparse representations piecewise-smooth graph signals. Finally, we validate theproposed graph dictionary on two tasks: approximation and localization. Theempirical results show that the proposed graph dictionary outperforms eightother representation methods on six datasets, including traffic networks,social networks and point cloud meshes.

Robust Nonparametric Regression under Huber's $Îµ$-contamination  Model

  We consider the non-parametric regression problem under Huber's$\epsilon$-contamination model, in which an $\epsilon$ fraction of observationsare subject to arbitrary adversarial noise. We first show that a simple localbinning median step can effectively remove the adversary noise and this medianestimator is minimax optimal up to absolute constants over the H\"{o}lderfunction class with smoothness parameters smaller than or equal to 1.Furthermore, when the underlying function has higher smoothness, we show thatusing local binning median as pre-preprocessing step to remove the adversarialnoise, then we can apply any non-parametric estimator on top of the medians. Inparticular we show local median binning followed by kernel smoothing and localpolynomial regression achieve minimaxity over H\"{o}lder and Sobolev classeswith arbitrary smoothness parameters. Our main proof technique is a decoupledanalysis of adversary noise and stochastic noise, which can be potentiallyapplied to other robust estimation problems. We also provide numerical resultsto verify the effectiveness of our proposed methods.

PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review

  We consider the problem of automated assignment of papers to reviewers inconference peer review, with a focus on fairness and statistical accuracy. Ourfairness objective is to maximize the review quality of the most disadvantagedpaper, in contrast to the commonly used objective of maximizing the totalquality over all papers. We design an assignment algorithm based on anincremental max-flow procedure that we prove is near-optimally fair. Ourstatistical accuracy objective is to ensure correct recovery of the papers thatshould be accepted. We provide a sharp minimax analysis of the accuracy of thepeer-review process for a popular objective-score model as well as for a novelsubjective-score model that we propose in the paper. Our analysis proves thatour proposed assignment algorithm also leads to a near-optimal statisticalaccuracy. Finally, we design a novel experiment that allows for an objectivecomparison of various assignment algorithms, and overcomes the inherentdifficulty posed by the absence of a ground truth in experiments onpeer-review. The results of this experiment corroborate the theoreticalguarantees of our algorithm.

Gradient Descent Provably Optimizes Over-parameterized Neural Networks

  One of the mysteries in the success of neural networks is randomlyinitialized first order methods like gradient descent can achieve zero trainingloss even though the objective function is non-convex and non-smooth. Thispaper demystifies this surprising phenomenon for two-layer fully connected ReLUactivated neural networks. For an $m$ hidden node shallow neural network withReLU activation and $n$ training data, we show as long as $m$ is large enoughand no two inputs are parallel, randomly initialized gradient descent convergesto a globally optimal solution at a linear convergence rate for the quadraticloss function.  Our analysis relies on the following observation: over-parameterization andrandom initialization jointly restrict every weight vector to be close to itsinitialization for all iterations, which allows us to exploit a strongconvexity-like property to show that gradient descent converges at a globallinear rate to the global optimum. We believe these insights are also useful inanalyzing deep models and other first order methods.

Provably Correct Algorithms for Matrix Column Subset Selection with  Selectively Sampled Data

  We consider the problem of matrix column subset selection, which selects asubset of columns from an input matrix such that the input can be wellapproximated by the span of the selected columns. Column subset selection hasbeen applied to numerous real-world data applications such as populationgenetics summarization, electronic circuits testing and recommendation systems.In many applications the complete data matrix is unavailable and one needs toselect representative columns by inspecting only a small portion of the inputmatrix. In this paper we propose the first provably correct column subsetselection algorithms for partially observed data matrices. Our proposedalgorithms exhibit different merits and limitations in terms of statisticalaccuracy, computational efficiency, sample complexity and sampling schemes,which provides a nice exploration of the tradeoff between these desiredproperties for column subset selection. The proposed methods employ the idea offeedback driven sampling and are inspired by several sampling schemespreviously introduced for low-rank matrix approximation tasks (Drineas et al.,2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy andSingh, 2014). Our analysis shows that, under the assumption that the input datamatrix has incoherent rows but possibly coherent columns, all algorithmsprovably converge to the best low-rank approximation of the original data asnumber of selected columns increases. Furthermore, two of the proposedalgorithms enjoy a relative error bound, which is preferred for column subsetselection and matrix approximation purposes. We also demonstrate through boththeoretical and empirical analysis the power of feedback driven samplingcompared to uniform random sampling on input matrices with highly correlatedcolumns.

Detecting Anomalous Activity on Networks with the Graph Fourier Scan  Statistic

  We consider the problem of deciding, based on a single noisy measurement ateach vertex of a given graph, whether the underlying unknown signal is constantover the graph or there exists a cluster of vertices with anomalous activation.This problem is relevant to several applications such as surveillance, diseaseoutbreak detection, biomedical imaging, environmental monitoring, etc. Sincethe activations in these problems often tend to be localized to small groups ofvertices in the graphs, we model such activity by a class of signals that aresupported over a (possibly disconnected) cluster with low cut size relative toits size. We analyze the corresponding generalized likelihood ratio (GLR)statistics and relate it to the problem of finding a sparsest cut in the graph.We develop a tractable relaxation of the GLR statistic based on thecombinatorial Laplacian of the graph, which we call the graph Fourier scanstatistic, and analyze its properties. We show how its performance as a testingprocedure depends directly on the spectrum of the graph, and use this result toexplicitly derive its asymptotic properties on a few significant graphtopologies. Finally, we demonstrate theoretically and with simulations that thegraph Fourier scan statistic can outperform naive testing procedures based onglobal averaging and vertex-wise thresholding. We also demonstrate theusefulness of the GFSS by analyzing groundwater Arsenic concentrations from aU.S. Geological Survey dataset.

Noise-adaptive Margin-based Active Learning and Lower Bounds under  Tsybakov Noise Condition

  We present a simple noise-robust margin-based active learning algorithm tofind homogeneous (passing the origin) linear separators and analyze its errorconvergence when labels are corrupted by noise. We show that when the imposednoise satisfies the Tsybakov low noise condition (Mammen, Tsybakov, and others1999; Tsybakov 2004) the algorithm is able to adapt to unknown level of noiseand achieves optimal statistical rate up to poly-logarithmic factors. We alsoderive lower bounds for margin based active learning algorithms under Tsybakovnoise conditions (TNC) for the membership query synthesis scenario (Angluin1988). Our result implies lower bounds for the stream based selective samplingscenario (Cohn 1990) under TNC for some fairly simple data distributions. Quitesurprisingly, we show that the sample complexity cannot be improved even if theunderlying data distribution is as simple as the uniform distribution on theunit ball. Our proof involves the construction of a well separated hypothesisset on the d-dimensional unit ball along with carefully designed labeldistributions for the Tsybakov noise condition. Our analysis might provideinsights for other forms of lower bounds as well.

