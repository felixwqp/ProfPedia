Variational Chernoff Bounds for Graphical Models

  Recent research has made significant progress on the problem of bounding logpartition functions for exponential family graphical models. Such bounds haveassociated dual parameters that are often used as heuristic estimates of themarginal probabilities required in inference and learning. However thesevariational estimates do not give rigorous bounds on marginal probabilities,nor do they give estimates for probabilities of more general events than simplemarginals. In this paper we build on this recent work by deriving rigorousupper and lower bounds on event probabilities for graphical models. Ourapproach is based on the use of generalized Chernoff bounds to express boundson event probabilities in terms of convex optimization problems; theseoptimization problems, in turn, require estimates of generalized log partitionfunctions. Simulations indicate that this technique can result in useful,rigorous bounds to complement the heuristic variational estimates, withcomparable computational cost.

A Hierarchical Graphical Model for Record Linkage

  The task of matching co-referent records is known among other names as rocordlinkage. For large record-linkage problems, often there is little or no labeleddata available, but unlabeled data shows a reasonable clear structure. For suchproblems, unsupervised or semi-supervised methods are preferable to supervisedmethods. In this paper, we describe a hierarchical graphical model frameworkfor the linakge-problem in an unsupervised setting. In addition to proposingnew methods, we also cast existing unsupervised probabilistic record-linkagemethods in this framework. Some of the techniques we propose to minimizeoverfitting in the above model are of interest in the general graphical modelsetting. We describe a method for incorporating monotinicity constraints in agraphical model. We also outline a bootstrapping approach of using"single-field" classifiers to noisily label latent variables in a hierarchicalmodel. Experimental results show that our proposed unsupervised methods performquite competitively even with fully supervised record-linkage methods.

Error-Correcting Tournaments

  We present a family of pairwise tournaments reducing $k$-class classificationto binary classification. These reductions are provably robust against aconstant fraction of binary errors. The results improve on the PECOCconstruction \cite{SECOC} with an exponential improvement in computation, from$O(k)$ to $O(\log_2 k)$, and the removal of a square root in the regretdependence, matching the best possible computation and regret up to a constant.

Information-theoretic lower bounds on the oracle complexity of  stochastic convex optimization

  Relative to the large literature on upper bounds on complexity of convexoptimization, lesser attention has been paid to the fundamental hardness ofthese problems. Given the extensive use of convex optimization in machinelearning and statistics, gaining an understanding of these complexity-theoreticissues is important. In this paper, we study the complexity of stochasticconvex optimization in an oracle model of computation. We improve upon knownresults and obtain tight minimax complexity estimates for various functionclasses.

Sparsistency of $\ell_1$-Regularized $M$-Estimators

  We consider the model selection consistency or sparsistency of a broad set of$\ell_1$-regularized $M$-estimators for linear and non-linear statisticalmodels in a unified fashion. For this purpose, we propose the local structuredsmoothness condition (LSSC) on the loss function. We provide a general resultgiving deterministic sufficient conditions for sparsistency in terms of theregularization parameter, ambient dimension, sparsity level, and number ofmeasurements. We show that several important statistical models have$M$-estimators that indeed satisfy the LSSC, and as a result, the sparsistencyguarantees for the corresponding $\ell_1$-regularized $M$-estimators can bederived as simple applications of our main theorem.

Towards Aggregating Weighted Feature Attributions

  Current approaches for explaining machine learning models fall into twodistinct classes: antecedent event influence and value attribution. The formerleverages training instances to describe how much influence a training pointexerts on a test point, while the latter attempts to attribute value to thefeatures most pertinent to a given prediction. In this work, we discuss analgorithm, AVA: Aggregate Valuation of Antecedents, that fuses these twoexplanation classes to form a new approach to feature attribution that not onlyretrieves local explanations but also captures global patterns learned by amodel. Our experimentation convincingly favors weighting and aggregatingfeature attributions via AVA.

High-dimensional Ising model selection using ${\ell_1}$-regularized  logistic regression

  We consider the problem of estimating the graph associated with a binaryIsing Markov random field. We describe a method based on $\ell_1$-regularizedlogistic regression, in which the neighborhood of any given node is estimatedby performing logistic regression subject to an $\ell_1$-constraint. The methodis analyzed under high-dimensional scaling in which both the number of nodes$p$ and maximum neighborhood size $d$ are allowed to grow as a function of thenumber of observations $n$. Our main results provide sufficient conditions onthe triple $(n,p,d)$ and the model parameters for the method to succeed inconsistently estimating the neighborhood of every node in the graphsimultaneously. With coherence conditions imposed on the population Fisherinformation matrix, we prove that consistent neighborhood selection can beobtained for sample sizes $n=\Omega(d^3\log p)$ with exponentially decayingerror. When these same conditions are imposed directly on the sample matrices,we show that a reduced sample size of $n=\Omega(d^2\log p)$ suffices for themethod to estimate neighborhoods consistently. Although this paper focuses onthe binary graphical models, we indicate how a generalization of the method ofthe paper would apply to general discrete Markov random fields.

Exponential Family Matrix Completion under Structural Constraints

  We consider the matrix completion problem of recovering a structured matrixfrom noisy and partial measurements. Recent works have proposed tractableestimators with strong statistical guarantees for the case where the underlyingmatrix is low--rank, and the measurements consist of a subset, either of theexact individual entries, or of the entries perturbed by additive Gaussiannoise, which is thus implicitly suited for thin--tailed continuous data.Arguably, common applications of matrix completion require estimators for (a)heterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)for heterogeneous noise models (beyond Gaussian), which capture varieduncertainty in the measurements, and (c) heterogeneous structural constraintsbeyond low--rank, such as block--sparsity, or a superposition structure oflow--rank plus elementwise sparseness, among others. In this paper, we providea vastly unified framework for generalized matrix completion by considering amatrix completion setting wherein the matrix entries are sampled from anymember of the rich family of exponential family distributions; and imposegeneral structural constraints on the underlying matrix, as captured by ageneral regularizer $\mathcal{R}(.)$. We propose a simple convex regularized$M$--estimator for the generalized framework, and provide a unified and novelstatistical analysis for this general class of estimators. We finallycorroborate our theoretical results on simulated datasets.

High-dimensional covariance estimation by minimizing $\ell_1$-penalized  log-determinant divergence

  Given i.i.d. observations of a random vector $X \in \mathbb{R}^p$, we studythe problem of estimating both its covariance matrix $\Sigma^*$, and itsinverse covariance or concentration matrix {$\Theta^* = (\Sigma^*)^{-1}$.} Weestimate $\Theta^*$ by minimizing an $\ell_1$-penalized log-determinant Bregmandivergence; in the multivariate Gaussian case, this approach corresponds to$\ell_1$-penalized maximum likelihood, and the structure of $\Theta^*$ isspecified by the graph of an associated Gaussian Markov random field. Weanalyze the performance of this estimator under high-dimensional scaling, inwhich the number of nodes in the graph $p$, the number of edges $s$ and themaximum node degree $d$, are allowed to grow as a function of the sample size$n$. In addition to the parameters $(p,s,d)$, our analysis identifies other keyquantities covariance matrix $\Sigma^*$; and (b) the $\ell_\infty$ operatornorm of the sub-matrix $\Gamma^*_{S S}$, where $S$ indexes the graph edges, and$\Gamma^* = (\Theta^*)^{-1} \otimes (\Theta^*)^{-1}$; and (c) a mutualincoherence or irrepresentability measure on the matrix $\Gamma^*$ and (d) therate of decay $1/f(n,\delta)$ on the probabilities $ \{|\hat{\Sigma}^n_{ij}-\Sigma^*_{ij}| > \delta \}$, where $\hat{\Sigma}^n$ is the sample covariancebased on $n$ samples. Our first result establishes consistency of our estimate$\hat{\Theta}$ in the elementwise maximum-norm. This in turn allows us toderive convergence rates in Frobenius and spectral norms, with improvementsupon existing results for graphs with maximum node degrees $d = o(\sqrt{s})$.In our second result, we show that with probability converging to one, theestimate $\hat{\Theta}$ correctly specifies the zero pattern of theconcentration matrix $\Theta^*$.

Sparse Additive Models

  We present a new class of methods for high-dimensional nonparametricregression and classification called sparse additive models (SpAM). Our methodscombine ideas from sparse linear modeling and additive nonparametricregression. We derive an algorithm for fitting the models that is practical andeffective even when the number of covariates is larger than the sample size.SpAM is closely related to the COSSO model of Lin and Zhang (2006), butdecouples smoothing and sparsity, enabling the use of arbitrary nonparametricsmoothers. An analysis of the theoretical properties of SpAM is given. We alsostudy a greedy estimator that is a nonparametric version of forward stepwiseregression. Empirical results on synthetic and real data are presented, showingthat SpAM can be effective in fitting sparse nonparametric models in highdimensional data.

High-Dimensional Graphical Model Selection Using $\ell_1$-Regularized  Logistic Regression

  We consider the problem of estimating the graph structure associated with adiscrete Markov random field. We describe a method based on$\ell_1$-regularized logistic regression, in which the neighborhood of anygiven node is estimated by performing logistic regression subject to an$\ell_1$-constraint. Our framework applies to the high-dimensional setting, inwhich both the number of nodes $p$ and maximum neighborhood sizes $d$ areallowed to grow as a function of the number of observations $n$. Our mainresults provide sufficient conditions on the triple $(n, p, d)$ for the methodto succeed in consistently estimating the neighborhood of every node in thegraph simultaneously. Under certain assumptions on the population Fisherinformation matrix, we prove that consistent neighborhood selection can beobtained for sample sizes $n = \Omega(d^3 \log p)$, with the error decaying as$\order(\exp(-C n/d^3))$ for some constant $C$. If these same assumptions areimposed directly on the sample matrices, we show that $n = \Omega(d^2 \log p)$samples are sufficient.

On Learning Discrete Graphical Models Using Greedy Methods

  In this paper, we address the problem of learning the structure of a pairwisegraphical model from samples in a high-dimensional setting. Our first mainresult studies the sparsistency, or consistency in sparsity pattern recovery,properties of a forward-backward greedy algorithm as applied to generalstatistical models. As a special case, we then apply this algorithm to learnthe structure of a discrete graphical model via neighborhood estimation. As acorollary of our general result, we derive sufficient conditions on the numberof samples n, the maximum node-degree d and the problem size p, as well asother conditions on the model parameters, so that the algorithm recovers allthe edges with high probability. Our result guarantees graph selection forsamples scaling as n = Omega(d^2 log(p)), in contrast to existingconvex-optimization based algorithms that require a sample complexity of\Omega(d^3 log(p)). Further, the greedy algorithm only requires a restrictedstrong convexity condition which is typically milder than irrepresentabilityassumptions. We corroborate these results using numerical simulations at theend.

Virus Dynamics on Starlike Graphs

  The field of epidemiology has presented fascinating and relevant questionsfor mathematicians, primarily concerning the spread of viruses in a community.The importance of this research has greatly increased over time as itsapplications have expanded to also include studies of electronic and socialnetworks and the spread of information and ideas. We study virus propagation ona non-linear hub and spoke graph (which models well many airline networks). Wedetermine the long-term behavior as a function of the cure and infection rates,as well as the number of spokes n. For each n we prove the existence of acritical threshold relating the two rates. Below this threshold, the virusalways dies out; above this threshold, all non-trivial initial conditionsiterate to a unique non-trivial steady state. We end with some generalizationsto other networks.

On Graphical Models via Univariate Exponential Family Distributions

  Undirected graphical models, or Markov networks, are a popular class ofstatistical models, used in a wide variety of applications. Popular instancesof this class include Gaussian graphical models and Ising models. In manysettings, however, it might not be clear which subclass of graphical models touse, particularly for non-Gaussian and non-categorical data. In this paper, weconsider a general sub-class of graphical models where the node-wiseconditional distributions arise from exponential families. This allows us toderive multivariate graphical model distributions from univariate exponentialfamily distributions, such as the Poisson, negative binomial, and exponentialdistributions. Our key contributions include a class of M-estimators to fitthese graphical model distributions; and rigorous statistical analysis showingthat these M-estimators recover the true graphical model structure exactly,with high probability. We provide examples of genomic and proteomic networkslearned via instances of our class of graphical models derived from Poisson andexponential distributions.

Sparse Inverse Covariance Matrix Estimation Using Quadratic  Approximation

  The L1-regularized Gaussian maximum likelihood estimator (MLE) has been shownto have strong statistical guarantees in recovering a sparse inverse covariancematrix, or alternatively the underlying graph structure of a Gaussian MarkovRandom Field, from very limited samples. We propose a novel algorithm forsolving the resulting optimization problem which is a regularizedlog-determinant program. In contrast to recent state-of-the-art methods thatlargely use first order gradient information, our algorithm is based onNewton's method and employs a quadratic approximation, but with somemodifications that leverage the structure of the sparse Gaussian MLE problem.We show that our method is superlinearly convergent, and present experimentalresults using synthetic and real-world application data that demonstrate theconsiderable improvements in performance of our method when compared to otherstate-of-the-art methods.

Proximal Quasi-Newton for Computationally Intensive L1-regularized  M-estimators

  We consider the class of optimization problems arising from computationallyintensive L1-regularized M-estimators, where the function or gradient valuesare very expensive to compute. A particular instance of interest is theL1-regularized MLE for learning Conditional Random Fields (CRFs), which are apopular class of statistical models for varied structured prediction problemssuch as sequence labeling, alignment, and classification with label taxonomy.L1-regularized MLEs for CRFs are particularly expensive to optimize sincecomputing the gradient values requires an expensive inference step. In thiswork, we propose the use of a carefully constructed proximal quasi-Newtonalgorithm for such computationally intensive M-estimation problems, where weemploy an aggressive active set selection technique. In a key contribution ofthe paper, we show that the proximal quasi-Newton method is provablysuper-linearly convergent, even in the absence of strong convexity, byleveraging a restricted variant of strong convexity. In our experiments, theproposed algorithm converges considerably faster than current state-of-the-arton the problems of sequence labeling and hierarchical classification.

On the Information Theoretic Limits of Learning Ising Models

  We provide a general framework for computing lower-bounds on the samplecomplexity of recovering the underlying graphs of Ising models, given i.i.dsamples. While there have been recent results for specific graph classes, theseinvolve fairly extensive technical arguments that are specialized to eachspecific graph class. In contrast, we isolate two key graph-structuralingredients that can then be used to specify sample complexity lower-bounds.Presence of these structural properties makes the graph class hard to learn. Wederive corollaries of our main result that not only recover existing recentresults, but also provide lower bounds for novel graph classes not consideredpreviously. We also extend our framework to the random graph setting and derivecorollaries for Erd\H{o}s-R\'{e}nyi graphs in a certain dense setting.

Optimal Decision-Theoretic Classification Using Non-Decomposable  Performance Metrics

  We provide a general theoretical analysis of expected out-of-sample utility,also referred to as decision-theoretic classification, for non-decomposablebinary classification metrics such as F-measure and Jaccard coefficient. Ourkey result is that the expected out-of-sample utility for many performancemetrics is provably optimized by a classifier which is equivalent to a signedthresholding of the conditional probability of the positive class. Our analysisbridges a gap in the literature on binary classification, revealed in light ofrecent results for non-decomposable metrics in population utility maximizationstyle classification. Our results identify checkable properties of aperformance metric which are sufficient to guarantee a probability rankingprinciple. We propose consistent estimators for optimal expected out-of-sampleclassification. As a consequence of the probability ranking principle,computational requirements can be reduced from exponential to cubic complexityin the general case, and further reduced to quadratic complexity in specialcases. We provide empirical results on simulated and benchmark datasetsevaluating the performance of the proposed algorithms for decision-theoreticclassification and comparing them to baseline and state-of-the-art methods inpopulation utility maximization for non-decomposable metrics.

Vector-Space Markov Random Fields via Exponential Families

  We present Vector-Space Markov Random Fields (VS-MRFs), a novel class ofundirected graphical models where each variable can belong to an arbitraryvector space. VS-MRFs generalize a recent line of work on scalar-valued,uni-parameter exponential family and mixed graphical models, thereby greatlybroadening the class of exponential families available (e.g., allowingmultinomial and Dirichlet distributions). Specifically, VS-MRFs are the jointgraphical model distributions where the node-conditional distributions belongto generic exponential families with general vector space domains. We alsopresent a sparsistent $M$-estimator for learning our class of MRFs thatrecovers the correct set of edges with high probability. We validate ourapproach via a set of synthetic data experiments as well as a real-world casestudy of over four million foods from the popular diet tracking appMyFitnessPal. Our results demonstrate that our algorithm performs wellempirically and that VS-MRFs are capable of capturing and highlightinginteresting structure in complex, real-world data. All code for our algorithmis open source and publicly available.

Square Root Graphical Models: Multivariate Generalizations of Univariate  Exponential Families that Permit Positive Dependencies

  We develop Square Root Graphical Models (SQR), a novel class of parametricgraphical models that provides multivariate generalizations of univariateexponential family distributions. Previous multivariate graphical models [Yanget al. 2015] did not allow positive dependencies for the exponential andPoisson generalizations. However, in many real-world datasets, variablesclearly have positive dependencies. For example, the airport delay time in NewYork---modeled as an exponential distribution---is positively related to thedelay time in Boston. With this motivation, we give an example of our modelclass derived from the univariate exponential distribution that allows foralmost arbitrary positive and negative dependencies with only a mild conditionon the parameter matrix---a condition akin to the positive definiteness of theGaussian covariance matrix. Our Poisson generalization allows for both positiveand negative dependencies without any constraints on the parameter values. Wealso develop parameter estimation methods using node-wise regressions with$\ell_1$ regularization and likelihood approximation methods using sampling.Finally, we demonstrate our exponential generalization on a synthetic datasetand a real-world dataset of airport delay times.

Kernel Ridge Regression via Partitioning

  In this paper, we investigate a divide and conquer approach to Kernel RidgeRegression (KRR). Given n samples, the division step involves separating thepoints based on some underlying disjoint partition of the input space (possiblyvia clustering), and then computing a KRR estimate for each partition. Theconquering step is simple: for each partition, we only consider its own localestimate for prediction. We establish conditions under which we can givegeneralization bounds for this estimator, as well as achieve optimal minimaxrates. We also show that the approximation error component of thegeneralization error is lesser than when a single KRR estimate is fit on thedata: thus providing both statistical and computational advantages over asingle KRR estimate over the entire data (or an averaging over randompartitions as in other recent work, [30]). Lastly, we provide experimentalvalidation for our proposed estimator and our assumptions.

A Voting-Based System for Ethical Decision Making

  We present a general approach to automating ethical decisions, drawing onmachine learning and computational social choice. In a nutshell, we propose tolearn a model of societal preferences, and, when faced with a specific ethicaldilemma at runtime, efficiently aggregate those preferences to identify adesirable choice. We provide a concrete algorithm that instantiates ourapproach; some of its crucial steps are informed by a new theory ofswap-dominance efficient voting rules. Finally, we implement and evaluate asystem for ethical decision making in the autonomous vehicle domain, usingpreference data collected from 1.3 million people through the Moral Machinewebsite.

Robust Estimation via Robust Gradient Estimation

  We provide a new computationally-efficient class of estimators for riskminimization. We show that these estimators are robust for general statisticalmodels: in the classical Huber epsilon-contamination model and in heavy-tailedsettings. Our workhorse is a novel robust variant of gradient descent, and weprovide conditions under which our gradient descent variant provides accurateestimators in a general convex risk minimization problem. We provide specificconsequences of our theory for linear regression, logistic regression and forestimation of the canonical parameters in an exponential family. These resultsprovide some of the first computationally tractable and provably robustestimators for these canonical statistical models. Finally, we study theempirical performance of our proposed methods on synthetic and real datasets,and find that our methods convincingly outperform a variety of baselines.

DAGs with NO TEARS: Continuous Optimization for Structure Learning

  Estimating the structure of directed acyclic graphs (DAGs, also known asBayesian networks) is a challenging problem since the search space of DAGs iscombinatorial and scales superexponentially with the number of nodes. Existingapproaches rely on various local heuristics for enforcing the acyclicityconstraint. In this paper, we introduce a fundamentally different strategy: Weformulate the structure learning problem as a purely \emph{continuous}optimization problem over real matrices that avoids this combinatorialconstraint entirely. This is achieved by a novel characterization of acyclicitythat is not only smooth but also exact. The resulting problem can beefficiently solved by standard numerical algorithms, which also makesimplementation effortless. The proposed method outperforms existing ones,without imposing any structural assumptions on the graph such as boundedtreewidth or in-degree. Code implementing the proposed algorithm is open-sourceand publicly available at https://github.com/xunzheng/notears.

Robust Nonparametric Regression under Huber's $Îµ$-contamination  Model

  We consider the non-parametric regression problem under Huber's$\epsilon$-contamination model, in which an $\epsilon$ fraction of observationsare subject to arbitrary adversarial noise. We first show that a simple localbinning median step can effectively remove the adversary noise and this medianestimator is minimax optimal up to absolute constants over the H\"{o}lderfunction class with smoothness parameters smaller than or equal to 1.Furthermore, when the underlying function has higher smoothness, we show thatusing local binning median as pre-preprocessing step to remove the adversarialnoise, then we can apply any non-parametric estimator on top of the medians. Inparticular we show local median binning followed by kernel smoothing and localpolynomial regression achieve minimaxity over H\"{o}lder and Sobolev classeswith arbitrary smoothness parameters. Our main proof technique is a decoupledanalysis of adversary noise and stochastic noise, which can be potentiallyapplied to other robust estimation problems. We also provide numerical resultsto verify the effectiveness of our proposed methods.

Binary Classification with Karmic, Threshold-Quasi-Concave Metrics

  Complex performance measures, beyond the popular measure of accuracy, areincreasingly being used in the context of binary classification. These complexperformance measures are typically not even decomposable, that is, the lossevaluated on a batch of samples cannot typically be expressed as a sum oraverage of losses evaluated at individual samples, which in turn requires newtheoretical and methodological developments beyond standard treatments ofsupervised learning. In this paper, we advance this understanding of binaryclassification for complex performance measures by identifying two keyproperties: a so-called Karmic property, and a more technicalthreshold-quasi-concavity property, which we show is milder than existingstructural assumptions imposed on performance measures. Under these properties,we show that the Bayes optimal classifier is a threshold function of theconditional probability of positive class. We then leverage this result to comeup with a computationally practical plug-in classifier, via a novel thresholdestimator, and further, provide a novel statistical analysis of classificationerror with respect to complex performance measures.

Revisiting Adversarial Risk

  Recent works on adversarial perturbations show that there is an inherenttrade-off between standard test accuracy and adversarial accuracy.Specifically, they show that no classifier can simultaneously be robust toadversarial perturbations and achieve high standard test accuracy. However,this is contrary to the standard notion that on tasks such as imageclassification, humans are robust classifiers with low error rate. In thiswork, we show that the main reason behind this confusion is the inexactdefinition of adversarial perturbation that is used in the literature. To fixthis issue, we propose a slight, yet important modification to the existingdefinition of adversarial perturbation. Based on the modified definition, weshow that there is no trade-off between adversarial and standard accuracies;there exist classifiers that are robust and achieve high standard accuracy. Wefurther study several properties of this new definition of adversarial risk andits relation to the existing definition.

Sample Complexity of Nonparametric Semi-Supervised Learning

  We study the sample complexity of semi-supervised learning (SSL) andintroduce new assumptions based on the mismatch between a mixture model learnedfrom unlabeled data and the true mixture model induced by the (unknown) classconditional distributions. Under these assumptions, we establish an$\Omega(K\log K)$ labeled sample complexity bound without imposing parametricassumptions, where $K$ is the number of classes. Our results suggest that evenin nonparametric settings it is possible to learn a near-optimal classifierusing only a few labeled samples. Unlike previous theoretical work whichfocuses on binary classification, we consider general multiclass classification($K>2$), which requires solving a difficult permutation learning problem. Thispermutation defines a classifier whose classification error is controlled bythe Wasserstein distance between mixing measures, and we provide finite-sampleresults characterizing the behaviour of the excess risk of this classifier.Finally, we describe three algorithms for computing these estimators based on aconnection to bipartite graph matching, and perform experiments to illustratethe superiority of the MLE over the majority vote estimator.

Learning Tensor Latent Features

  We study the problem of learning latent feature models (LFMs) for tensor datacommonly observed in science and engineering such as hyperspectral imagery.However, the problem is challenging not only due to the non-convex formulation,the combinatorial nature of the constraints in LFMs, but also the high-ordercorrelations in the data. In this work, we formulate a tensor latent featurelearning problem by representing the data as a mixture of high-order latentfeatures and binary codes, which are memory efficient and easy to interpret. Tomake the learning tractable, we propose a novel optimization procedure, Binarymatching pursuit (BMP), that iteratively searches for binary bases via aMAXCUT-like boolean quadratic solver. Such a procedure is guaranteed to achievean? suboptimal solution in O($1/\epsilon$) greedy steps, resulting in atrade-off between accuracy and sparsity. When evaluated on both synthetic andreal datasets, our experiments show superior performance over baseline methods.

Word Mover's Embedding: From Word2Vec to Document Embedding

  While the celebrated Word2Vec technique yields semantically richrepresentations for individual words, there has been relatively less success inextending to generate unsupervised sentences or documents embeddings. Recentwork has demonstrated that a distance measure between documents called\emph{Word Mover's Distance} (WMD) that aligns semantically similar words,yields unprecedented KNN classification accuracy. However, WMD is expensive tocompute, and it is hard to extend its use beyond a KNN classifier. In thispaper, we propose the \emph{Word Mover's Embedding } (WME), a novel approach tobuilding an unsupervised document (sentence) embedding from pre-trained wordembeddings. In our experiments on 9 benchmark text classification datasets and22 textual similarity tasks, the proposed technique consistently matches oroutperforms state-of-the-art techniques, with significantly higher accuracy onproblems of short length.

Representer Point Selection for Explaining Deep Neural Networks

  We propose to explain the predictions of a deep neural network, by pointingto the set of what we call representer points in the training set, for a giventest point prediction. Specifically, we show that we can decompose thepre-activation prediction of a neural network into a linear combination ofactivations of training points, with the weights corresponding to what we callrepresenter values, which thus capture the importance of that training point onthe learned parameters of the network. But it provides a deeper understandingof the network than simply training point influence: with positive representervalues corresponding to excitatory training points, and negative valuescorresponding to inhibitory points, which as we show provides considerably moreinsight. Our method is also much more scalable, allowing for real-time feedbackin a manner not feasible with influence functions.

Adaptive Hard Thresholding for Near-optimal Consistent Robust Regression

  We study the problem of robust linear regression with response variablecorruptions. We consider the oblivious adversary model, where the adversarycorrupts a fraction of the responses in complete ignorance of the data. Weprovide a nearly linear time estimator which consistently estimates the trueregression vector, even with $1-o(1)$ fraction of corruptions. Existing resultsin this setting either don't guarantee consistent estimates or can only handlea small fraction of corruptions. We also extend our estimator to robust sparselinear regression and show that similar guarantees hold in this setting.Finally, we apply our estimator to the problem of linear regression withheavy-tailed noise and show that our estimator consistently estimates theregression vector even when the noise has unbounded variance (e.g., Cauchydistribution), for which most existing results don't even apply. Our estimatoris based on a novel variant of outlier removal via hard thresholding in whichthe threshold is chosen adaptively and crucially relies on randomness to escapebad fixed points of the non-convex hard thresholding operation.

A Unified Framework for High-Dimensional Analysis of M-Estimators with  Decomposable Regularizers

  High-dimensional statistical inference deals with models in which the thenumber of parameters p is comparable to or larger than the sample size n. Sinceit is usually impossible to obtain consistent procedures unless$p/n\rightarrow0$, a line of recent work has studied models with various typesof low-dimensional structure, including sparse vectors, sparse and structuredmatrices, low-rank matrices and combinations thereof. In such settings, ageneral approach to estimation is to solve a regularized optimization problem,which combines a loss function measuring how well the model fits the data withsome regularization function that encourages the assumed structure. This paperprovides a unified framework for establishing consistency and convergence ratesfor such regularized M-estimators under high-dimensional scaling. We state onemain theorem and show how it can be used to re-derive some existing results,and also to obtain a number of new results on consistency and convergencerates, in both $\ell_2$-error and related norms. Our analysis also identifiestwo key properties of loss and regularization functions, referred to asrestricted strong convexity and decomposability, that ensure correspondingregularized M-estimators have fast convergence rates and which are optimal inmany well-studied cases.

Encoding and decoding V1 fMRI responses to natural images with sparse  nonparametric models

  Functional MRI (fMRI) has become the most common method for investigating thehuman brain. However, fMRI data present some complications for statisticalanalysis and modeling. One recently developed approach to these data focuses onestimation of computational encoding models that describe how stimuli aretransformed into brain activity measured in individual voxels. Here we aim atbuilding encoding models for fMRI signals recorded in the primary visual cortexof the human brain. We use residual analyses to reveal systematic nonlinearityacross voxels not taken into account by previous models. We then show how asparse nonparametric method [J. Roy. Statist. Soc. Ser. B 71 (2009b) 1009-1030]can be used together with correlation screening to estimate nonlinear encodingmodels effectively. Our approach produces encoding models that predict about25% more accurately than models estimated using other methods [Nature 452(2008a) 352--355]. The estimated nonlinearity impacts the inferred propertiesof individual voxels, and it has a plausible biological interpretation. Onebenefit of quantitative encoding models is that estimated models can be used todecode brain activity, in order to identify which specific image was seen by anobserver. Encoding models estimated by our approach also improve such imageidentification by about 12% when the correct image is one of 11,500 possibleimages.

A Dirty Model for Multiple Sparse Regression

  Sparse linear regression -- finding an unknown vector from linearmeasurements -- is now known to be possible with fewer samples than variables,via methods like the LASSO. We consider the multiple sparse linear regressionproblem, where several related vectors -- with partially shared support sets --have to be recovered. A natural question in this setting is whether one can usethe sharing to further decrease the overall number of samples required. A lineof recent research has studied the use of \ell_1/\ell_q normblock-regularizations with q>1 for such problems; however these could actuallyperform worse in sample complexity -- vis a vis solving each problem separatelyignoring sharing -- depending on the level of sharing.  We present a new method for multiple sparse linear regression that canleverage support and parameter overlap when it exists, but not pay a penaltywhen it does not. A very simple idea: we decompose the parameters into twocomponents and regularize these differently. We show both theoretically andempirically, our method strictly and noticeably outperforms both \ell_1 or\ell_1/\ell_q methods, over the entire range of possible overlaps (except atboundary cases, where we match the best method). We also provide theoreticalguarantees that the method performs well under high-dimensional scaling.

Special section on statistics in neuroscience

  This article provides a brief introduction to seven papers that are includedin this special section on Statistics in Neuroscience: (1) Xiaoyan Shi, JosephG. Ibrahim, Jeffrey Lieberman, Martin Styner, Yimei Li and Hongtu Zhu:Two-state empirical likelihood for longitudinal neuroimaging data (2) VincentQ. Vu, Pradeep Ravikumar, Thomas Naselaris, Kendrick N. Kay, Jack L. Gallantand Bin Yu: Encoding and decoding V1 fMRI responses to natural images withsparse nonparametric models (3) Sourabh Bhattacharya and Ranjan Maitra: Anonstationary nonparametric Bayesian approach to dynamically modeling effectiveconnectivity in functional magnetic resonance imaging experiments (4)Christopher J. Long, Patrick L. Purdon, Simona Temereanca, Neil U. Desai, MattiS. H\"{a}m\"{a}l\"{a}inen and Emery Neal Brown: State-space solutions to thedynamic magnetoencephalography inverse problem using high performance computing(5) Yuriy Mishchencko, Joshua T. Vogelstein and Liam Paninski: A Bayesianapproach for inferring neuronal connectivity from calcium fluorescent imagingdata (6) Robert E. Kass, Ryan C. Kelly and Wei-Liem Loh: Assessment ofsynchrony in multiple neural spike trains using loglinear point process models(7) Sofia Olhede and Brandon Whitcher: Nonparametric tests of structure forhigh angular resolution diffusion imaging in Q-space

High-dimensional Sparse Inverse Covariance Estimation using Greedy  Methods

  In this paper we consider the task of estimating the non-zero pattern of thesparse inverse covariance matrix of a zero-mean Gaussian random vector from aset of iid samples. Note that this is also equivalent to recovering theunderlying graph structure of a sparse Gaussian Markov Random Field (GMRF). Wepresent two novel greedy approaches to solving this problem. The firstestimates the non-zero covariates of the overall inverse covariance matrixusing a series of global forward and backward greedy steps. The secondestimates the neighborhood of each node in the graph separately, again usinggreedy forward and backward steps, and combines the intermediate neighborhoodsto form an overall estimate. The principal contribution of this paper is arigorous analysis of the sparsistency, or consistency in recovering thesparsity pattern of the inverse covariance matrix. Surprisingly, we show thatboth the local and global greedy methods learn the full structure of the modelwith high probability given just $O(d\log(p))$ samples, which is a\emph{significant} improvement over state of the art $\ell_1$-regularizedGaussian MLE (Graphical Lasso) that requires $O(d^2\log(p))$ samples. Moreover,the restricted eigenvalue and smoothness conditions imposed by our greedymethods are much weaker than the strong irrepresentable conditions required bythe $\ell_1$-regularization based methods. We corroborate our results withextensive simulations and examples, comparing our local and global greedymethods to the $\ell_1$-regularized Gaussian MLE as well as the NeighborhoodGreedy method to that of nodewise $\ell_1$-regularized linear regression(Neighborhood Lasso).

A General Framework for Mixed Graphical Models

  "Mixed Data" comprising a large number of heterogeneous variables (e.g.count, binary, continuous, skewed continuous, among other data types) areprevalent in varied areas such as genomics and proteomics, imaging genetics,national security, social networking, and Internet advertising. There have beenlimited efforts at statistically modeling such mixed data jointly, in partbecause of the lack of computationally amenable multivariate distributions thatcan capture direct dependencies between such mixed variables of differenttypes. In this paper, we address this by introducing a novel class of BlockDirected Markov Random Fields (BDMRFs). Using the basic building block ofnode-conditional univariate exponential families from Yang et al. (2012), weintroduce a class of mixed conditional random field distributions, that arethen chained according to a block-directed acyclic graph to form our class ofBlock Directed Markov Random Fields (BDMRFs). The Markov independence graphstructure underlying a BDMRF thus has both directed and undirected edges. Weintroduce conditions under which these distributions exist and arenormalizable, study several instances of our models, and propose scalablepenalized conditional likelihood estimators with statistical guarantees forrecovering the underlying network structure. Simulations as well as anapplication to learning mixed genomic networks from next generation sequencingexpression data and mutation data demonstrate the versatility of our methods.

Generalized Root Models: Beyond Pairwise Graphical Models for Univariate  Exponential Families

  We present a novel k-way high-dimensional graphical model called theGeneralized Root Model (GRM) that explicitly models dependencies betweenvariable sets of size k > 2---where k = 2 is the standard pairwise graphicalmodel. This model is based on taking the k-th root of the original sufficientstatistics of any univariate exponential family with positive sufficientstatistics, including the Poisson and exponential distributions. As in therecent work with square root graphical (SQR) models [Inouye et al.2016]---which was restricted to pairwise dependencies---we give the conditionsof the parameters that are needed for normalization using the radialconditionals similar to the pairwise case [Inouye et al. 2016]. In particular,we show that the Poisson GRM has no restrictions on the parameters and theexponential GRM only has a restriction akin to negative definiteness. Wedevelop a simple but general learning algorithm based on L1-regularizednode-wise regressions. We also present a general way of numericallyapproximating the log partition function and associated derivatives of the GRMunivariate node conditionals---in contrast to [Inouye et al. 2016], which onlyprovided algorithm for estimating the exponential SQR. To illustrate GRM, wemodel word counts with a Poisson GRM and show the associated k-sized variablesets. We finish by discussing methods for reducing the parameter space invarious situations.

A Review of Multivariate Distributions for Count Data Derived from the  Poisson Distribution

  The Poisson distribution has been widely studied and used for modelingunivariate count-valued data. Multivariate generalizations of the Poissondistribution that permit dependencies, however, have been far less popular.Yet, real-world high-dimensional count-valued data found in word counts,genomics, and crime statistics, for example, exhibit rich dependencies, andmotivate the need for multivariate distributions that can appropriately modelthis data. We review multivariate distributions derived from the univariatePoisson, categorizing these models into three main classes: 1) where themarginal distributions are Poisson, 2) where the joint distribution is amixture of independent multivariate Poisson distributions, and 3) where thenode-conditional distributions are derived from the Poisson. We discuss thedevelopment of multiple instances of these classes and compare the models interms of interpretability and theory. Then, we empirically compare multiplemodels from each class on three real-world datasets that have varying datacharacteristics from different domains, namely traffic accident data,biological next generation sequencing data, and text data. These empiricalexperiments develop intuition about the comparative advantages anddisadvantages of each class of multivariate distribution that was derived fromthe Poisson. Finally, we suggest new research directions as explored in thesubsequent discussion section.

Online Classification with Complex Metrics

  We present a framework and analysis of consistent binary classification forcomplex and non-decomposable performance metrics such as the F-measure and theJaccard measure. The proposed framework is general, as it applies to both batchand online learning, and to both linear and non-linear models. Our work followsrecent results showing that the Bayes optimal classifier for many complexmetrics is given by a thresholding of the conditional probability of thepositive class. This manuscript extends this thresholding characterization --showing that the utility is strictly locally quasi-concave with respect to thethreshold for a wide range of models and performance metrics. This, in turn,motivates simple normalized gradient ascent updates for threshold estimation.We present a finite-sample regret analysis for the resulting procedure. Inparticular, the risk for the batch case converges to the Bayes risk at the samerate as that of the underlying conditional probability estimation, and the riskof proposed online algorithm converges at a rate that depends on theconditional probability estimation risk. For instance, in the special casewhere the conditional probability model is logistic regression, our procedureachieves $O(\frac{1}{\sqrt{n}})$ sample complexity, both for batch and onlinetraining. Empirical evaluation shows that the proposed algorithms out-performalternatives in practice, with comparable or better prediction performance andreduced run time for various metrics and datasets.

Identifiability of Nonparametric Mixture Models and Bayes Optimal  Clustering

  Motivated by problems in data clustering, we establish general conditionsunder which families of nonparametric mixture models are identifiable, byintroducing a novel framework involving clustering overfitted \emph{parametric}(i.e. misspecified) mixture models. These identifiability conditions generalizeexisting conditions in the literature, and are flexible enough to include forexample mixtures of Gaussian mixtures. In contrast to the recent literature onestimating nonparametric mixtures, we allow for general nonparametric mixturecomponents, and instead impose regularity assumptions on the underlying mixingmeasure. As our primary application, we apply these results to partition-basedclustering, generalizing the notion of a Bayes optimal partition from classicalparametric model-based clustering to nonparametric settings. Furthermore, thisframework is constructive so that it yields a practical algorithm for learningidentified mixtures, which is illustrated through several examples on realdata. The key conceptual device in the analysis is the convex, metric geometryof probability measures on metric spaces and its connection to the Wassersteinconvergence of mixing measures. The result is a flexible framework fornonparametric clustering with formal consistency guarantees.

D2KE: From Distance to Kernel and Embedding

  For many machine learning problem settings, particularly with structuredinputs such as sequences or sets of objects, a distance measure between inputscan be specified more naturally than a feature representation. However, moststandard machine models are designed for inputs with a vector featurerepresentation. In this work, we consider the estimation of a function$f:\mathcal{X} \rightarrow \R$ based solely on a dissimilarity measure$d:\mathcal{X}\times\mathcal{X} \rightarrow \R$ between inputs. In particular,we propose a general framework to derive a family of \emph{positive definitekernels} from a given dissimilarity measure, which subsumes the widely-used\emph{representative-set method} as a special case, and relates to thewell-known \emph{distance substitution kernel} in a limiting case. We show thatfunctions in the corresponding Reproducing Kernel Hilbert Space (RKHS) areLipschitz-continuous w.r.t. the given distance metric. We provide a tractablealgorithm to estimate a function from this RKHS, and show that it enjoys bettergeneralizability than Nearest-Neighbor estimates. Our approach draws from theliterature of Random Features, but instead of deriving feature maps from anexisting kernel, we construct novel kernels from a random feature map, that wespecify given the distance measure. We conduct classification experiments withsuch disparate domains as strings, time series, and sets of vectors, where ourproposed framework compares favorably to existing distance-based learningmethods such as $k$-nearest-neighbors, distance-substitution kernels,pseudo-Euclidean embedding, and the representative-set method.

How Sensitive are Sensitivity-Based Explanations?

  We propose a simple objective evaluation measure for explanations of acomplex black-box machine learning model. While most such model explanationshave largely been evaluated via qualitative measures, such as how humans mightqualitatively perceive the explanations, it is vital to also consider objectivemeasures such as the one we propose in this paper. Our evaluation measure thatwe naturally call sensitivity is simple: it characterizes how an explanationchanges as we vary the test input, and depending on how we measure thesechanges, and how we vary the input, we arrive at different notions ofsensitivity. We also provide a calculus for deriving sensitivity of complexexplanations in terms of that for simpler explanations, which thus allows aneasy computation of sensitivities for yet to be proposed explanations. Oneadvantage of an objective evaluation measure is that we can optimize theexplanation with respect to the measure: we show that (1) any given explanationcan be simply modified to improve its sensitivity with just a modest deviationfrom the original explanation, and (2) gradient based explanations of anadversarially trained network are less sensitive. Perhaps surprisingly, ourexperiments show that explanations optimized to have lower sensitivity can bemore faithful to the model predictions.

