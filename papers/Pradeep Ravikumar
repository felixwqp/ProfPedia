Variational Chernoff Bounds for Graphical Models

  Recent research has made significant progress on the problem of bounding log
partition functions for exponential family graphical models. Such bounds have
associated dual parameters that are often used as heuristic estimates of the
marginal probabilities required in inference and learning. However these
variational estimates do not give rigorous bounds on marginal probabilities,
nor do they give estimates for probabilities of more general events than simple
marginals. In this paper we build on this recent work by deriving rigorous
upper and lower bounds on event probabilities for graphical models. Our
approach is based on the use of generalized Chernoff bounds to express bounds
on event probabilities in terms of convex optimization problems; these
optimization problems, in turn, require estimates of generalized log partition
functions. Simulations indicate that this technique can result in useful,
rigorous bounds to complement the heuristic variational estimates, with
comparable computational cost.


A Hierarchical Graphical Model for Record Linkage

  The task of matching co-referent records is known among other names as rocord
linkage. For large record-linkage problems, often there is little or no labeled
data available, but unlabeled data shows a reasonable clear structure. For such
problems, unsupervised or semi-supervised methods are preferable to supervised
methods. In this paper, we describe a hierarchical graphical model framework
for the linakge-problem in an unsupervised setting. In addition to proposing
new methods, we also cast existing unsupervised probabilistic record-linkage
methods in this framework. Some of the techniques we propose to minimize
overfitting in the above model are of interest in the general graphical model
setting. We describe a method for incorporating monotinicity constraints in a
graphical model. We also outline a bootstrapping approach of using
"single-field" classifiers to noisily label latent variables in a hierarchical
model. Experimental results show that our proposed unsupervised methods perform
quite competitively even with fully supervised record-linkage methods.


Error-Correcting Tournaments

  We present a family of pairwise tournaments reducing $k$-class classification
to binary classification. These reductions are provably robust against a
constant fraction of binary errors. The results improve on the PECOC
construction \cite{SECOC} with an exponential improvement in computation, from
$O(k)$ to $O(\log_2 k)$, and the removal of a square root in the regret
dependence, matching the best possible computation and regret up to a constant.


Information-theoretic lower bounds on the oracle complexity of
  stochastic convex optimization

  Relative to the large literature on upper bounds on complexity of convex
optimization, lesser attention has been paid to the fundamental hardness of
these problems. Given the extensive use of convex optimization in machine
learning and statistics, gaining an understanding of these complexity-theoretic
issues is important. In this paper, we study the complexity of stochastic
convex optimization in an oracle model of computation. We improve upon known
results and obtain tight minimax complexity estimates for various function
classes.


Sparsistency of $\ell_1$-Regularized $M$-Estimators

  We consider the model selection consistency or sparsistency of a broad set of
$\ell_1$-regularized $M$-estimators for linear and non-linear statistical
models in a unified fashion. For this purpose, we propose the local structured
smoothness condition (LSSC) on the loss function. We provide a general result
giving deterministic sufficient conditions for sparsistency in terms of the
regularization parameter, ambient dimension, sparsity level, and number of
measurements. We show that several important statistical models have
$M$-estimators that indeed satisfy the LSSC, and as a result, the sparsistency
guarantees for the corresponding $\ell_1$-regularized $M$-estimators can be
derived as simple applications of our main theorem.


Towards Aggregating Weighted Feature Attributions

  Current approaches for explaining machine learning models fall into two
distinct classes: antecedent event influence and value attribution. The former
leverages training instances to describe how much influence a training point
exerts on a test point, while the latter attempts to attribute value to the
features most pertinent to a given prediction. In this work, we discuss an
algorithm, AVA: Aggregate Valuation of Antecedents, that fuses these two
explanation classes to form a new approach to feature attribution that not only
retrieves local explanations but also captures global patterns learned by a
model. Our experimentation convincingly favors weighting and aggregating
feature attributions via AVA.


High-dimensional Ising model selection using ${\ell_1}$-regularized
  logistic regression

  We consider the problem of estimating the graph associated with a binary
Ising Markov random field. We describe a method based on $\ell_1$-regularized
logistic regression, in which the neighborhood of any given node is estimated
by performing logistic regression subject to an $\ell_1$-constraint. The method
is analyzed under high-dimensional scaling in which both the number of nodes
$p$ and maximum neighborhood size $d$ are allowed to grow as a function of the
number of observations $n$. Our main results provide sufficient conditions on
the triple $(n,p,d)$ and the model parameters for the method to succeed in
consistently estimating the neighborhood of every node in the graph
simultaneously. With coherence conditions imposed on the population Fisher
information matrix, we prove that consistent neighborhood selection can be
obtained for sample sizes $n=\Omega(d^3\log p)$ with exponentially decaying
error. When these same conditions are imposed directly on the sample matrices,
we show that a reduced sample size of $n=\Omega(d^2\log p)$ suffices for the
method to estimate neighborhoods consistently. Although this paper focuses on
the binary graphical models, we indicate how a generalization of the method of
the paper would apply to general discrete Markov random fields.


Exponential Family Matrix Completion under Structural Constraints

  We consider the matrix completion problem of recovering a structured matrix
from noisy and partial measurements. Recent works have proposed tractable
estimators with strong statistical guarantees for the case where the underlying
matrix is low--rank, and the measurements consist of a subset, either of the
exact individual entries, or of the entries perturbed by additive Gaussian
noise, which is thus implicitly suited for thin--tailed continuous data.
Arguably, common applications of matrix completion require estimators for (a)
heterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)
for heterogeneous noise models (beyond Gaussian), which capture varied
uncertainty in the measurements, and (c) heterogeneous structural constraints
beyond low--rank, such as block--sparsity, or a superposition structure of
low--rank plus elementwise sparseness, among others. In this paper, we provide
a vastly unified framework for generalized matrix completion by considering a
matrix completion setting wherein the matrix entries are sampled from any
member of the rich family of exponential family distributions; and impose
general structural constraints on the underlying matrix, as captured by a
general regularizer $\mathcal{R}(.)$. We propose a simple convex regularized
$M$--estimator for the generalized framework, and provide a unified and novel
statistical analysis for this general class of estimators. We finally
corroborate our theoretical results on simulated datasets.


High-dimensional covariance estimation by minimizing $\ell_1$-penalized
  log-determinant divergence

  Given i.i.d. observations of a random vector $X \in \mathbb{R}^p$, we study
the problem of estimating both its covariance matrix $\Sigma^*$, and its
inverse covariance or concentration matrix {$\Theta^* = (\Sigma^*)^{-1}$.} We
estimate $\Theta^*$ by minimizing an $\ell_1$-penalized log-determinant Bregman
divergence; in the multivariate Gaussian case, this approach corresponds to
$\ell_1$-penalized maximum likelihood, and the structure of $\Theta^*$ is
specified by the graph of an associated Gaussian Markov random field. We
analyze the performance of this estimator under high-dimensional scaling, in
which the number of nodes in the graph $p$, the number of edges $s$ and the
maximum node degree $d$, are allowed to grow as a function of the sample size
$n$. In addition to the parameters $(p,s,d)$, our analysis identifies other key
quantities covariance matrix $\Sigma^*$; and (b) the $\ell_\infty$ operator
norm of the sub-matrix $\Gamma^*_{S S}$, where $S$ indexes the graph edges, and
$\Gamma^* = (\Theta^*)^{-1} \otimes (\Theta^*)^{-1}$; and (c) a mutual
incoherence or irrepresentability measure on the matrix $\Gamma^*$ and (d) the
rate of decay $1/f(n,\delta)$ on the probabilities $ \{|\hat{\Sigma}^n_{ij}-
\Sigma^*_{ij}| > \delta \}$, where $\hat{\Sigma}^n$ is the sample covariance
based on $n$ samples. Our first result establishes consistency of our estimate
$\hat{\Theta}$ in the elementwise maximum-norm. This in turn allows us to
derive convergence rates in Frobenius and spectral norms, with improvements
upon existing results for graphs with maximum node degrees $d = o(\sqrt{s})$.
In our second result, we show that with probability converging to one, the
estimate $\hat{\Theta}$ correctly specifies the zero pattern of the
concentration matrix $\Theta^*$.


Sparse Additive Models

  We present a new class of methods for high-dimensional nonparametric
regression and classification called sparse additive models (SpAM). Our methods
combine ideas from sparse linear modeling and additive nonparametric
regression. We derive an algorithm for fitting the models that is practical and
effective even when the number of covariates is larger than the sample size.
SpAM is closely related to the COSSO model of Lin and Zhang (2006), but
decouples smoothing and sparsity, enabling the use of arbitrary nonparametric
smoothers. An analysis of the theoretical properties of SpAM is given. We also
study a greedy estimator that is a nonparametric version of forward stepwise
regression. Empirical results on synthetic and real data are presented, showing
that SpAM can be effective in fitting sparse nonparametric models in high
dimensional data.


High-Dimensional Graphical Model Selection Using $\ell_1$-Regularized
  Logistic Regression

  We consider the problem of estimating the graph structure associated with a
discrete Markov random field. We describe a method based on
$\ell_1$-regularized logistic regression, in which the neighborhood of any
given node is estimated by performing logistic regression subject to an
$\ell_1$-constraint. Our framework applies to the high-dimensional setting, in
which both the number of nodes $p$ and maximum neighborhood sizes $d$ are
allowed to grow as a function of the number of observations $n$. Our main
results provide sufficient conditions on the triple $(n, p, d)$ for the method
to succeed in consistently estimating the neighborhood of every node in the
graph simultaneously. Under certain assumptions on the population Fisher
information matrix, we prove that consistent neighborhood selection can be
obtained for sample sizes $n = \Omega(d^3 \log p)$, with the error decaying as
$\order(\exp(-C n/d^3))$ for some constant $C$. If these same assumptions are
imposed directly on the sample matrices, we show that $n = \Omega(d^2 \log p)$
samples are sufficient.


On Learning Discrete Graphical Models Using Greedy Methods

  In this paper, we address the problem of learning the structure of a pairwise
graphical model from samples in a high-dimensional setting. Our first main
result studies the sparsistency, or consistency in sparsity pattern recovery,
properties of a forward-backward greedy algorithm as applied to general
statistical models. As a special case, we then apply this algorithm to learn
the structure of a discrete graphical model via neighborhood estimation. As a
corollary of our general result, we derive sufficient conditions on the number
of samples n, the maximum node-degree d and the problem size p, as well as
other conditions on the model parameters, so that the algorithm recovers all
the edges with high probability. Our result guarantees graph selection for
samples scaling as n = Omega(d^2 log(p)), in contrast to existing
convex-optimization based algorithms that require a sample complexity of
\Omega(d^3 log(p)). Further, the greedy algorithm only requires a restricted
strong convexity condition which is typically milder than irrepresentability
assumptions. We corroborate these results using numerical simulations at the
end.


Sparse Inverse Covariance Matrix Estimation Using Quadratic
  Approximation

  The L1-regularized Gaussian maximum likelihood estimator (MLE) has been shown
to have strong statistical guarantees in recovering a sparse inverse covariance
matrix, or alternatively the underlying graph structure of a Gaussian Markov
Random Field, from very limited samples. We propose a novel algorithm for
solving the resulting optimization problem which is a regularized
log-determinant program. In contrast to recent state-of-the-art methods that
largely use first order gradient information, our algorithm is based on
Newton's method and employs a quadratic approximation, but with some
modifications that leverage the structure of the sparse Gaussian MLE problem.
We show that our method is superlinearly convergent, and present experimental
results using synthetic and real-world application data that demonstrate the
considerable improvements in performance of our method when compared to other
state-of-the-art methods.


On Graphical Models via Univariate Exponential Family Distributions

  Undirected graphical models, or Markov networks, are a popular class of
statistical models, used in a wide variety of applications. Popular instances
of this class include Gaussian graphical models and Ising models. In many
settings, however, it might not be clear which subclass of graphical models to
use, particularly for non-Gaussian and non-categorical data. In this paper, we
consider a general sub-class of graphical models where the node-wise
conditional distributions arise from exponential families. This allows us to
derive multivariate graphical model distributions from univariate exponential
family distributions, such as the Poisson, negative binomial, and exponential
distributions. Our key contributions include a class of M-estimators to fit
these graphical model distributions; and rigorous statistical analysis showing
that these M-estimators recover the true graphical model structure exactly,
with high probability. We provide examples of genomic and proteomic networks
learned via instances of our class of graphical models derived from Poisson and
exponential distributions.


Optimal Decision-Theoretic Classification Using Non-Decomposable
  Performance Metrics

  We provide a general theoretical analysis of expected out-of-sample utility,
also referred to as decision-theoretic classification, for non-decomposable
binary classification metrics such as F-measure and Jaccard coefficient. Our
key result is that the expected out-of-sample utility for many performance
metrics is provably optimized by a classifier which is equivalent to a signed
thresholding of the conditional probability of the positive class. Our analysis
bridges a gap in the literature on binary classification, revealed in light of
recent results for non-decomposable metrics in population utility maximization
style classification. Our results identify checkable properties of a
performance metric which are sufficient to guarantee a probability ranking
principle. We propose consistent estimators for optimal expected out-of-sample
classification. As a consequence of the probability ranking principle,
computational requirements can be reduced from exponential to cubic complexity
in the general case, and further reduced to quadratic complexity in special
cases. We provide empirical results on simulated and benchmark datasets
evaluating the performance of the proposed algorithms for decision-theoretic
classification and comparing them to baseline and state-of-the-art methods in
population utility maximization for non-decomposable metrics.


Vector-Space Markov Random Fields via Exponential Families

  We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of
undirected graphical models where each variable can belong to an arbitrary
vector space. VS-MRFs generalize a recent line of work on scalar-valued,
uni-parameter exponential family and mixed graphical models, thereby greatly
broadening the class of exponential families available (e.g., allowing
multinomial and Dirichlet distributions). Specifically, VS-MRFs are the joint
graphical model distributions where the node-conditional distributions belong
to generic exponential families with general vector space domains. We also
present a sparsistent $M$-estimator for learning our class of MRFs that
recovers the correct set of edges with high probability. We validate our
approach via a set of synthetic data experiments as well as a real-world case
study of over four million foods from the popular diet tracking app
MyFitnessPal. Our results demonstrate that our algorithm performs well
empirically and that VS-MRFs are capable of capturing and highlighting
interesting structure in complex, real-world data. All code for our algorithm
is open source and publicly available.


Virus Dynamics on Starlike Graphs

  The field of epidemiology has presented fascinating and relevant questions
for mathematicians, primarily concerning the spread of viruses in a community.
The importance of this research has greatly increased over time as its
applications have expanded to also include studies of electronic and social
networks and the spread of information and ideas. We study virus propagation on
a non-linear hub and spoke graph (which models well many airline networks). We
determine the long-term behavior as a function of the cure and infection rates,
as well as the number of spokes n. For each n we prove the existence of a
critical threshold relating the two rates. Below this threshold, the virus
always dies out; above this threshold, all non-trivial initial conditions
iterate to a unique non-trivial steady state. We end with some generalizations
to other networks.


Proximal Quasi-Newton for Computationally Intensive L1-regularized
  M-estimators

  We consider the class of optimization problems arising from computationally
intensive L1-regularized M-estimators, where the function or gradient values
are very expensive to compute. A particular instance of interest is the
L1-regularized MLE for learning Conditional Random Fields (CRFs), which are a
popular class of statistical models for varied structured prediction problems
such as sequence labeling, alignment, and classification with label taxonomy.
L1-regularized MLEs for CRFs are particularly expensive to optimize since
computing the gradient values requires an expensive inference step. In this
work, we propose the use of a carefully constructed proximal quasi-Newton
algorithm for such computationally intensive M-estimation problems, where we
employ an aggressive active set selection technique. In a key contribution of
the paper, we show that the proximal quasi-Newton method is provably
super-linearly convergent, even in the absence of strong convexity, by
leveraging a restricted variant of strong convexity. In our experiments, the
proposed algorithm converges considerably faster than current state-of-the-art
on the problems of sequence labeling and hierarchical classification.


Square Root Graphical Models: Multivariate Generalizations of Univariate
  Exponential Families that Permit Positive Dependencies

  We develop Square Root Graphical Models (SQR), a novel class of parametric
graphical models that provides multivariate generalizations of univariate
exponential family distributions. Previous multivariate graphical models [Yang
et al. 2015] did not allow positive dependencies for the exponential and
Poisson generalizations. However, in many real-world datasets, variables
clearly have positive dependencies. For example, the airport delay time in New
York---modeled as an exponential distribution---is positively related to the
delay time in Boston. With this motivation, we give an example of our model
class derived from the univariate exponential distribution that allows for
almost arbitrary positive and negative dependencies with only a mild condition
on the parameter matrix---a condition akin to the positive definiteness of the
Gaussian covariance matrix. Our Poisson generalization allows for both positive
and negative dependencies without any constraints on the parameter values. We
also develop parameter estimation methods using node-wise regressions with
$\ell_1$ regularization and likelihood approximation methods using sampling.
Finally, we demonstrate our exponential generalization on a synthetic dataset
and a real-world dataset of airport delay times.


On the Information Theoretic Limits of Learning Ising Models

  We provide a general framework for computing lower-bounds on the sample
complexity of recovering the underlying graphs of Ising models, given i.i.d
samples. While there have been recent results for specific graph classes, these
involve fairly extensive technical arguments that are specialized to each
specific graph class. In contrast, we isolate two key graph-structural
ingredients that can then be used to specify sample complexity lower-bounds.
Presence of these structural properties makes the graph class hard to learn. We
derive corollaries of our main result that not only recover existing recent
results, but also provide lower bounds for novel graph classes not considered
previously. We also extend our framework to the random graph setting and derive
corollaries for Erd\H{o}s-R\'{e}nyi graphs in a certain dense setting.


Kernel Ridge Regression via Partitioning

  In this paper, we investigate a divide and conquer approach to Kernel Ridge
Regression (KRR). Given n samples, the division step involves separating the
points based on some underlying disjoint partition of the input space (possibly
via clustering), and then computing a KRR estimate for each partition. The
conquering step is simple: for each partition, we only consider its own local
estimate for prediction. We establish conditions under which we can give
generalization bounds for this estimator, as well as achieve optimal minimax
rates. We also show that the approximation error component of the
generalization error is lesser than when a single KRR estimate is fit on the
data: thus providing both statistical and computational advantages over a
single KRR estimate over the entire data (or an averaging over random
partitions as in other recent work, [30]). Lastly, we provide experimental
validation for our proposed estimator and our assumptions.


A Voting-Based System for Ethical Decision Making

  We present a general approach to automating ethical decisions, drawing on
machine learning and computational social choice. In a nutshell, we propose to
learn a model of societal preferences, and, when faced with a specific ethical
dilemma at runtime, efficiently aggregate those preferences to identify a
desirable choice. We provide a concrete algorithm that instantiates our
approach; some of its crucial steps are informed by a new theory of
swap-dominance efficient voting rules. Finally, we implement and evaluate a
system for ethical decision making in the autonomous vehicle domain, using
preference data collected from 1.3 million people through the Moral Machine
website.


Robust Estimation via Robust Gradient Estimation

  We provide a new computationally-efficient class of estimators for risk
minimization. We show that these estimators are robust for general statistical
models: in the classical Huber epsilon-contamination model and in heavy-tailed
settings. Our workhorse is a novel robust variant of gradient descent, and we
provide conditions under which our gradient descent variant provides accurate
estimators in a general convex risk minimization problem. We provide specific
consequences of our theory for linear regression, logistic regression and for
estimation of the canonical parameters in an exponential family. These results
provide some of the first computationally tractable and provably robust
estimators for these canonical statistical models. Finally, we study the
empirical performance of our proposed methods on synthetic and real datasets,
and find that our methods convincingly outperform a variety of baselines.


DAGs with NO TEARS: Continuous Optimization for Structure Learning

  Estimating the structure of directed acyclic graphs (DAGs, also known as
Bayesian networks) is a challenging problem since the search space of DAGs is
combinatorial and scales superexponentially with the number of nodes. Existing
approaches rely on various local heuristics for enforcing the acyclicity
constraint. In this paper, we introduce a fundamentally different strategy: We
formulate the structure learning problem as a purely \emph{continuous}
optimization problem over real matrices that avoids this combinatorial
constraint entirely. This is achieved by a novel characterization of acyclicity
that is not only smooth but also exact. The resulting problem can be
efficiently solved by standard numerical algorithms, which also makes
implementation effortless. The proposed method outperforms existing ones,
without imposing any structural assumptions on the graph such as bounded
treewidth or in-degree. Code implementing the proposed algorithm is open-source
and publicly available at https://github.com/xunzheng/notears.


Robust Nonparametric Regression under Huber's $Îµ$-contamination
  Model

  We consider the non-parametric regression problem under Huber's
$\epsilon$-contamination model, in which an $\epsilon$ fraction of observations
are subject to arbitrary adversarial noise. We first show that a simple local
binning median step can effectively remove the adversary noise and this median
estimator is minimax optimal up to absolute constants over the H\"{o}lder
function class with smoothness parameters smaller than or equal to 1.
Furthermore, when the underlying function has higher smoothness, we show that
using local binning median as pre-preprocessing step to remove the adversarial
noise, then we can apply any non-parametric estimator on top of the medians. In
particular we show local median binning followed by kernel smoothing and local
polynomial regression achieve minimaxity over H\"{o}lder and Sobolev classes
with arbitrary smoothness parameters. Our main proof technique is a decoupled
analysis of adversary noise and stochastic noise, which can be potentially
applied to other robust estimation problems. We also provide numerical results
to verify the effectiveness of our proposed methods.


Binary Classification with Karmic, Threshold-Quasi-Concave Metrics

  Complex performance measures, beyond the popular measure of accuracy, are
increasingly being used in the context of binary classification. These complex
performance measures are typically not even decomposable, that is, the loss
evaluated on a batch of samples cannot typically be expressed as a sum or
average of losses evaluated at individual samples, which in turn requires new
theoretical and methodological developments beyond standard treatments of
supervised learning. In this paper, we advance this understanding of binary
classification for complex performance measures by identifying two key
properties: a so-called Karmic property, and a more technical
threshold-quasi-concavity property, which we show is milder than existing
structural assumptions imposed on performance measures. Under these properties,
we show that the Bayes optimal classifier is a threshold function of the
conditional probability of positive class. We then leverage this result to come
up with a computationally practical plug-in classifier, via a novel threshold
estimator, and further, provide a novel statistical analysis of classification
error with respect to complex performance measures.


Revisiting Adversarial Risk

  Recent works on adversarial perturbations show that there is an inherent
trade-off between standard test accuracy and adversarial accuracy.
Specifically, they show that no classifier can simultaneously be robust to
adversarial perturbations and achieve high standard test accuracy. However,
this is contrary to the standard notion that on tasks such as image
classification, humans are robust classifiers with low error rate. In this
work, we show that the main reason behind this confusion is the inexact
definition of adversarial perturbation that is used in the literature. To fix
this issue, we propose a slight, yet important modification to the existing
definition of adversarial perturbation. Based on the modified definition, we
show that there is no trade-off between adversarial and standard accuracies;
there exist classifiers that are robust and achieve high standard accuracy. We
further study several properties of this new definition of adversarial risk and
its relation to the existing definition.


Sample Complexity of Nonparametric Semi-Supervised Learning

  We study the sample complexity of semi-supervised learning (SSL) and
introduce new assumptions based on the mismatch between a mixture model learned
from unlabeled data and the true mixture model induced by the (unknown) class
conditional distributions. Under these assumptions, we establish an
$\Omega(K\log K)$ labeled sample complexity bound without imposing parametric
assumptions, where $K$ is the number of classes. Our results suggest that even
in nonparametric settings it is possible to learn a near-optimal classifier
using only a few labeled samples. Unlike previous theoretical work which
focuses on binary classification, we consider general multiclass classification
($K>2$), which requires solving a difficult permutation learning problem. This
permutation defines a classifier whose classification error is controlled by
the Wasserstein distance between mixing measures, and we provide finite-sample
results characterizing the behaviour of the excess risk of this classifier.
Finally, we describe three algorithms for computing these estimators based on a
connection to bipartite graph matching, and perform experiments to illustrate
the superiority of the MLE over the majority vote estimator.


Learning Tensor Latent Features

  We study the problem of learning latent feature models (LFMs) for tensor data
commonly observed in science and engineering such as hyperspectral imagery.
However, the problem is challenging not only due to the non-convex formulation,
the combinatorial nature of the constraints in LFMs, but also the high-order
correlations in the data. In this work, we formulate a tensor latent feature
learning problem by representing the data as a mixture of high-order latent
features and binary codes, which are memory efficient and easy to interpret. To
make the learning tractable, we propose a novel optimization procedure, Binary
matching pursuit (BMP), that iteratively searches for binary bases via a
MAXCUT-like boolean quadratic solver. Such a procedure is guaranteed to achieve
an? suboptimal solution in O($1/\epsilon$) greedy steps, resulting in a
trade-off between accuracy and sparsity. When evaluated on both synthetic and
real datasets, our experiments show superior performance over baseline methods.


Word Mover's Embedding: From Word2Vec to Document Embedding

  While the celebrated Word2Vec technique yields semantically rich
representations for individual words, there has been relatively less success in
extending to generate unsupervised sentences or documents embeddings. Recent
work has demonstrated that a distance measure between documents called
\emph{Word Mover's Distance} (WMD) that aligns semantically similar words,
yields unprecedented KNN classification accuracy. However, WMD is expensive to
compute, and it is hard to extend its use beyond a KNN classifier. In this
paper, we propose the \emph{Word Mover's Embedding } (WME), a novel approach to
building an unsupervised document (sentence) embedding from pre-trained word
embeddings. In our experiments on 9 benchmark text classification datasets and
22 textual similarity tasks, the proposed technique consistently matches or
outperforms state-of-the-art techniques, with significantly higher accuracy on
problems of short length.


Representer Point Selection for Explaining Deep Neural Networks

  We propose to explain the predictions of a deep neural network, by pointing
to the set of what we call representer points in the training set, for a given
test point prediction. Specifically, we show that we can decompose the
pre-activation prediction of a neural network into a linear combination of
activations of training points, with the weights corresponding to what we call
representer values, which thus capture the importance of that training point on
the learned parameters of the network. But it provides a deeper understanding
of the network than simply training point influence: with positive representer
values corresponding to excitatory training points, and negative values
corresponding to inhibitory points, which as we show provides considerably more
insight. Our method is also much more scalable, allowing for real-time feedback
in a manner not feasible with influence functions.


Adaptive Hard Thresholding for Near-optimal Consistent Robust Regression

  We study the problem of robust linear regression with response variable
corruptions. We consider the oblivious adversary model, where the adversary
corrupts a fraction of the responses in complete ignorance of the data. We
provide a nearly linear time estimator which consistently estimates the true
regression vector, even with $1-o(1)$ fraction of corruptions. Existing results
in this setting either don't guarantee consistent estimates or can only handle
a small fraction of corruptions. We also extend our estimator to robust sparse
linear regression and show that similar guarantees hold in this setting.
Finally, we apply our estimator to the problem of linear regression with
heavy-tailed noise and show that our estimator consistently estimates the
regression vector even when the noise has unbounded variance (e.g., Cauchy
distribution), for which most existing results don't even apply. Our estimator
is based on a novel variant of outlier removal via hard thresholding in which
the threshold is chosen adaptively and crucially relies on randomness to escape
bad fixed points of the non-convex hard thresholding operation.


A Unified Framework for High-Dimensional Analysis of M-Estimators with
  Decomposable Regularizers

  High-dimensional statistical inference deals with models in which the the
number of parameters p is comparable to or larger than the sample size n. Since
it is usually impossible to obtain consistent procedures unless
$p/n\rightarrow0$, a line of recent work has studied models with various types
of low-dimensional structure, including sparse vectors, sparse and structured
matrices, low-rank matrices and combinations thereof. In such settings, a
general approach to estimation is to solve a regularized optimization problem,
which combines a loss function measuring how well the model fits the data with
some regularization function that encourages the assumed structure. This paper
provides a unified framework for establishing consistency and convergence rates
for such regularized M-estimators under high-dimensional scaling. We state one
main theorem and show how it can be used to re-derive some existing results,
and also to obtain a number of new results on consistency and convergence
rates, in both $\ell_2$-error and related norms. Our analysis also identifies
two key properties of loss and regularization functions, referred to as
restricted strong convexity and decomposability, that ensure corresponding
regularized M-estimators have fast convergence rates and which are optimal in
many well-studied cases.


Encoding and decoding V1 fMRI responses to natural images with sparse
  nonparametric models

  Functional MRI (fMRI) has become the most common method for investigating the
human brain. However, fMRI data present some complications for statistical
analysis and modeling. One recently developed approach to these data focuses on
estimation of computational encoding models that describe how stimuli are
transformed into brain activity measured in individual voxels. Here we aim at
building encoding models for fMRI signals recorded in the primary visual cortex
of the human brain. We use residual analyses to reveal systematic nonlinearity
across voxels not taken into account by previous models. We then show how a
sparse nonparametric method [J. Roy. Statist. Soc. Ser. B 71 (2009b) 1009-1030]
can be used together with correlation screening to estimate nonlinear encoding
models effectively. Our approach produces encoding models that predict about
25% more accurately than models estimated using other methods [Nature 452
(2008a) 352--355]. The estimated nonlinearity impacts the inferred properties
of individual voxels, and it has a plausible biological interpretation. One
benefit of quantitative encoding models is that estimated models can be used to
decode brain activity, in order to identify which specific image was seen by an
observer. Encoding models estimated by our approach also improve such image
identification by about 12% when the correct image is one of 11,500 possible
images.


Special section on statistics in neuroscience

  This article provides a brief introduction to seven papers that are included
in this special section on Statistics in Neuroscience: (1) Xiaoyan Shi, Joseph
G. Ibrahim, Jeffrey Lieberman, Martin Styner, Yimei Li and Hongtu Zhu:
Two-state empirical likelihood for longitudinal neuroimaging data (2) Vincent
Q. Vu, Pradeep Ravikumar, Thomas Naselaris, Kendrick N. Kay, Jack L. Gallant
and Bin Yu: Encoding and decoding V1 fMRI responses to natural images with
sparse nonparametric models (3) Sourabh Bhattacharya and Ranjan Maitra: A
nonstationary nonparametric Bayesian approach to dynamically modeling effective
connectivity in functional magnetic resonance imaging experiments (4)
Christopher J. Long, Patrick L. Purdon, Simona Temereanca, Neil U. Desai, Matti
S. H\"{a}m\"{a}l\"{a}inen and Emery Neal Brown: State-space solutions to the
dynamic magnetoencephalography inverse problem using high performance computing
(5) Yuriy Mishchencko, Joshua T. Vogelstein and Liam Paninski: A Bayesian
approach for inferring neuronal connectivity from calcium fluorescent imaging
data (6) Robert E. Kass, Ryan C. Kelly and Wei-Liem Loh: Assessment of
synchrony in multiple neural spike trains using loglinear point process models
(7) Sofia Olhede and Brandon Whitcher: Nonparametric tests of structure for
high angular resolution diffusion imaging in Q-space


Generalized Root Models: Beyond Pairwise Graphical Models for Univariate
  Exponential Families

  We present a novel k-way high-dimensional graphical model called the
Generalized Root Model (GRM) that explicitly models dependencies between
variable sets of size k > 2---where k = 2 is the standard pairwise graphical
model. This model is based on taking the k-th root of the original sufficient
statistics of any univariate exponential family with positive sufficient
statistics, including the Poisson and exponential distributions. As in the
recent work with square root graphical (SQR) models [Inouye et al.
2016]---which was restricted to pairwise dependencies---we give the conditions
of the parameters that are needed for normalization using the radial
conditionals similar to the pairwise case [Inouye et al. 2016]. In particular,
we show that the Poisson GRM has no restrictions on the parameters and the
exponential GRM only has a restriction akin to negative definiteness. We
develop a simple but general learning algorithm based on L1-regularized
node-wise regressions. We also present a general way of numerically
approximating the log partition function and associated derivatives of the GRM
univariate node conditionals---in contrast to [Inouye et al. 2016], which only
provided algorithm for estimating the exponential SQR. To illustrate GRM, we
model word counts with a Poisson GRM and show the associated k-sized variable
sets. We finish by discussing methods for reducing the parameter space in
various situations.


A Dirty Model for Multiple Sparse Regression

  Sparse linear regression -- finding an unknown vector from linear
measurements -- is now known to be possible with fewer samples than variables,
via methods like the LASSO. We consider the multiple sparse linear regression
problem, where several related vectors -- with partially shared support sets --
have to be recovered. A natural question in this setting is whether one can use
the sharing to further decrease the overall number of samples required. A line
of recent research has studied the use of \ell_1/\ell_q norm
block-regularizations with q>1 for such problems; however these could actually
perform worse in sample complexity -- vis a vis solving each problem separately
ignoring sharing -- depending on the level of sharing.
  We present a new method for multiple sparse linear regression that can
leverage support and parameter overlap when it exists, but not pay a penalty
when it does not. A very simple idea: we decompose the parameters into two
components and regularize these differently. We show both theoretically and
empirically, our method strictly and noticeably outperforms both \ell_1 or
\ell_1/\ell_q methods, over the entire range of possible overlaps (except at
boundary cases, where we match the best method). We also provide theoretical
guarantees that the method performs well under high-dimensional scaling.


High-dimensional Sparse Inverse Covariance Estimation using Greedy
  Methods

  In this paper we consider the task of estimating the non-zero pattern of the
sparse inverse covariance matrix of a zero-mean Gaussian random vector from a
set of iid samples. Note that this is also equivalent to recovering the
underlying graph structure of a sparse Gaussian Markov Random Field (GMRF). We
present two novel greedy approaches to solving this problem. The first
estimates the non-zero covariates of the overall inverse covariance matrix
using a series of global forward and backward greedy steps. The second
estimates the neighborhood of each node in the graph separately, again using
greedy forward and backward steps, and combines the intermediate neighborhoods
to form an overall estimate. The principal contribution of this paper is a
rigorous analysis of the sparsistency, or consistency in recovering the
sparsity pattern of the inverse covariance matrix. Surprisingly, we show that
both the local and global greedy methods learn the full structure of the model
with high probability given just $O(d\log(p))$ samples, which is a
\emph{significant} improvement over state of the art $\ell_1$-regularized
Gaussian MLE (Graphical Lasso) that requires $O(d^2\log(p))$ samples. Moreover,
the restricted eigenvalue and smoothness conditions imposed by our greedy
methods are much weaker than the strong irrepresentable conditions required by
the $\ell_1$-regularization based methods. We corroborate our results with
extensive simulations and examples, comparing our local and global greedy
methods to the $\ell_1$-regularized Gaussian MLE as well as the Neighborhood
Greedy method to that of nodewise $\ell_1$-regularized linear regression
(Neighborhood Lasso).


A General Framework for Mixed Graphical Models

  "Mixed Data" comprising a large number of heterogeneous variables (e.g.
count, binary, continuous, skewed continuous, among other data types) are
prevalent in varied areas such as genomics and proteomics, imaging genetics,
national security, social networking, and Internet advertising. There have been
limited efforts at statistically modeling such mixed data jointly, in part
because of the lack of computationally amenable multivariate distributions that
can capture direct dependencies between such mixed variables of different
types. In this paper, we address this by introducing a novel class of Block
Directed Markov Random Fields (BDMRFs). Using the basic building block of
node-conditional univariate exponential families from Yang et al. (2012), we
introduce a class of mixed conditional random field distributions, that are
then chained according to a block-directed acyclic graph to form our class of
Block Directed Markov Random Fields (BDMRFs). The Markov independence graph
structure underlying a BDMRF thus has both directed and undirected edges. We
introduce conditions under which these distributions exist and are
normalizable, study several instances of our models, and propose scalable
penalized conditional likelihood estimators with statistical guarantees for
recovering the underlying network structure. Simulations as well as an
application to learning mixed genomic networks from next generation sequencing
expression data and mutation data demonstrate the versatility of our methods.


A Review of Multivariate Distributions for Count Data Derived from the
  Poisson Distribution

  The Poisson distribution has been widely studied and used for modeling
univariate count-valued data. Multivariate generalizations of the Poisson
distribution that permit dependencies, however, have been far less popular.
Yet, real-world high-dimensional count-valued data found in word counts,
genomics, and crime statistics, for example, exhibit rich dependencies, and
motivate the need for multivariate distributions that can appropriately model
this data. We review multivariate distributions derived from the univariate
Poisson, categorizing these models into three main classes: 1) where the
marginal distributions are Poisson, 2) where the joint distribution is a
mixture of independent multivariate Poisson distributions, and 3) where the
node-conditional distributions are derived from the Poisson. We discuss the
development of multiple instances of these classes and compare the models in
terms of interpretability and theory. Then, we empirically compare multiple
models from each class on three real-world datasets that have varying data
characteristics from different domains, namely traffic accident data,
biological next generation sequencing data, and text data. These empirical
experiments develop intuition about the comparative advantages and
disadvantages of each class of multivariate distribution that was derived from
the Poisson. Finally, we suggest new research directions as explored in the
subsequent discussion section.


Online Classification with Complex Metrics

  We present a framework and analysis of consistent binary classification for
complex and non-decomposable performance metrics such as the F-measure and the
Jaccard measure. The proposed framework is general, as it applies to both batch
and online learning, and to both linear and non-linear models. Our work follows
recent results showing that the Bayes optimal classifier for many complex
metrics is given by a thresholding of the conditional probability of the
positive class. This manuscript extends this thresholding characterization --
showing that the utility is strictly locally quasi-concave with respect to the
threshold for a wide range of models and performance metrics. This, in turn,
motivates simple normalized gradient ascent updates for threshold estimation.
We present a finite-sample regret analysis for the resulting procedure. In
particular, the risk for the batch case converges to the Bayes risk at the same
rate as that of the underlying conditional probability estimation, and the risk
of proposed online algorithm converges at a rate that depends on the
conditional probability estimation risk. For instance, in the special case
where the conditional probability model is logistic regression, our procedure
achieves $O(\frac{1}{\sqrt{n}})$ sample complexity, both for batch and online
training. Empirical evaluation shows that the proposed algorithms out-perform
alternatives in practice, with comparable or better prediction performance and
reduced run time for various metrics and datasets.


Identifiability of Nonparametric Mixture Models and Bayes Optimal
  Clustering

  Motivated by problems in data clustering, we establish general conditions
under which families of nonparametric mixture models are identifiable, by
introducing a novel framework involving clustering overfitted \emph{parametric}
(i.e. misspecified) mixture models. These identifiability conditions generalize
existing conditions in the literature, and are flexible enough to include for
example mixtures of Gaussian mixtures. In contrast to the recent literature on
estimating nonparametric mixtures, we allow for general nonparametric mixture
components, and instead impose regularity assumptions on the underlying mixing
measure. As our primary application, we apply these results to partition-based
clustering, generalizing the notion of a Bayes optimal partition from classical
parametric model-based clustering to nonparametric settings. Furthermore, this
framework is constructive so that it yields a practical algorithm for learning
identified mixtures, which is illustrated through several examples on real
data. The key conceptual device in the analysis is the convex, metric geometry
of probability measures on metric spaces and its connection to the Wasserstein
convergence of mixing measures. The result is a flexible framework for
nonparametric clustering with formal consistency guarantees.


D2KE: From Distance to Kernel and Embedding

  For many machine learning problem settings, particularly with structured
inputs such as sequences or sets of objects, a distance measure between inputs
can be specified more naturally than a feature representation. However, most
standard machine models are designed for inputs with a vector feature
representation. In this work, we consider the estimation of a function
$f:\mathcal{X} \rightarrow \R$ based solely on a dissimilarity measure
$d:\mathcal{X}\times\mathcal{X} \rightarrow \R$ between inputs. In particular,
we propose a general framework to derive a family of \emph{positive definite
kernels} from a given dissimilarity measure, which subsumes the widely-used
\emph{representative-set method} as a special case, and relates to the
well-known \emph{distance substitution kernel} in a limiting case. We show that
functions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are
Lipschitz-continuous w.r.t. the given distance metric. We provide a tractable
algorithm to estimate a function from this RKHS, and show that it enjoys better
generalizability than Nearest-Neighbor estimates. Our approach draws from the
literature of Random Features, but instead of deriving feature maps from an
existing kernel, we construct novel kernels from a random feature map, that we
specify given the distance measure. We conduct classification experiments with
such disparate domains as strings, time series, and sets of vectors, where our
proposed framework compares favorably to existing distance-based learning
methods such as $k$-nearest-neighbors, distance-substitution kernels,
pseudo-Euclidean embedding, and the representative-set method.


How Sensitive are Sensitivity-Based Explanations?

  We propose a simple objective evaluation measure for explanations of a
complex black-box machine learning model. While most such model explanations
have largely been evaluated via qualitative measures, such as how humans might
qualitatively perceive the explanations, it is vital to also consider objective
measures such as the one we propose in this paper. Our evaluation measure that
we naturally call sensitivity is simple: it characterizes how an explanation
changes as we vary the test input, and depending on how we measure these
changes, and how we vary the input, we arrive at different notions of
sensitivity. We also provide a calculus for deriving sensitivity of complex
explanations in terms of that for simpler explanations, which thus allows an
easy computation of sensitivities for yet to be proposed explanations. One
advantage of an objective evaluation measure is that we can optimize the
explanation with respect to the measure: we show that (1) any given explanation
can be simply modified to improve its sensitivity with just a modest deviation
from the original explanation, and (2) gradient based explanations of an
adversarially trained network are less sensitive. Perhaps surprisingly, our
experiments show that explanations optimized to have lower sensitivity can be
more faithful to the model predictions.


