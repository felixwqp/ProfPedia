Unsupervised Learning of Geometry with Edge-aware Depth-Normal
  Consistency

  Learning to reconstruct depths in a single image by watching unlabeled videos
via deep convolutional network (DCN) is attracting significant attention in
recent years. In this paper, we introduce a surface normal representation for
unsupervised depth estimation framework. Our estimated depths are constrained
to be compatible with predicted normals, yielding more robust geometry results.
Specifically, we formulate an edge-aware depth-normal consistency term, and
solve it by constructing a depth-to-normal layer and a normal-to-depth layer
inside of the DCN. The depth-to-normal layer takes estimated depths as input,
and computes normal directions using cross production based on neighboring
pixels. Then given the estimated normals, the normal-to-depth layer outputs a
regularized depth map through local planar smoothness. Both layers are computed
with awareness of edges inside the image to help address the issue of
depth/normal discontinuity and preserve sharp edges. Finally, to train the
network, we apply the photometric error and gradient smoothness for both depth
and normal predictions. We conducted experiments on both outdoor (KITTI) and
indoor (NYUv2) datasets, and show that our algorithm vastly outperforms state
of the art, which demonstrates the benefits from our approach.


